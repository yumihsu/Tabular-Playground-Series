{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "from tensorflow import keras\r\n",
    "import numpy as np\r\n",
    "import pandas as pd\r\n",
    "from sklearn.model_selection import train_test_split\r\n",
    "from sklearn import preprocessing\r\n",
    "from keras import backend as K\r\n",
    "\r\n",
    "from tensorflow.python.client import device_lib\r\n",
    "print(device_lib.list_local_devices())"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 7574379227863431877\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 4963368960\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 15516034772602226166\n",
      "physical_device_desc: \"device: 0, name: GeForce RTX 2060, pci bus id: 0000:01:00.0, compute capability: 7.5\"\n",
      "]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "def df_to_x_y(df):\r\n",
    "    df = df.reset_index(drop=True)\r\n",
    "    y_train = df[['target_carbon_monoxide','target_benzene','target_nitrogen_oxides']]\r\n",
    "    x_train = df.drop(columns= ['target_carbon_monoxide','target_benzene','target_nitrogen_oxides'],axis = 1)\r\n",
    "    x_train, x_valid , y_train,y_valid = train_test_split(x_train, y_train, test_size=0.2, random_state=23)\r\n",
    "    return x_train,y_train, x_valid, y_valid\r\n",
    "\r\n",
    "def feature_engine(df):\r\n",
    "    df = df.reset_index(drop = True)\r\n",
    "    # datetime_processing\r\n",
    "    datatime = df['date_time']\r\n",
    "    df['hour'] = pd.to_datetime(df['date_time']).dt.hour\r\n",
    "    df['hour'] = abs(df['hour']-12)/12\r\n",
    "\r\n",
    "    df['month'] = pd.to_datetime(df['date_time']).dt.month\r\n",
    "    df['month'] = abs(df['hour']-6)/6\r\n",
    "\r\n",
    "    df = df.drop(['date_time'],axis = 1)\r\n",
    "    \r\n",
    "    # adjust mean_deg_c\r\n",
    "    gap = 1\r\n",
    "\r\n",
    "    mean_deg_c = df['deg_C'].copy()\r\n",
    "    change = 0\r\n",
    "    for i in range(1,len(mean_deg_c)-1):\r\n",
    "        #increase too much\r\n",
    "        if mean_deg_c[i-1] - mean_deg_c[i] >  gap:\r\n",
    "            mean_deg_c[i] = mean_deg_c[i-1] - gap\r\n",
    "            change +=1\r\n",
    "        #decrease too much\r\n",
    "        if mean_deg_c[i-1] - mean_deg_c[i] <  gap*-1:\r\n",
    "            mean_deg_c[i] = mean_deg_c[i-1] + gap\r\n",
    "            change +=1\r\n",
    "    df['mean_deg_c'] = mean_deg_c\r\n",
    "    df = df.drop(['deg_C'],axis = 1)\r\n",
    "    df = pd.DataFrame(df)\r\n",
    "    return df ,datatime\r\n",
    "\r\n",
    "def df_to_series(df,step = 12):\r\n",
    "    for idx in range(step,len(df)):\r\n",
    "        if idx == step:\r\n",
    "            series_df = [df[idx-step:idx].values]\r\n",
    "        else :\r\n",
    "            series_df = np.concatenate([series_df,[df[idx-step:idx].values]],axis = 0)\r\n",
    "    return series_df\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "origin_train = pd.read_csv(\"train.csv\")\r\n",
    "origin_test = pd.read_csv('test.csv')\r\n",
    "print(list(origin_train.columns))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['date_time', 'deg_C', 'relative_humidity', 'absolute_humidity', 'sensor_1', 'sensor_2', 'sensor_3', 'sensor_4', 'sensor_5', 'target_carbon_monoxide', 'target_benzene', 'target_nitrogen_oxides']\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "print(origin_train.head(),'\\n',origin_test.head())"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "             date_time  deg_C  relative_humidity  absolute_humidity  sensor_1  \\\n",
      "0  2010-03-10 18:00:00   13.1               46.0             0.7578    1387.2   \n",
      "1  2010-03-10 19:00:00   13.2               45.3             0.7255    1279.1   \n",
      "2  2010-03-10 20:00:00   12.6               56.2             0.7502    1331.9   \n",
      "3  2010-03-10 21:00:00   11.0               62.4             0.7867    1321.0   \n",
      "4  2010-03-10 22:00:00   11.9               59.0             0.7888    1272.0   \n",
      "\n",
      "   sensor_2  sensor_3  sensor_4  sensor_5  target_carbon_monoxide  \\\n",
      "0    1087.8    1056.0    1742.8    1293.4                     2.5   \n",
      "1     888.2    1197.5    1449.9    1010.9                     2.1   \n",
      "2     929.6    1060.2    1586.1    1117.0                     2.2   \n",
      "3     929.0    1102.9    1536.5    1263.2                     2.2   \n",
      "4     852.7    1180.9    1415.5    1132.2                     1.5   \n",
      "\n",
      "   target_benzene  target_nitrogen_oxides  \n",
      "0            12.0                   167.7  \n",
      "1             9.9                    98.9  \n",
      "2             9.2                   127.1  \n",
      "3             9.7                   177.2  \n",
      "4             6.4                   121.8   \n",
      "              date_time  deg_C  relative_humidity  absolute_humidity  sensor_1  \\\n",
      "0  2011-01-01 00:00:00    8.0               41.3             0.4375    1108.8   \n",
      "1  2011-01-01 01:00:00    5.1               51.7             0.4564    1249.5   \n",
      "2  2011-01-01 02:00:00    5.8               51.5             0.4689    1102.6   \n",
      "3  2011-01-01 03:00:00    5.0               52.3             0.4693    1139.7   \n",
      "4  2011-01-01 04:00:00    4.5               57.5             0.4650    1022.4   \n",
      "\n",
      "   sensor_2  sensor_3  sensor_4  sensor_5  \n",
      "0     745.7     797.1     880.0    1273.1  \n",
      "1     864.9     687.9     972.8    1714.0  \n",
      "2     878.0     693.7     941.9    1300.8  \n",
      "3     916.2     725.6    1011.0    1283.0  \n",
      "4     838.5     871.5     967.0    1142.3  \n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "none_train = origin_train.copy()\r\n",
    "print(len(none_train))\r\n",
    "for i in  range(1,len(none_train.columns)):\r\n",
    "    target = none_train.iloc[:,i]\r\n",
    "    scope = target.mean()+(target.std()*2)\r\n",
    "    column_name = none_train.columns[i]\r\n",
    "    none_train = none_train[none_train[column_name]<float(scope)]\r\n",
    "    print(f'{none_train.columns[i]}:{len(none_train)}')\r\n",
    "clean_df = none_train.copy()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "7111\n",
      "deg_C:6873\n",
      "relative_humidity:6775\n",
      "absolute_humidity:6638\n",
      "sensor_1:6368\n",
      "sensor_2:6200\n",
      "sensor_3:5846\n",
      "sensor_4:5706\n",
      "sensor_5:5513\n",
      "target_carbon_monoxide:5276\n",
      "target_benzene:5082\n",
      "target_nitrogen_oxides:4809\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "df = clean_df.copy()\r\n",
    "df ,datatime = feature_engine(df)\r\n",
    "x_train,y_train,x_valid, y_valid = df_to_x_y(df)\r\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\r\n",
    "x_train = pd.DataFrame(min_max_scaler.fit_transform(x_train))\r\n",
    "x_valid = pd.DataFrame(min_max_scaler.fit_transform(x_valid))\r\n",
    "print(x_train)\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "             0         1         2         3         4         5         6  \\\n",
      "0     0.317872  0.562694  0.488849  0.723723  0.272981  0.772650  0.659181   \n",
      "1     0.536153  0.722544  0.517427  0.588904  0.223217  0.802924  0.664698   \n",
      "2     0.268759  0.630858  0.206097  0.213932  0.593609  0.527579  0.089051   \n",
      "3     0.440655  0.487742  0.472263  0.731600  0.384349  0.642694  0.349444   \n",
      "4     0.518417  0.251126  0.295192  0.138758  0.966311  0.360186  0.135442   \n",
      "...        ...       ...       ...       ...       ...       ...       ...   \n",
      "3842  0.368349  0.400737  0.242071  0.230587  0.876600  0.365281  0.199394   \n",
      "3843  0.508868  0.528524  0.107363  0.260072  0.543748  0.382633  0.230943   \n",
      "3844  0.609823  0.656954  0.371064  0.532185  0.313697  0.714391  0.531743   \n",
      "3845  0.271487  0.260781  0.346632  0.580239  0.352969  0.539688  0.305385   \n",
      "3846  0.237381  0.244222  0.273114  0.256696  0.851670  0.347781  0.069936   \n",
      "\n",
      "             7         8         9  \n",
      "0     0.833333  0.166667  0.678261  \n",
      "1     0.416667  0.583333  0.672464  \n",
      "2     0.083333  0.916667  0.823188  \n",
      "3     0.500000  0.500000  0.588406  \n",
      "4     0.666667  0.333333  0.263768  \n",
      "...        ...       ...       ...  \n",
      "3842  0.333333  0.666667  0.504348  \n",
      "3843  0.583333  0.416667  0.579710  \n",
      "3844  0.500000  0.500000  0.591304  \n",
      "3845  0.083333  0.916667  0.504348  \n",
      "3846  0.416667  0.583333  0.475362  \n",
      "\n",
      "[3847 rows x 10 columns]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "per_step = 24\r\n",
    "def df_to_series(df,step = per_step):\r\n",
    "    for idx in range(step,len(df)):\r\n",
    "        if idx == step:\r\n",
    "            series_df = [df[idx-step:idx].values]\r\n",
    "        else :\r\n",
    "            series_df = np.concatenate([series_df,[df[idx-step:idx].values]],axis = 0)\r\n",
    "    return series_df"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "x_train_series = df_to_series(x_train).astype('float32')\r\n",
    "y_train_series = y_train[per_step:].values\r\n",
    "x_valid_series = df_to_series(x_valid).astype('float32')\r\n",
    "y_valid_series = y_valid[per_step:].values"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "def loss_rmsle(pred , actual):\r\n",
    "    return K.sqrt(K.mean(K.square(K.mean(K.log(pred + 1)-K.log(actual +1)))))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "source": [
    "model0 = keras.Sequential()\r\n",
    "model0.add(keras.layers.LSTM(5,input_shape = (per_step,x_train.shape[1])))\r\n",
    "model0.add(keras.layers.Dense(1))\r\n",
    "model0.compile(optimizer = 'adam', loss = loss_rmsle,metrics =[loss_rmsle])\r\n",
    "\r\n",
    "model1 = keras.Sequential()\r\n",
    "model1.add(keras.layers.LSTM(5,input_shape = (per_step,x_train.shape[1])))\r\n",
    "model1.add(keras.layers.Dense(1))\r\n",
    "model1.compile(optimizer = 'adam', loss = loss_rmsle,metrics =[loss_rmsle])\r\n",
    "\r\n",
    "model2 = keras.Sequential()\r\n",
    "model2.add(keras.layers.LSTM(5,input_shape = (per_step,x_train.shape[1])))\r\n",
    "model2.add(keras.layers.Dense(1))\r\n",
    "model2.compile(optimizer = 'adam', loss = loss_rmsle,metrics =[loss_rmsle])\r\n",
    "\r\n",
    "model_checkpoint_callback = keras.callbacks.ModelCheckpoint(\r\n",
    "    filepath = 'E:\\jupyter\\mygithub\\Tabular-Playground-Series',\r\n",
    "    save_weights_only = False,\r\n",
    "    monitor = loss_rmsle,\r\n",
    "    mode = 'min',\r\n",
    "    save_best_only = True)\r\n",
    "\r\n",
    "model_early_stop = keras.callbacks.EarlyStopping(\r\n",
    "    monitor = 'loss_rmsle',\r\n",
    "    patience = 3,\r\n",
    ")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "source": [
    "model0.fit(x_train_series,y_train_series[:,0],epochs=250, batch_size=50 ,callbacks=[model_checkpoint_callback,model_early_stop])\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Train on 3823 samples\n",
      "Epoch 1/250\n",
      "3150/3823 [=======================>......] - ETA: 0s - loss: 0.1321 - loss_rmsle: 0.1321WARNING:tensorflow:Can save best model only with <function loss_rmsle at 0x000001A08D467158> available, skipping.\n",
      "3823/3823 [==============================] - 1s 386us/sample - loss: 0.1215 - loss_rmsle: 0.1206\n",
      "Epoch 2/250\n",
      "3750/3823 [============================>.] - ETA: 0s - loss: 0.0398 - loss_rmsle: 0.0398WARNING:tensorflow:Can save best model only with <function loss_rmsle at 0x000001A08D467158> available, skipping.\n",
      "3823/3823 [==============================] - 0s 72us/sample - loss: 0.0397 - loss_rmsle: 0.0395\n",
      "Epoch 3/250\n",
      "3550/3823 [==========================>...] - ETA: 0s - loss: 0.0408 - loss_rmsle: 0.0408WARNING:tensorflow:Can save best model only with <function loss_rmsle at 0x000001A08D467158> available, skipping.\n",
      "3823/3823 [==============================] - 0s 74us/sample - loss: 0.0417 - loss_rmsle: 0.0422\n",
      "Epoch 4/250\n",
      "3700/3823 [============================>.] - ETA: 0s - loss: 0.0336 - loss_rmsle: 0.0336WARNING:tensorflow:Can save best model only with <function loss_rmsle at 0x000001A08D467158> available, skipping.\n",
      "3823/3823 [==============================] - 0s 74us/sample - loss: 0.0338 - loss_rmsle: 0.0339\n",
      "Epoch 5/250\n",
      "3550/3823 [==========================>...] - ETA: 0s - loss: 0.0390 - loss_rmsle: 0.0390WARNING:tensorflow:Can save best model only with <function loss_rmsle at 0x000001A08D467158> available, skipping.\n",
      "3823/3823 [==============================] - 0s 76us/sample - loss: 0.0402 - loss_rmsle: 0.0404\n",
      "Epoch 6/250\n",
      "3450/3823 [==========================>...] - ETA: 0s - loss: 0.0371 - loss_rmsle: 0.0371WARNING:tensorflow:Can save best model only with <function loss_rmsle at 0x000001A08D467158> available, skipping.\n",
      "3823/3823 [==============================] - 0s 80us/sample - loss: 0.0370 - loss_rmsle: 0.0376\n",
      "Epoch 7/250\n",
      "3150/3823 [=======================>......] - ETA: 0s - loss: 0.0413 - loss_rmsle: 0.0413WARNING:tensorflow:Can save best model only with <function loss_rmsle at 0x000001A08D467158> available, skipping.\n",
      "3823/3823 [==============================] - 0s 68us/sample - loss: 0.0378 - loss_rmsle: 0.0376\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1a0878d2518>"
      ]
     },
     "metadata": {},
     "execution_count": 28
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "source": [
    "model1.fit(x_train_series,y_train_series[:,1],epochs=250, batch_size=50,callbacks=[model_checkpoint_callback,model_early_stop])\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Train on 3823 samples\n",
      "Epoch 1/250\n",
      "3100/3823 [=======================>......] - ETA: 0s - loss: 1.1756 - loss_rmsle: 1.1756WARNING:tensorflow:Can save best model only with <function loss_rmsle at 0x000001A08D467158> available, skipping.\n",
      "3823/3823 [==============================] - 2s 394us/sample - loss: 1.1303 - loss_rmsle: 1.1307\n",
      "Epoch 2/250\n",
      "3100/3823 [=======================>......] - ETA: 0s - loss: 0.7786 - loss_rmsle: 0.7786WARNING:tensorflow:Can save best model only with <function loss_rmsle at 0x000001A08D467158> available, skipping.\n",
      "3823/3823 [==============================] - 0s 69us/sample - loss: 0.7514 - loss_rmsle: 0.7486\n",
      "Epoch 3/250\n",
      "3700/3823 [============================>.] - ETA: 0s - loss: 0.5415 - loss_rmsle: 0.5415WARNING:tensorflow:Can save best model only with <function loss_rmsle at 0x000001A08D467158> available, skipping.\n",
      "3823/3823 [==============================] - 0s 72us/sample - loss: 0.5376 - loss_rmsle: 0.5354\n",
      "Epoch 4/250\n",
      "3700/3823 [============================>.] - ETA: 0s - loss: 0.4230 - loss_rmsle: 0.4230WARNING:tensorflow:Can save best model only with <function loss_rmsle at 0x000001A08D467158> available, skipping.\n",
      "3823/3823 [==============================] - 0s 73us/sample - loss: 0.4220 - loss_rmsle: 0.4221\n",
      "Epoch 5/250\n",
      "3300/3823 [========================>.....] - ETA: 0s - loss: 0.3342 - loss_rmsle: 0.3342WARNING:tensorflow:Can save best model only with <function loss_rmsle at 0x000001A08D467158> available, skipping.\n",
      "3823/3823 [==============================] - 0s 98us/sample - loss: 0.3328 - loss_rmsle: 0.3328\n",
      "Epoch 6/250\n",
      "3300/3823 [========================>.....] - ETA: 0s - loss: 0.2569 - loss_rmsle: 0.2569WARNING:tensorflow:Can save best model only with <function loss_rmsle at 0x000001A08D467158> available, skipping.\n",
      "3823/3823 [==============================] - 0s 96us/sample - loss: 0.2573 - loss_rmsle: 0.2572\n",
      "Epoch 7/250\n",
      "3450/3823 [==========================>...] - ETA: 0s - loss: 0.1980 - loss_rmsle: 0.1980WARNING:tensorflow:Can save best model only with <function loss_rmsle at 0x000001A08D467158> available, skipping.\n",
      "3823/3823 [==============================] - 0s 77us/sample - loss: 0.1935 - loss_rmsle: 0.1925\n",
      "Epoch 8/250\n",
      "3800/3823 [============================>.] - ETA: 0s - loss: 0.1379 - loss_rmsle: 0.1379WARNING:tensorflow:Can save best model only with <function loss_rmsle at 0x000001A08D467158> available, skipping.\n",
      "3823/3823 [==============================] - 0s 70us/sample - loss: 0.1386 - loss_rmsle: 0.1394\n",
      "Epoch 9/250\n",
      "3650/3823 [===========================>..] - ETA: 0s - loss: 0.1002 - loss_rmsle: 0.1002WARNING:tensorflow:Can save best model only with <function loss_rmsle at 0x000001A08D467158> available, skipping.\n",
      "3823/3823 [==============================] - 0s 92us/sample - loss: 0.0980 - loss_rmsle: 0.0991\n",
      "Epoch 10/250\n",
      "3650/3823 [===========================>..] - ETA: 0s - loss: 0.0743 - loss_rmsle: 0.0743WARNING:tensorflow:Can save best model only with <function loss_rmsle at 0x000001A08D467158> available, skipping.\n",
      "3823/3823 [==============================] - 0s 104us/sample - loss: 0.0727 - loss_rmsle: 0.0731\n",
      "Epoch 11/250\n",
      "3550/3823 [==========================>...] - ETA: 0s - loss: 0.0684 - loss_rmsle: 0.0684WARNING:tensorflow:Can save best model only with <function loss_rmsle at 0x000001A08D467158> available, skipping.\n",
      "3823/3823 [==============================] - 0s 74us/sample - loss: 0.0678 - loss_rmsle: 0.0675\n",
      "Epoch 12/250\n",
      "3650/3823 [===========================>..] - ETA: 0s - loss: 0.0618 - loss_rmsle: 0.0618WARNING:tensorflow:Can save best model only with <function loss_rmsle at 0x000001A08D467158> available, skipping.\n",
      "3823/3823 [==============================] - 0s 72us/sample - loss: 0.0617 - loss_rmsle: 0.0614\n",
      "Epoch 13/250\n",
      "3750/3823 [============================>.] - ETA: 0s - loss: 0.0579 - loss_rmsle: 0.0579WARNING:tensorflow:Can save best model only with <function loss_rmsle at 0x000001A08D467158> available, skipping.\n",
      "3823/3823 [==============================] - 0s 72us/sample - loss: 0.0594 - loss_rmsle: 0.0607\n",
      "Epoch 14/250\n",
      "3550/3823 [==========================>...] - ETA: 0s - loss: 0.0617 - loss_rmsle: 0.0617WARNING:tensorflow:Can save best model only with <function loss_rmsle at 0x000001A08D467158> available, skipping.\n",
      "3823/3823 [==============================] - 0s 75us/sample - loss: 0.0609 - loss_rmsle: 0.0608\n",
      "Epoch 15/250\n",
      "3800/3823 [============================>.] - ETA: 0s - loss: 0.0522 - loss_rmsle: 0.0522WARNING:tensorflow:Can save best model only with <function loss_rmsle at 0x000001A08D467158> available, skipping.\n",
      "3823/3823 [==============================] - 0s 72us/sample - loss: 0.0520 - loss_rmsle: 0.0517\n",
      "Epoch 16/250\n",
      "3800/3823 [============================>.] - ETA: 0s - loss: 0.0560 - loss_rmsle: 0.0560WARNING:tensorflow:Can save best model only with <function loss_rmsle at 0x000001A08D467158> available, skipping.\n",
      "3823/3823 [==============================] - 0s 71us/sample - loss: 0.0558 - loss_rmsle: 0.0556\n",
      "Epoch 17/250\n",
      "3100/3823 [=======================>......] - ETA: 0s - loss: 0.0720 - loss_rmsle: 0.0720WARNING:tensorflow:Can save best model only with <function loss_rmsle at 0x000001A08D467158> available, skipping.\n",
      "3823/3823 [==============================] - 0s 70us/sample - loss: 0.0699 - loss_rmsle: 0.0695\n",
      "Epoch 18/250\n",
      "3700/3823 [============================>.] - ETA: 0s - loss: 0.0618 - loss_rmsle: 0.0618WARNING:tensorflow:Can save best model only with <function loss_rmsle at 0x000001A08D467158> available, skipping.\n",
      "3823/3823 [==============================] - 0s 72us/sample - loss: 0.0612 - loss_rmsle: 0.0612\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1a08a6792b0>"
      ]
     },
     "metadata": {},
     "execution_count": 29
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "source": [
    "model2.fit(x_train_series,y_train_series[:,2],epochs=400, batch_size=50,callbacks=[model_checkpoint_callback,model_early_stop])"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Train on 3823 samples\n",
      "Epoch 1/400\n",
      "3100/3823 [=======================>......] - ETA: 0s - loss: 1.6528 - loss_rmsle: 1.6528WARNING:tensorflow:Can save best model only with <function loss_rmsle at 0x000001A08D467158> available, skipping.\n",
      "3823/3823 [==============================] - 0s 70us/sample - loss: 1.6509 - loss_rmsle: 1.6516\n",
      "Epoch 2/400\n",
      "3150/3823 [=======================>......] - ETA: 0s - loss: 1.6381 - loss_rmsle: 1.6381WARNING:tensorflow:Can save best model only with <function loss_rmsle at 0x000001A08D467158> available, skipping.\n",
      "3823/3823 [==============================] - 0s 68us/sample - loss: 1.6353 - loss_rmsle: 1.6359\n",
      "Epoch 3/400\n",
      "3650/3823 [===========================>..] - ETA: 0s - loss: 1.6248 - loss_rmsle: 1.6248WARNING:tensorflow:Can save best model only with <function loss_rmsle at 0x000001A08D467158> available, skipping.\n",
      "3823/3823 [==============================] - 0s 72us/sample - loss: 1.6200 - loss_rmsle: 1.6187\n",
      "Epoch 4/400\n",
      "3800/3823 [============================>.] - ETA: 0s - loss: 1.6027 - loss_rmsle: 1.6027WARNING:tensorflow:Can save best model only with <function loss_rmsle at 0x000001A08D467158> available, skipping.\n",
      "3823/3823 [==============================] - 0s 72us/sample - loss: 1.6048 - loss_rmsle: 1.6072\n",
      "Epoch 5/400\n",
      "3800/3823 [============================>.] - ETA: 0s - loss: 1.5894 - loss_rmsle: 1.5895WARNING:tensorflow:Can save best model only with <function loss_rmsle at 0x000001A08D467158> available, skipping.\n",
      "3823/3823 [==============================] - 0s 70us/sample - loss: 1.5897 - loss_rmsle: 1.5900\n",
      "Epoch 6/400\n",
      "3800/3823 [============================>.] - ETA: 0s - loss: 1.5749 - loss_rmsle: 1.5749WARNING:tensorflow:Can save best model only with <function loss_rmsle at 0x000001A08D467158> available, skipping.\n",
      "3823/3823 [==============================] - 0s 71us/sample - loss: 1.5748 - loss_rmsle: 1.5747\n",
      "Epoch 7/400\n",
      "3700/3823 [============================>.] - ETA: 0s - loss: 1.5572 - loss_rmsle: 1.5572WARNING:tensorflow:Can save best model only with <function loss_rmsle at 0x000001A08D467158> available, skipping.\n",
      "3823/3823 [==============================] - 0s 72us/sample - loss: 1.5601 - loss_rmsle: 1.5610\n",
      "Epoch 8/400\n",
      "3800/3823 [============================>.] - ETA: 0s - loss: 1.5465 - loss_rmsle: 1.5465WARNING:tensorflow:Can save best model only with <function loss_rmsle at 0x000001A08D467158> available, skipping.\n",
      "3823/3823 [==============================] - 0s 71us/sample - loss: 1.5455 - loss_rmsle: 1.5443\n",
      "Epoch 9/400\n",
      "3250/3823 [========================>.....] - ETA: 0s - loss: 1.5312 - loss_rmsle: 1.5312WARNING:tensorflow:Can save best model only with <function loss_rmsle at 0x000001A08D467158> available, skipping.\n",
      "3823/3823 [==============================] - 0s 82us/sample - loss: 1.5311 - loss_rmsle: 1.5324\n",
      "Epoch 10/400\n",
      "3600/3823 [===========================>..] - ETA: 0s - loss: 1.5165 - loss_rmsle: 1.5165WARNING:tensorflow:Can save best model only with <function loss_rmsle at 0x000001A08D467158> available, skipping.\n",
      "3823/3823 [==============================] - 0s 76us/sample - loss: 1.5168 - loss_rmsle: 1.5182\n",
      "Epoch 11/400\n",
      "3150/3823 [=======================>......] - ETA: 0s - loss: 1.5026 - loss_rmsle: 1.5026WARNING:tensorflow:Can save best model only with <function loss_rmsle at 0x000001A08D467158> available, skipping.\n",
      "3823/3823 [==============================] - 0s 82us/sample - loss: 1.5026 - loss_rmsle: 1.5020\n",
      "Epoch 12/400\n",
      "3500/3823 [==========================>...] - ETA: 0s - loss: 1.4904 - loss_rmsle: 1.4904WARNING:tensorflow:Can save best model only with <function loss_rmsle at 0x000001A08D467158> available, skipping.\n",
      "3823/3823 [==============================] - 0s 76us/sample - loss: 1.4886 - loss_rmsle: 1.4902\n",
      "Epoch 13/400\n",
      "3600/3823 [===========================>..] - ETA: 0s - loss: 1.4727 - loss_rmsle: 1.4727WARNING:tensorflow:Can save best model only with <function loss_rmsle at 0x000001A08D467158> available, skipping.\n",
      "3823/3823 [==============================] - 0s 76us/sample - loss: 1.4747 - loss_rmsle: 1.4750\n",
      "Epoch 14/400\n",
      "3450/3823 [==========================>...] - ETA: 0s - loss: 1.4544 - loss_rmsle: 1.4544WARNING:tensorflow:Can save best model only with <function loss_rmsle at 0x000001A08D467158> available, skipping.\n",
      "3823/3823 [==============================] - 0s 78us/sample - loss: 1.4610 - loss_rmsle: 1.4617\n",
      "Epoch 15/400\n",
      "3550/3823 [==========================>...] - ETA: 0s - loss: 1.4456 - loss_rmsle: 1.4456WARNING:tensorflow:Can save best model only with <function loss_rmsle at 0x000001A08D467158> available, skipping.\n",
      "3823/3823 [==============================] - 0s 89us/sample - loss: 1.4473 - loss_rmsle: 1.4478\n",
      "Epoch 16/400\n",
      "3500/3823 [==========================>...] - ETA: 0s - loss: 1.4324 - loss_rmsle: 1.4324WARNING:tensorflow:Can save best model only with <function loss_rmsle at 0x000001A08D467158> available, skipping.\n",
      "3823/3823 [==============================] - 0s 78us/sample - loss: 1.4339 - loss_rmsle: 1.4332\n",
      "Epoch 17/400\n",
      "3500/3823 [==========================>...] - ETA: 0s - loss: 1.4195 - loss_rmsle: 1.4195WARNING:tensorflow:Can save best model only with <function loss_rmsle at 0x000001A08D467158> available, skipping.\n",
      "3823/3823 [==============================] - 0s 79us/sample - loss: 1.4205 - loss_rmsle: 1.4207\n",
      "Epoch 18/400\n",
      "3650/3823 [===========================>..] - ETA: 0s - loss: 1.4058 - loss_rmsle: 1.4058WARNING:tensorflow:Can save best model only with <function loss_rmsle at 0x000001A08D467158> available, skipping.\n",
      "3823/3823 [==============================] - 0s 90us/sample - loss: 1.4073 - loss_rmsle: 1.4075\n",
      "Epoch 19/400\n",
      "3700/3823 [============================>.] - ETA: 0s - loss: 1.3952 - loss_rmsle: 1.3952WARNING:tensorflow:Can save best model only with <function loss_rmsle at 0x000001A08D467158> available, skipping.\n",
      "3823/3823 [==============================] - 0s 88us/sample - loss: 1.3941 - loss_rmsle: 1.3949\n",
      "Epoch 20/400\n",
      "3550/3823 [==========================>...] - ETA: 0s - loss: 1.3794 - loss_rmsle: 1.3794WARNING:tensorflow:Can save best model only with <function loss_rmsle at 0x000001A08D467158> available, skipping.\n",
      "3823/3823 [==============================] - 0s 77us/sample - loss: 1.3811 - loss_rmsle: 1.3824\n",
      "Epoch 21/400\n",
      "3550/3823 [==========================>...] - ETA: 0s - loss: 1.3691 - loss_rmsle: 1.3691WARNING:tensorflow:Can save best model only with <function loss_rmsle at 0x000001A08D467158> available, skipping.\n",
      "3823/3823 [==============================] - 0s 75us/sample - loss: 1.3683 - loss_rmsle: 1.3682\n",
      "Epoch 22/400\n",
      "3800/3823 [============================>.] - ETA: 0s - loss: 1.3552 - loss_rmsle: 1.3552WARNING:tensorflow:Can save best model only with <function loss_rmsle at 0x000001A08D467158> available, skipping.\n",
      "3823/3823 [==============================] - 0s 85us/sample - loss: 1.3555 - loss_rmsle: 1.3559\n",
      "Epoch 23/400\n",
      "3700/3823 [============================>.] - ETA: 0s - loss: 1.3434 - loss_rmsle: 1.3434WARNING:tensorflow:Can save best model only with <function loss_rmsle at 0x000001A08D467158> available, skipping.\n",
      "3823/3823 [==============================] - 0s 85us/sample - loss: 1.3428 - loss_rmsle: 1.3422\n",
      "Epoch 24/400\n",
      "3700/3823 [============================>.] - ETA: 0s - loss: 1.3293 - loss_rmsle: 1.3293WARNING:tensorflow:Can save best model only with <function loss_rmsle at 0x000001A08D467158> available, skipping.\n",
      "3823/3823 [==============================] - 0s 72us/sample - loss: 1.3303 - loss_rmsle: 1.3318\n",
      "Epoch 25/400\n",
      "3700/3823 [============================>.] - ETA: 0s - loss: 1.3227 - loss_rmsle: 1.3227WARNING:tensorflow:Can save best model only with <function loss_rmsle at 0x000001A08D467158> available, skipping.\n",
      "3823/3823 [==============================] - 0s 73us/sample - loss: 1.3179 - loss_rmsle: 1.3172\n",
      "Epoch 26/400\n",
      "3500/3823 [==========================>...] - ETA: 0s - loss: 1.3043 - loss_rmsle: 1.3043WARNING:tensorflow:Can save best model only with <function loss_rmsle at 0x000001A08D467158> available, skipping.\n",
      "3823/3823 [==============================] - 0s 78us/sample - loss: 1.3056 - loss_rmsle: 1.3062\n",
      "Epoch 27/400\n",
      "3750/3823 [============================>.] - ETA: 0s - loss: 1.2928 - loss_rmsle: 1.2928WARNING:tensorflow:Can save best model only with <function loss_rmsle at 0x000001A08D467158> available, skipping.\n",
      "3823/3823 [==============================] - 0s 85us/sample - loss: 1.2933 - loss_rmsle: 1.2943\n",
      "Epoch 28/400\n",
      "3700/3823 [============================>.] - ETA: 0s - loss: 1.2779 - loss_rmsle: 1.2779WARNING:tensorflow:Can save best model only with <function loss_rmsle at 0x000001A08D467158> available, skipping.\n",
      "3823/3823 [==============================] - 0s 73us/sample - loss: 1.2812 - loss_rmsle: 1.2827\n",
      "Epoch 29/400\n",
      "3750/3823 [============================>.] - ETA: 0s - loss: 1.2688 - loss_rmsle: 1.2688WARNING:tensorflow:Can save best model only with <function loss_rmsle at 0x000001A08D467158> available, skipping.\n",
      "3823/3823 [==============================] - 0s 71us/sample - loss: 1.2692 - loss_rmsle: 1.2707\n",
      "Epoch 30/400\n",
      "3700/3823 [============================>.] - ETA: 0s - loss: 1.2571 - loss_rmsle: 1.2571WARNING:tensorflow:Can save best model only with <function loss_rmsle at 0x000001A08D467158> available, skipping.\n",
      "3823/3823 [==============================] - 0s 72us/sample - loss: 1.2573 - loss_rmsle: 1.2556\n",
      "Epoch 31/400\n",
      "3150/3823 [=======================>......] - ETA: 0s - loss: 1.2400 - loss_rmsle: 1.2400WARNING:tensorflow:Can save best model only with <function loss_rmsle at 0x000001A08D467158> available, skipping.\n",
      "3823/3823 [==============================] - 0s 70us/sample - loss: 1.2455 - loss_rmsle: 1.2459\n",
      "Epoch 32/400\n",
      "3100/3823 [=======================>......] - ETA: 0s - loss: 1.2295 - loss_rmsle: 1.2295WARNING:tensorflow:Can save best model only with <function loss_rmsle at 0x000001A08D467158> available, skipping.\n",
      "3823/3823 [==============================] - 0s 69us/sample - loss: 1.2338 - loss_rmsle: 1.2347\n",
      "Epoch 33/400\n",
      "3550/3823 [==========================>...] - ETA: 0s - loss: 1.2188 - loss_rmsle: 1.2188WARNING:tensorflow:Can save best model only with <function loss_rmsle at 0x000001A08D467158> available, skipping.\n",
      "3823/3823 [==============================] - 0s 75us/sample - loss: 1.2222 - loss_rmsle: 1.2229\n",
      "Epoch 34/400\n",
      "3750/3823 [============================>.] - ETA: 0s - loss: 1.2122 - loss_rmsle: 1.2122WARNING:tensorflow:Can save best model only with <function loss_rmsle at 0x000001A08D467158> available, skipping.\n",
      "3823/3823 [==============================] - 0s 71us/sample - loss: 1.2107 - loss_rmsle: 1.2107\n",
      "Epoch 35/400\n",
      "3750/3823 [============================>.] - ETA: 0s - loss: 1.1990 - loss_rmsle: 1.1990WARNING:tensorflow:Can save best model only with <function loss_rmsle at 0x000001A08D467158> available, skipping.\n",
      "3823/3823 [==============================] - 0s 71us/sample - loss: 1.1993 - loss_rmsle: 1.1978\n",
      "Epoch 36/400\n",
      "3150/3823 [=======================>......] - ETA: 0s - loss: 1.1851 - loss_rmsle: 1.1851WARNING:tensorflow:Can save best model only with <function loss_rmsle at 0x000001A08D467158> available, skipping.\n",
      "3823/3823 [==============================] - 0s 69us/sample - loss: 1.1880 - loss_rmsle: 1.1869\n",
      "Epoch 37/400\n",
      "3800/3823 [============================>.] - ETA: 0s - loss: 1.1773 - loss_rmsle: 1.1773WARNING:tensorflow:Can save best model only with <function loss_rmsle at 0x000001A08D467158> available, skipping.\n",
      "3823/3823 [==============================] - 0s 70us/sample - loss: 1.1768 - loss_rmsle: 1.1761\n",
      "Epoch 38/400\n",
      "3150/3823 [=======================>......] - ETA: 0s - loss: 1.1642 - loss_rmsle: 1.1642WARNING:tensorflow:Can save best model only with <function loss_rmsle at 0x000001A08D467158> available, skipping.\n",
      "3823/3823 [==============================] - 0s 69us/sample - loss: 1.1657 - loss_rmsle: 1.1659\n",
      "Epoch 39/400\n",
      "3750/3823 [============================>.] - ETA: 0s - loss: 1.1541 - loss_rmsle: 1.1541WARNING:tensorflow:Can save best model only with <function loss_rmsle at 0x000001A08D467158> available, skipping.\n",
      "3823/3823 [==============================] - 0s 70us/sample - loss: 1.1546 - loss_rmsle: 1.1546\n",
      "Epoch 40/400\n",
      "3100/3823 [=======================>......] - ETA: 0s - loss: 1.1529 - loss_rmsle: 1.1529WARNING:tensorflow:Can save best model only with <function loss_rmsle at 0x000001A08D467158> available, skipping.\n",
      "3823/3823 [==============================] - 0s 68us/sample - loss: 1.1437 - loss_rmsle: 1.1420\n",
      "Epoch 41/400\n",
      "3050/3823 [======================>.......] - ETA: 0s - loss: 1.1314 - loss_rmsle: 1.1314WARNING:tensorflow:Can save best model only with <function loss_rmsle at 0x000001A08D467158> available, skipping.\n",
      "3823/3823 [==============================] - 0s 69us/sample - loss: 1.1328 - loss_rmsle: 1.1320\n",
      "Epoch 42/400\n",
      "3100/3823 [=======================>......] - ETA: 0s - loss: 1.1177 - loss_rmsle: 1.1177WARNING:tensorflow:Can save best model only with <function loss_rmsle at 0x000001A08D467158> available, skipping.\n",
      "3823/3823 [==============================] - 0s 69us/sample - loss: 1.1220 - loss_rmsle: 1.1217\n",
      "Epoch 43/400\n",
      "3800/3823 [============================>.] - ETA: 0s - loss: 1.1100 - loss_rmsle: 1.1100WARNING:tensorflow:Can save best model only with <function loss_rmsle at 0x000001A08D467158> available, skipping.\n",
      "3823/3823 [==============================] - 0s 72us/sample - loss: 1.1114 - loss_rmsle: 1.1129\n",
      "Epoch 44/400\n",
      "3700/3823 [============================>.] - ETA: 0s - loss: 1.1010 - loss_rmsle: 1.1010WARNING:tensorflow:Can save best model only with <function loss_rmsle at 0x000001A08D467158> available, skipping.\n",
      "3823/3823 [==============================] - 0s 72us/sample - loss: 1.1008 - loss_rmsle: 1.1006\n",
      "Epoch 45/400\n",
      "3100/3823 [=======================>......] - ETA: 0s - loss: 1.0822 - loss_rmsle: 1.0822WARNING:tensorflow:Can save best model only with <function loss_rmsle at 0x000001A08D467158> available, skipping.\n",
      "3823/3823 [==============================] - 0s 70us/sample - loss: 1.0902 - loss_rmsle: 1.0901\n",
      "Epoch 46/400\n",
      "3350/3823 [=========================>....] - ETA: 0s - loss: 1.0778 - loss_rmsle: 1.0778WARNING:tensorflow:Can save best model only with <function loss_rmsle at 0x000001A08D467158> available, skipping.\n",
      "3823/3823 [==============================] - 0s 80us/sample - loss: 1.0798 - loss_rmsle: 1.0801\n",
      "Epoch 47/400\n",
      "3600/3823 [===========================>..] - ETA: 0s - loss: 1.0721 - loss_rmsle: 1.0721WARNING:tensorflow:Can save best model only with <function loss_rmsle at 0x000001A08D467158> available, skipping.\n",
      "3823/3823 [==============================] - 0s 74us/sample - loss: 1.0694 - loss_rmsle: 1.0670\n",
      "Epoch 48/400\n",
      "3100/3823 [=======================>......] - ETA: 0s - loss: 1.0610 - loss_rmsle: 1.0610WARNING:tensorflow:Can save best model only with <function loss_rmsle at 0x000001A08D467158> available, skipping.\n",
      "3823/3823 [==============================] - 0s 69us/sample - loss: 1.0592 - loss_rmsle: 1.0587\n",
      "Epoch 49/400\n",
      "3050/3823 [======================>.......] - ETA: 0s - loss: 1.0497 - loss_rmsle: 1.0497WARNING:tensorflow:Can save best model only with <function loss_rmsle at 0x000001A08D467158> available, skipping.\n",
      "3823/3823 [==============================] - 0s 69us/sample - loss: 1.0490 - loss_rmsle: 1.0473\n",
      "Epoch 50/400\n",
      "3050/3823 [======================>.......] - ETA: 0s - loss: 1.0404 - loss_rmsle: 1.0404WARNING:tensorflow:Can save best model only with <function loss_rmsle at 0x000001A08D467158> available, skipping.\n",
      "3823/3823 [==============================] - 0s 69us/sample - loss: 1.0389 - loss_rmsle: 1.0398\n",
      "Epoch 51/400\n",
      "3750/3823 [============================>.] - ETA: 0s - loss: 1.0298 - loss_rmsle: 1.0298WARNING:tensorflow:Can save best model only with <function loss_rmsle at 0x000001A08D467158> available, skipping.\n",
      "3823/3823 [==============================] - 0s 71us/sample - loss: 1.0288 - loss_rmsle: 1.0283\n",
      "Epoch 52/400\n",
      "3050/3823 [======================>.......] - ETA: 0s - loss: 1.0189 - loss_rmsle: 1.0189WARNING:tensorflow:Can save best model only with <function loss_rmsle at 0x000001A08D467158> available, skipping.\n",
      "3823/3823 [==============================] - 0s 69us/sample - loss: 1.0189 - loss_rmsle: 1.0185\n",
      "Epoch 53/400\n",
      "3050/3823 [======================>.......] - ETA: 0s - loss: 1.0152 - loss_rmsle: 1.0152WARNING:tensorflow:Can save best model only with <function loss_rmsle at 0x000001A08D467158> available, skipping.\n",
      "3823/3823 [==============================] - 0s 68us/sample - loss: 1.0090 - loss_rmsle: 1.0088\n",
      "Epoch 54/400\n",
      "3750/3823 [============================>.] - ETA: 0s - loss: 0.9986 - loss_rmsle: 0.9986WARNING:tensorflow:Can save best model only with <function loss_rmsle at 0x000001A08D467158> available, skipping.\n",
      "3823/3823 [==============================] - 0s 70us/sample - loss: 0.9992 - loss_rmsle: 1.0006\n",
      "Epoch 55/400\n",
      "3150/3823 [=======================>......] - ETA: 0s - loss: 0.9852 - loss_rmsle: 0.9852WARNING:tensorflow:Can save best model only with <function loss_rmsle at 0x000001A08D467158> available, skipping.\n",
      "3823/3823 [==============================] - 0s 68us/sample - loss: 0.9895 - loss_rmsle: 0.9918\n",
      "Epoch 56/400\n",
      "3800/3823 [============================>.] - ETA: 0s - loss: 0.9789 - loss_rmsle: 0.9789WARNING:tensorflow:Can save best model only with <function loss_rmsle at 0x000001A08D467158> available, skipping.\n",
      "3823/3823 [==============================] - 0s 71us/sample - loss: 0.9798 - loss_rmsle: 0.9809\n",
      "Epoch 57/400\n",
      "3150/3823 [=======================>......] - ETA: 0s - loss: 0.9737 - loss_rmsle: 0.9737WARNING:tensorflow:Can save best model only with <function loss_rmsle at 0x000001A08D467158> available, skipping.\n",
      "3823/3823 [==============================] - 0s 68us/sample - loss: 0.9702 - loss_rmsle: 0.9705\n",
      "Epoch 58/400\n",
      "3050/3823 [======================>.......] - ETA: 0s - loss: 0.9597 - loss_rmsle: 0.9597WARNING:tensorflow:Can save best model only with <function loss_rmsle at 0x000001A08D467158> available, skipping.\n",
      "3823/3823 [==============================] - 0s 69us/sample - loss: 0.9607 - loss_rmsle: 0.9600\n",
      "Epoch 59/400\n",
      "3100/3823 [=======================>......] - ETA: 0s - loss: 0.9544 - loss_rmsle: 0.9544WARNING:tensorflow:Can save best model only with <function loss_rmsle at 0x000001A08D467158> available, skipping.\n",
      "3823/3823 [==============================] - 0s 69us/sample - loss: 0.9513 - loss_rmsle: 0.9495\n",
      "Epoch 60/400\n",
      "3800/3823 [============================>.] - ETA: 0s - loss: 0.9420 - loss_rmsle: 0.9420WARNING:tensorflow:Can save best model only with <function loss_rmsle at 0x000001A08D467158> available, skipping.\n",
      "3823/3823 [==============================] - 0s 70us/sample - loss: 0.9419 - loss_rmsle: 0.9418\n",
      "Epoch 61/400\n",
      "3100/3823 [=======================>......] - ETA: 0s - loss: 0.9291 - loss_rmsle: 0.9291WARNING:tensorflow:Can save best model only with <function loss_rmsle at 0x000001A08D467158> available, skipping.\n",
      "3823/3823 [==============================] - 0s 68us/sample - loss: 0.9326 - loss_rmsle: 0.9330\n",
      "Epoch 62/400\n",
      "3650/3823 [===========================>..] - ETA: 0s - loss: 0.9267 - loss_rmsle: 0.9267WARNING:tensorflow:Can save best model only with <function loss_rmsle at 0x000001A08D467158> available, skipping.\n",
      "3823/3823 [==============================] - 0s 72us/sample - loss: 0.9234 - loss_rmsle: 0.9222\n",
      "Epoch 63/400\n",
      "3050/3823 [======================>.......] - ETA: 0s - loss: 0.9181 - loss_rmsle: 0.9181WARNING:tensorflow:Can save best model only with <function loss_rmsle at 0x000001A08D467158> available, skipping.\n",
      "3823/3823 [==============================] - 0s 70us/sample - loss: 0.9142 - loss_rmsle: 0.9124\n",
      "Epoch 64/400\n",
      "3600/3823 [===========================>..] - ETA: 0s - loss: 0.9074 - loss_rmsle: 0.9074WARNING:tensorflow:Can save best model only with <function loss_rmsle at 0x000001A08D467158> available, skipping.\n",
      "3823/3823 [==============================] - 0s 73us/sample - loss: 0.9051 - loss_rmsle: 0.9038\n",
      "Epoch 65/400\n",
      "3350/3823 [=========================>....] - ETA: 0s - loss: 0.9020 - loss_rmsle: 0.9020WARNING:tensorflow:Can save best model only with <function loss_rmsle at 0x000001A08D467158> available, skipping.\n",
      "3823/3823 [==============================] - 0s 79us/sample - loss: 0.8961 - loss_rmsle: 0.8945\n",
      "Epoch 66/400\n",
      "3800/3823 [============================>.] - ETA: 0s - loss: 0.8889 - loss_rmsle: 0.8889WARNING:tensorflow:Can save best model only with <function loss_rmsle at 0x000001A08D467158> available, skipping.\n",
      "3823/3823 [==============================] - 0s 71us/sample - loss: 0.8871 - loss_rmsle: 0.8850\n",
      "Epoch 67/400\n",
      "3800/3823 [============================>.] - ETA: 0s - loss: 0.8785 - loss_rmsle: 0.8785WARNING:tensorflow:Can save best model only with <function loss_rmsle at 0x000001A08D467158> available, skipping.\n",
      "3823/3823 [==============================] - 0s 70us/sample - loss: 0.8782 - loss_rmsle: 0.8779\n",
      "Epoch 68/400\n",
      "3800/3823 [============================>.] - ETA: 0s - loss: 0.8699 - loss_rmsle: 0.8699WARNING:tensorflow:Can save best model only with <function loss_rmsle at 0x000001A08D467158> available, skipping.\n",
      "3823/3823 [==============================] - 0s 71us/sample - loss: 0.8694 - loss_rmsle: 0.8688\n",
      "Epoch 69/400\n",
      "3750/3823 [============================>.] - ETA: 0s - loss: 0.8599 - loss_rmsle: 0.8599WARNING:tensorflow:Can save best model only with <function loss_rmsle at 0x000001A08D467158> available, skipping.\n",
      "3823/3823 [==============================] - 0s 71us/sample - loss: 0.8606 - loss_rmsle: 0.8616\n",
      "Epoch 70/400\n",
      "3750/3823 [============================>.] - ETA: 0s - loss: 0.8519 - loss_rmsle: 0.8519WARNING:tensorflow:Can save best model only with <function loss_rmsle at 0x000001A08D467158> available, skipping.\n",
      "3823/3823 [==============================] - 0s 70us/sample - loss: 0.8519 - loss_rmsle: 0.8529\n",
      "Epoch 71/400\n",
      "3250/3823 [========================>.....] - ETA: 0s - loss: 0.8418 - loss_rmsle: 0.8418WARNING:tensorflow:Can save best model only with <function loss_rmsle at 0x000001A08D467158> available, skipping.\n",
      "3823/3823 [==============================] - 0s 80us/sample - loss: 0.8432 - loss_rmsle: 0.8422\n",
      "Epoch 72/400\n",
      "3150/3823 [=======================>......] - ETA: 0s - loss: 0.8314 - loss_rmsle: 0.8314WARNING:tensorflow:Can save best model only with <function loss_rmsle at 0x000001A08D467158> available, skipping.\n",
      "3823/3823 [==============================] - 0s 69us/sample - loss: 0.8346 - loss_rmsle: 0.8356\n",
      "Epoch 73/400\n",
      "3800/3823 [============================>.] - ETA: 0s - loss: 0.8258 - loss_rmsle: 0.8258WARNING:tensorflow:Can save best model only with <function loss_rmsle at 0x000001A08D467158> available, skipping.\n",
      "3823/3823 [==============================] - 0s 70us/sample - loss: 0.8261 - loss_rmsle: 0.8265\n",
      "Epoch 74/400\n",
      "3750/3823 [============================>.] - ETA: 0s - loss: 0.8186 - loss_rmsle: 0.8186WARNING:tensorflow:Can save best model only with <function loss_rmsle at 0x000001A08D467158> available, skipping.\n",
      "3823/3823 [==============================] - 0s 72us/sample - loss: 0.8176 - loss_rmsle: 0.8179\n",
      "Epoch 75/400\n",
      "3050/3823 [======================>.......] - ETA: 0s - loss: 0.8086 - loss_rmsle: 0.8086WARNING:tensorflow:Can save best model only with <function loss_rmsle at 0x000001A08D467158> available, skipping.\n",
      "3823/3823 [==============================] - 0s 69us/sample - loss: 0.8092 - loss_rmsle: 0.8104\n",
      "Epoch 76/400\n",
      "3750/3823 [============================>.] - ETA: 0s - loss: 0.8004 - loss_rmsle: 0.8004WARNING:tensorflow:Can save best model only with <function loss_rmsle at 0x000001A08D467158> available, skipping.\n",
      "3823/3823 [==============================] - 0s 72us/sample - loss: 0.8009 - loss_rmsle: 0.8019\n",
      "Epoch 77/400\n",
      "3800/3823 [============================>.] - ETA: 0s - loss: 0.7919 - loss_rmsle: 0.7919WARNING:tensorflow:Can save best model only with <function loss_rmsle at 0x000001A08D467158> available, skipping.\n",
      "3823/3823 [==============================] - 0s 70us/sample - loss: 0.7926 - loss_rmsle: 0.7933\n",
      "Epoch 78/400\n",
      "3750/3823 [============================>.] - ETA: 0s - loss: 0.7865 - loss_rmsle: 0.7865WARNING:tensorflow:Can save best model only with <function loss_rmsle at 0x000001A08D467158> available, skipping.\n",
      "3823/3823 [==============================] - 0s 70us/sample - loss: 0.7843 - loss_rmsle: 0.7845\n",
      "Epoch 79/400\n",
      "3250/3823 [========================>.....] - ETA: 0s - loss: 0.7747 - loss_rmsle: 0.7747WARNING:tensorflow:Can save best model only with <function loss_rmsle at 0x000001A08D467158> available, skipping.\n",
      "3823/3823 [==============================] - 0s 82us/sample - loss: 0.7761 - loss_rmsle: 0.7773\n",
      "Epoch 80/400\n",
      "3750/3823 [============================>.] - ETA: 0s - loss: 0.7680 - loss_rmsle: 0.7680WARNING:tensorflow:Can save best model only with <function loss_rmsle at 0x000001A08D467158> available, skipping.\n",
      "3823/3823 [==============================] - 0s 72us/sample - loss: 0.7680 - loss_rmsle: 0.7674\n",
      "Epoch 81/400\n",
      "3350/3823 [=========================>....] - ETA: 0s - loss: 0.7647 - loss_rmsle: 0.7647WARNING:tensorflow:Can save best model only with <function loss_rmsle at 0x000001A08D467158> available, skipping.\n",
      "3823/3823 [==============================] - 0s 78us/sample - loss: 0.7599 - loss_rmsle: 0.7603\n",
      "Epoch 82/400\n",
      "3450/3823 [==========================>...] - ETA: 0s - loss: 0.7527 - loss_rmsle: 0.7527WARNING:tensorflow:Can save best model only with <function loss_rmsle at 0x000001A08D467158> available, skipping.\n",
      "3823/3823 [==============================] - 0s 79us/sample - loss: 0.7519 - loss_rmsle: 0.7526\n",
      "Epoch 83/400\n",
      "3700/3823 [============================>.] - ETA: 0s - loss: 0.7467 - loss_rmsle: 0.7467WARNING:tensorflow:Can save best model only with <function loss_rmsle at 0x000001A08D467158> available, skipping.\n",
      "3823/3823 [==============================] - 0s 72us/sample - loss: 0.7440 - loss_rmsle: 0.7430\n",
      "Epoch 84/400\n",
      "3200/3823 [========================>.....] - ETA: 0s - loss: 0.7370 - loss_rmsle: 0.7370WARNING:tensorflow:Can save best model only with <function loss_rmsle at 0x000001A08D467158> available, skipping.\n",
      "3823/3823 [==============================] - 0s 68us/sample - loss: 0.7360 - loss_rmsle: 0.7379\n",
      "Epoch 85/400\n",
      "3550/3823 [==========================>...] - ETA: 0s - loss: 0.7332 - loss_rmsle: 0.7332WARNING:tensorflow:Can save best model only with <function loss_rmsle at 0x000001A08D467158> available, skipping.\n",
      "3823/3823 [==============================] - 0s 75us/sample - loss: 0.7282 - loss_rmsle: 0.7277\n",
      "Epoch 86/400\n",
      "3700/3823 [============================>.] - ETA: 0s - loss: 0.7212 - loss_rmsle: 0.7212WARNING:tensorflow:Can save best model only with <function loss_rmsle at 0x000001A08D467158> available, skipping.\n",
      "3823/3823 [==============================] - 0s 73us/sample - loss: 0.7204 - loss_rmsle: 0.7202\n",
      "Epoch 87/400\n",
      "3150/3823 [=======================>......] - ETA: 0s - loss: 0.7059 - loss_rmsle: 0.7059WARNING:tensorflow:Can save best model only with <function loss_rmsle at 0x000001A08D467158> available, skipping.\n",
      "3823/3823 [==============================] - 0s 69us/sample - loss: 0.7126 - loss_rmsle: 0.7118\n",
      "Epoch 88/400\n",
      "3700/3823 [============================>.] - ETA: 0s - loss: 0.7041 - loss_rmsle: 0.7041WARNING:tensorflow:Can save best model only with <function loss_rmsle at 0x000001A08D467158> available, skipping.\n",
      "3823/3823 [==============================] - 0s 72us/sample - loss: 0.7049 - loss_rmsle: 0.7040\n",
      "Epoch 89/400\n",
      "3150/3823 [=======================>......] - ETA: 0s - loss: 0.6991 - loss_rmsle: 0.6991WARNING:tensorflow:Can save best model only with <function loss_rmsle at 0x000001A08D467158> available, skipping.\n",
      "3823/3823 [==============================] - 0s 69us/sample - loss: 0.6972 - loss_rmsle: 0.6962\n",
      "Epoch 90/400\n",
      "3650/3823 [===========================>..] - ETA: 0s - loss: 0.6942 - loss_rmsle: 0.6942WARNING:tensorflow:Can save best model only with <function loss_rmsle at 0x000001A08D467158> available, skipping.\n",
      "3823/3823 [==============================] - 0s 73us/sample - loss: 0.6896 - loss_rmsle: 0.6874\n",
      "Epoch 91/400\n",
      "3350/3823 [=========================>....] - ETA: 0s - loss: 0.6810 - loss_rmsle: 0.6810WARNING:tensorflow:Can save best model only with <function loss_rmsle at 0x000001A08D467158> available, skipping.\n",
      "3823/3823 [==============================] - 0s 78us/sample - loss: 0.6821 - loss_rmsle: 0.6827\n",
      "Epoch 92/400\n",
      "3650/3823 [===========================>..] - ETA: 0s - loss: 0.6705 - loss_rmsle: 0.6705WARNING:tensorflow:Can save best model only with <function loss_rmsle at 0x000001A08D467158> available, skipping.\n",
      "3823/3823 [==============================] - 0s 73us/sample - loss: 0.6746 - loss_rmsle: 0.6750\n",
      "Epoch 93/400\n",
      "3750/3823 [============================>.] - ETA: 0s - loss: 0.6674 - loss_rmsle: 0.6674WARNING:tensorflow:Can save best model only with <function loss_rmsle at 0x000001A08D467158> available, skipping.\n",
      "3823/3823 [==============================] - 0s 72us/sample - loss: 0.6671 - loss_rmsle: 0.6661\n",
      "Epoch 94/400\n",
      "3700/3823 [============================>.] - ETA: 0s - loss: 0.6608 - loss_rmsle: 0.6608WARNING:tensorflow:Can save best model only with <function loss_rmsle at 0x000001A08D467158> available, skipping.\n",
      "3823/3823 [==============================] - 0s 73us/sample - loss: 0.6597 - loss_rmsle: 0.6596\n",
      "Epoch 95/400\n",
      "3800/3823 [============================>.] - ETA: 0s - loss: 0.6522 - loss_rmsle: 0.6522WARNING:tensorflow:Can save best model only with <function loss_rmsle at 0x000001A08D467158> available, skipping.\n",
      "3823/3823 [==============================] - 0s 70us/sample - loss: 0.6523 - loss_rmsle: 0.6525\n",
      "Epoch 96/400\n",
      "3800/3823 [============================>.] - ETA: 0s - loss: 0.6428 - loss_rmsle: 0.6428WARNING:tensorflow:Can save best model only with <function loss_rmsle at 0x000001A08D467158> available, skipping.\n",
      "3823/3823 [==============================] - 0s 70us/sample - loss: 0.6450 - loss_rmsle: 0.6476\n",
      "Epoch 97/400\n",
      "3100/3823 [=======================>......] - ETA: 0s - loss: 0.6418 - loss_rmsle: 0.6418WARNING:tensorflow:Can save best model only with <function loss_rmsle at 0x000001A08D467158> available, skipping.\n",
      "3823/3823 [==============================] - 0s 69us/sample - loss: 0.6377 - loss_rmsle: 0.6374\n",
      "Epoch 98/400\n",
      "3550/3823 [==========================>...] - ETA: 0s - loss: 0.6301 - loss_rmsle: 0.6301WARNING:tensorflow:Can save best model only with <function loss_rmsle at 0x000001A08D467158> available, skipping.\n",
      "3823/3823 [==============================] - 0s 76us/sample - loss: 0.6305 - loss_rmsle: 0.6297\n",
      "Epoch 99/400\n",
      "3650/3823 [===========================>..] - ETA: 0s - loss: 0.6204 - loss_rmsle: 0.6204WARNING:tensorflow:Can save best model only with <function loss_rmsle at 0x000001A08D467158> available, skipping.\n",
      "3823/3823 [==============================] - 0s 74us/sample - loss: 0.6233 - loss_rmsle: 0.6239\n",
      "Epoch 100/400\n",
      "3600/3823 [===========================>..] - ETA: 0s - loss: 0.6145 - loss_rmsle: 0.6145WARNING:tensorflow:Can save best model only with <function loss_rmsle at 0x000001A08D467158> available, skipping.\n",
      "3823/3823 [==============================] - 0s 75us/sample - loss: 0.6162 - loss_rmsle: 0.6158\n",
      "Epoch 101/400\n",
      "3550/3823 [==========================>...] - ETA: 0s - loss: 0.6089 - loss_rmsle: 0.6089WARNING:tensorflow:Can save best model only with <function loss_rmsle at 0x000001A08D467158> available, skipping.\n",
      "3823/3823 [==============================] - 0s 78us/sample - loss: 0.6091 - loss_rmsle: 0.6071\n",
      "Epoch 102/400\n",
      "3750/3823 [============================>.] - ETA: 0s - loss: 0.6031 - loss_rmsle: 0.6031WARNING:tensorflow:Can save best model only with <function loss_rmsle at 0x000001A08D467158> available, skipping.\n",
      "3823/3823 [==============================] - 0s 86us/sample - loss: 0.6020 - loss_rmsle: 0.6016\n",
      "Epoch 103/400\n",
      "3800/3823 [============================>.] - ETA: 0s - loss: 0.5939 - loss_rmsle: 0.5939WARNING:tensorflow:Can save best model only with <function loss_rmsle at 0x000001A08D467158> available, skipping.\n",
      "3823/3823 [==============================] - 0s 70us/sample - loss: 0.5950 - loss_rmsle: 0.5962\n",
      "Epoch 104/400\n",
      "3800/3823 [============================>.] - ETA: 0s - loss: 0.5886 - loss_rmsle: 0.5886WARNING:tensorflow:Can save best model only with <function loss_rmsle at 0x000001A08D467158> available, skipping.\n",
      "3823/3823 [==============================] - 0s 70us/sample - loss: 0.5880 - loss_rmsle: 0.5873\n",
      "Epoch 105/400\n",
      "3800/3823 [============================>.] - ETA: 0s - loss: 0.5816 - loss_rmsle: 0.5816WARNING:tensorflow:Can save best model only with <function loss_rmsle at 0x000001A08D467158> available, skipping.\n",
      "3823/3823 [==============================] - 0s 72us/sample - loss: 0.5811 - loss_rmsle: 0.5805\n",
      "Epoch 106/400\n",
      "3650/3823 [===========================>..] - ETA: 0s - loss: 0.5753 - loss_rmsle: 0.5753WARNING:tensorflow:Can save best model only with <function loss_rmsle at 0x000001A08D467158> available, skipping.\n",
      "3823/3823 [==============================] - 0s 72us/sample - loss: 0.5742 - loss_rmsle: 0.5733\n",
      "Epoch 107/400\n",
      "3750/3823 [============================>.] - ETA: 0s - loss: 0.5670 - loss_rmsle: 0.5670WARNING:tensorflow:Can save best model only with <function loss_rmsle at 0x000001A08D467158> available, skipping.\n",
      "3823/3823 [==============================] - 0s 71us/sample - loss: 0.5674 - loss_rmsle: 0.5676\n",
      "Epoch 108/400\n",
      "3100/3823 [=======================>......] - ETA: 0s - loss: 0.5555 - loss_rmsle: 0.5555WARNING:tensorflow:Can save best model only with <function loss_rmsle at 0x000001A08D467158> available, skipping.\n",
      "3823/3823 [==============================] - 0s 69us/sample - loss: 0.5606 - loss_rmsle: 0.5610\n",
      "Epoch 109/400\n",
      "3550/3823 [==========================>...] - ETA: 0s - loss: 0.5492 - loss_rmsle: 0.5492WARNING:tensorflow:Can save best model only with <function loss_rmsle at 0x000001A08D467158> available, skipping.\n",
      "3823/3823 [==============================] - 0s 74us/sample - loss: 0.5538 - loss_rmsle: 0.5553\n",
      "Epoch 110/400\n",
      "3700/3823 [============================>.] - ETA: 0s - loss: 0.5458 - loss_rmsle: 0.5458WARNING:tensorflow:Can save best model only with <function loss_rmsle at 0x000001A08D467158> available, skipping.\n",
      "3823/3823 [==============================] - 0s 74us/sample - loss: 0.5471 - loss_rmsle: 0.5478\n",
      "Epoch 111/400\n",
      "3100/3823 [=======================>......] - ETA: 0s - loss: 0.5356 - loss_rmsle: 0.5356WARNING:tensorflow:Can save best model only with <function loss_rmsle at 0x000001A08D467158> available, skipping.\n",
      "3823/3823 [==============================] - 0s 71us/sample - loss: 0.5404 - loss_rmsle: 0.5405\n",
      "Epoch 112/400\n",
      "3300/3823 [========================>.....] - ETA: 0s - loss: 0.5267 - loss_rmsle: 0.5267WARNING:tensorflow:Can save best model only with <function loss_rmsle at 0x000001A08D467158> available, skipping.\n",
      "3823/3823 [==============================] - 0s 81us/sample - loss: 0.5337 - loss_rmsle: 0.5340\n",
      "Epoch 113/400\n",
      "3700/3823 [============================>.] - ETA: 0s - loss: 0.5275 - loss_rmsle: 0.5275WARNING:tensorflow:Can save best model only with <function loss_rmsle at 0x000001A08D467158> available, skipping.\n",
      "3823/3823 [==============================] - 0s 73us/sample - loss: 0.5271 - loss_rmsle: 0.5268\n",
      "Epoch 114/400\n",
      "3700/3823 [============================>.] - ETA: 0s - loss: 0.5179 - loss_rmsle: 0.5179WARNING:tensorflow:Can save best model only with <function loss_rmsle at 0x000001A08D467158> available, skipping.\n",
      "3823/3823 [==============================] - 0s 72us/sample - loss: 0.5206 - loss_rmsle: 0.5218\n",
      "Epoch 115/400\n",
      "3750/3823 [============================>.] - ETA: 0s - loss: 0.5138 - loss_rmsle: 0.5138WARNING:tensorflow:Can save best model only with <function loss_rmsle at 0x000001A08D467158> available, skipping.\n",
      "3823/3823 [==============================] - 0s 72us/sample - loss: 0.5140 - loss_rmsle: 0.5149\n",
      "Epoch 116/400\n",
      "3550/3823 [==========================>...] - ETA: 0s - loss: 0.5035 - loss_rmsle: 0.5035WARNING:tensorflow:Can save best model only with <function loss_rmsle at 0x000001A08D467158> available, skipping.\n",
      "3823/3823 [==============================] - 0s 76us/sample - loss: 0.5075 - loss_rmsle: 0.5080\n",
      "Epoch 117/400\n",
      "3400/3823 [=========================>....] - ETA: 0s - loss: 0.4977 - loss_rmsle: 0.4977WARNING:tensorflow:Can save best model only with <function loss_rmsle at 0x000001A08D467158> available, skipping.\n",
      "3823/3823 [==============================] - 0s 79us/sample - loss: 0.5011 - loss_rmsle: 0.5003\n",
      "Epoch 118/400\n",
      "3450/3823 [==========================>...] - ETA: 0s - loss: 0.4977 - loss_rmsle: 0.4977WARNING:tensorflow:Can save best model only with <function loss_rmsle at 0x000001A08D467158> available, skipping.\n",
      "3823/3823 [==============================] - 0s 77us/sample - loss: 0.4947 - loss_rmsle: 0.4924\n",
      "Epoch 119/400\n",
      "3650/3823 [===========================>..] - ETA: 0s - loss: 0.4879 - loss_rmsle: 0.4879WARNING:tensorflow:Can save best model only with <function loss_rmsle at 0x000001A08D467158> available, skipping.\n",
      "3823/3823 [==============================] - 0s 73us/sample - loss: 0.4883 - loss_rmsle: 0.4894\n",
      "Epoch 120/400\n",
      "3800/3823 [============================>.] - ETA: 0s - loss: 0.4826 - loss_rmsle: 0.4826WARNING:tensorflow:Can save best model only with <function loss_rmsle at 0x000001A08D467158> available, skipping.\n",
      "3823/3823 [==============================] - 0s 70us/sample - loss: 0.4819 - loss_rmsle: 0.4811\n",
      "Epoch 121/400\n",
      "3300/3823 [========================>.....] - ETA: 0s - loss: 0.4774 - loss_rmsle: 0.4774WARNING:tensorflow:Can save best model only with <function loss_rmsle at 0x000001A08D467158> available, skipping.\n",
      "3823/3823 [==============================] - 0s 81us/sample - loss: 0.4756 - loss_rmsle: 0.4758\n",
      "Epoch 122/400\n",
      "3550/3823 [==========================>...] - ETA: 0s - loss: 0.4686 - loss_rmsle: 0.4686WARNING:tensorflow:Can save best model only with <function loss_rmsle at 0x000001A08D467158> available, skipping.\n",
      "3823/3823 [==============================] - 0s 74us/sample - loss: 0.4693 - loss_rmsle: 0.4697\n",
      "Epoch 123/400\n",
      "3500/3823 [==========================>...] - ETA: 0s - loss: 0.4638 - loss_rmsle: 0.4638WARNING:tensorflow:Can save best model only with <function loss_rmsle at 0x000001A08D467158> available, skipping.\n",
      "3823/3823 [==============================] - 0s 78us/sample - loss: 0.4631 - loss_rmsle: 0.4628\n",
      "Epoch 124/400\n",
      "3550/3823 [==========================>...] - ETA: 0s - loss: 0.4560 - loss_rmsle: 0.4560WARNING:tensorflow:Can save best model only with <function loss_rmsle at 0x000001A08D467158> available, skipping.\n",
      "3823/3823 [==============================] - 0s 75us/sample - loss: 0.4569 - loss_rmsle: 0.4584\n",
      "Epoch 125/400\n",
      "3100/3823 [=======================>......] - ETA: 0s - loss: 0.4552 - loss_rmsle: 0.4552WARNING:tensorflow:Can save best model only with <function loss_rmsle at 0x000001A08D467158> available, skipping.\n",
      "3823/3823 [==============================] - 0s 69us/sample - loss: 0.4507 - loss_rmsle: 0.4487\n",
      "Epoch 126/400\n",
      "3050/3823 [======================>.......] - ETA: 0s - loss: 0.4414 - loss_rmsle: 0.4414WARNING:tensorflow:Can save best model only with <function loss_rmsle at 0x000001A08D467158> available, skipping.\n",
      "3823/3823 [==============================] - 0s 71us/sample - loss: 0.4446 - loss_rmsle: 0.4460\n",
      "Epoch 127/400\n",
      "3750/3823 [============================>.] - ETA: 0s - loss: 0.4382 - loss_rmsle: 0.4382WARNING:tensorflow:Can save best model only with <function loss_rmsle at 0x000001A08D467158> available, skipping.\n",
      "3823/3823 [==============================] - 0s 71us/sample - loss: 0.4385 - loss_rmsle: 0.4389\n",
      "Epoch 128/400\n",
      "3600/3823 [===========================>..] - ETA: 0s - loss: 0.4310 - loss_rmsle: 0.4310WARNING:tensorflow:Can save best model only with <function loss_rmsle at 0x000001A08D467158> available, skipping.\n",
      "3823/3823 [==============================] - 0s 74us/sample - loss: 0.4324 - loss_rmsle: 0.4326\n",
      "Epoch 129/400\n",
      "3800/3823 [============================>.] - ETA: 0s - loss: 0.4251 - loss_rmsle: 0.4251WARNING:tensorflow:Can save best model only with <function loss_rmsle at 0x000001A08D467158> available, skipping.\n",
      "3823/3823 [==============================] - 0s 70us/sample - loss: 0.4264 - loss_rmsle: 0.4278\n",
      "Epoch 130/400\n",
      "3350/3823 [=========================>....] - ETA: 0s - loss: 0.4199 - loss_rmsle: 0.4199WARNING:tensorflow:Can save best model only with <function loss_rmsle at 0x000001A08D467158> available, skipping.\n",
      "3823/3823 [==============================] - 0s 82us/sample - loss: 0.4203 - loss_rmsle: 0.4201\n",
      "Epoch 131/400\n",
      "3750/3823 [============================>.] - ETA: 0s - loss: 0.4155 - loss_rmsle: 0.4155WARNING:tensorflow:Can save best model only with <function loss_rmsle at 0x000001A08D467158> available, skipping.\n",
      "3823/3823 [==============================] - 0s 71us/sample - loss: 0.4144 - loss_rmsle: 0.4140\n",
      "Epoch 132/400\n",
      "3800/3823 [============================>.] - ETA: 0s - loss: 0.4079 - loss_rmsle: 0.4079WARNING:tensorflow:Can save best model only with <function loss_rmsle at 0x000001A08D467158> available, skipping.\n",
      "3823/3823 [==============================] - 0s 71us/sample - loss: 0.4084 - loss_rmsle: 0.4090\n",
      "Epoch 133/400\n",
      "3700/3823 [============================>.] - ETA: 0s - loss: 0.4036 - loss_rmsle: 0.4036WARNING:tensorflow:Can save best model only with <function loss_rmsle at 0x000001A08D467158> available, skipping.\n",
      "3823/3823 [==============================] - 0s 72us/sample - loss: 0.4025 - loss_rmsle: 0.4005\n",
      "Epoch 134/400\n",
      "3500/3823 [==========================>...] - ETA: 0s - loss: 0.4011 - loss_rmsle: 0.4011WARNING:tensorflow:Can save best model only with <function loss_rmsle at 0x000001A08D467158> available, skipping.\n",
      "3823/3823 [==============================] - 0s 77us/sample - loss: 0.3966 - loss_rmsle: 0.3977\n",
      "Epoch 135/400\n",
      "3400/3823 [=========================>....] - ETA: 0s - loss: 0.3966 - loss_rmsle: 0.3966WARNING:tensorflow:Can save best model only with <function loss_rmsle at 0x000001A08D467158> available, skipping.\n",
      "3823/3823 [==============================] - 0s 78us/sample - loss: 0.3908 - loss_rmsle: 0.3909\n",
      "Epoch 136/400\n",
      "3600/3823 [===========================>..] - ETA: 0s - loss: 0.3851 - loss_rmsle: 0.3851WARNING:tensorflow:Can save best model only with <function loss_rmsle at 0x000001A08D467158> available, skipping.\n",
      "3823/3823 [==============================] - 0s 74us/sample - loss: 0.3850 - loss_rmsle: 0.3861\n",
      "Epoch 137/400\n",
      "3650/3823 [===========================>..] - ETA: 0s - loss: 0.3775 - loss_rmsle: 0.3775WARNING:tensorflow:Can save best model only with <function loss_rmsle at 0x000001A08D467158> available, skipping.\n",
      "3823/3823 [==============================] - 0s 73us/sample - loss: 0.3792 - loss_rmsle: 0.3791\n",
      "Epoch 138/400\n",
      "3700/3823 [============================>.] - ETA: 0s - loss: 0.3783 - loss_rmsle: 0.3783WARNING:tensorflow:Can save best model only with <function loss_rmsle at 0x000001A08D467158> available, skipping.\n",
      "3823/3823 [==============================] - 0s 72us/sample - loss: 0.3734 - loss_rmsle: 0.3731\n",
      "Epoch 139/400\n",
      "3500/3823 [==========================>...] - ETA: 0s - loss: 0.3647 - loss_rmsle: 0.3647WARNING:tensorflow:Can save best model only with <function loss_rmsle at 0x000001A08D467158> available, skipping.\n",
      "3823/3823 [==============================] - 0s 79us/sample - loss: 0.3677 - loss_rmsle: 0.3693\n",
      "Epoch 140/400\n",
      "3150/3823 [=======================>......] - ETA: 0s - loss: 0.3552 - loss_rmsle: 0.3552WARNING:tensorflow:Can save best model only with <function loss_rmsle at 0x000001A08D467158> available, skipping.\n",
      "3823/3823 [==============================] - 0s 68us/sample - loss: 0.3620 - loss_rmsle: 0.3623\n",
      "Epoch 141/400\n",
      "3100/3823 [=======================>......] - ETA: 0s - loss: 0.3488 - loss_rmsle: 0.3488WARNING:tensorflow:Can save best model only with <function loss_rmsle at 0x000001A08D467158> available, skipping.\n",
      "3823/3823 [==============================] - 0s 69us/sample - loss: 0.3563 - loss_rmsle: 0.3570\n",
      "Epoch 142/400\n",
      "3700/3823 [============================>.] - ETA: 0s - loss: 0.3497 - loss_rmsle: 0.3497WARNING:tensorflow:Can save best model only with <function loss_rmsle at 0x000001A08D467158> available, skipping.\n",
      "3823/3823 [==============================] - 0s 72us/sample - loss: 0.3507 - loss_rmsle: 0.3521\n",
      "Epoch 143/400\n",
      "3800/3823 [============================>.] - ETA: 0s - loss: 0.3439 - loss_rmsle: 0.3439WARNING:tensorflow:Can save best model only with <function loss_rmsle at 0x000001A08D467158> available, skipping.\n",
      "3823/3823 [==============================] - 0s 71us/sample - loss: 0.3451 - loss_rmsle: 0.3463\n",
      "Epoch 144/400\n",
      "3800/3823 [============================>.] - ETA: 0s - loss: 0.3398 - loss_rmsle: 0.3398WARNING:tensorflow:Can save best model only with <function loss_rmsle at 0x000001A08D467158> available, skipping.\n",
      "3823/3823 [==============================] - 0s 70us/sample - loss: 0.3395 - loss_rmsle: 0.3391\n",
      "Epoch 145/400\n",
      "3750/3823 [============================>.] - ETA: 0s - loss: 0.3346 - loss_rmsle: 0.3346WARNING:tensorflow:Can save best model only with <function loss_rmsle at 0x000001A08D467158> available, skipping.\n",
      "3823/3823 [==============================] - 0s 72us/sample - loss: 0.3340 - loss_rmsle: 0.3337\n",
      "Epoch 146/400\n",
      "3200/3823 [========================>.....] - ETA: 0s - loss: 0.3262 - loss_rmsle: 0.3262WARNING:tensorflow:Can save best model only with <function loss_rmsle at 0x000001A08D467158> available, skipping.\n",
      "3823/3823 [==============================] - 0s 73us/sample - loss: 0.3285 - loss_rmsle: 0.3290\n",
      "Epoch 147/400\n",
      "3700/3823 [============================>.] - ETA: 0s - loss: 0.3245 - loss_rmsle: 0.3245WARNING:tensorflow:Can save best model only with <function loss_rmsle at 0x000001A08D467158> available, skipping.\n",
      "3823/3823 [==============================] - 0s 73us/sample - loss: 0.3230 - loss_rmsle: 0.3231\n",
      "Epoch 148/400\n",
      "3750/3823 [============================>.] - ETA: 0s - loss: 0.3182 - loss_rmsle: 0.3182WARNING:tensorflow:Can save best model only with <function loss_rmsle at 0x000001A08D467158> available, skipping.\n",
      "3823/3823 [==============================] - 0s 71us/sample - loss: 0.3175 - loss_rmsle: 0.3174\n",
      "Epoch 149/400\n",
      "3150/3823 [=======================>......] - ETA: 0s - loss: 0.3119 - loss_rmsle: 0.3119WARNING:tensorflow:Can save best model only with <function loss_rmsle at 0x000001A08D467158> available, skipping.\n",
      "3823/3823 [==============================] - 0s 69us/sample - loss: 0.3121 - loss_rmsle: 0.3109\n",
      "Epoch 150/400\n",
      "3150/3823 [=======================>......] - ETA: 0s - loss: 0.3150 - loss_rmsle: 0.3150WARNING:tensorflow:Can save best model only with <function loss_rmsle at 0x000001A08D467158> available, skipping.\n",
      "3823/3823 [==============================] - 0s 68us/sample - loss: 0.3067 - loss_rmsle: 0.3079\n",
      "Epoch 151/400\n",
      "3700/3823 [============================>.] - ETA: 0s - loss: 0.2990 - loss_rmsle: 0.2990WARNING:tensorflow:Can save best model only with <function loss_rmsle at 0x000001A08D467158> available, skipping.\n",
      "3823/3823 [==============================] - 0s 72us/sample - loss: 0.3013 - loss_rmsle: 0.3015\n",
      "Epoch 152/400\n",
      "3550/3823 [==========================>...] - ETA: 0s - loss: 0.2980 - loss_rmsle: 0.2980WARNING:tensorflow:Can save best model only with <function loss_rmsle at 0x000001A08D467158> available, skipping.\n",
      "3823/3823 [==============================] - 0s 75us/sample - loss: 0.2959 - loss_rmsle: 0.2948\n",
      "Epoch 153/400\n",
      "3700/3823 [============================>.] - ETA: 0s - loss: 0.2903 - loss_rmsle: 0.2903WARNING:tensorflow:Can save best model only with <function loss_rmsle at 0x000001A08D467158> available, skipping.\n",
      "3823/3823 [==============================] - 0s 73us/sample - loss: 0.2906 - loss_rmsle: 0.2916\n",
      "Epoch 154/400\n",
      "3700/3823 [============================>.] - ETA: 0s - loss: 0.2875 - loss_rmsle: 0.2875WARNING:tensorflow:Can save best model only with <function loss_rmsle at 0x000001A08D467158> available, skipping.\n",
      "3823/3823 [==============================] - 0s 88us/sample - loss: 0.2856 - loss_rmsle: 0.2855\n",
      "Epoch 155/400\n",
      "3100/3823 [=======================>......] - ETA: 0s - loss: 0.2854 - loss_rmsle: 0.2854WARNING:tensorflow:Can save best model only with <function loss_rmsle at 0x000001A08D467158> available, skipping.\n",
      "3823/3823 [==============================] - 0s 69us/sample - loss: 0.2801 - loss_rmsle: 0.2786\n",
      "Epoch 156/400\n",
      "3150/3823 [=======================>......] - ETA: 0s - loss: 0.2695 - loss_rmsle: 0.2695WARNING:tensorflow:Can save best model only with <function loss_rmsle at 0x000001A08D467158> available, skipping.\n",
      "3823/3823 [==============================] - 0s 69us/sample - loss: 0.2748 - loss_rmsle: 0.2748\n",
      "Epoch 157/400\n",
      "3800/3823 [============================>.] - ETA: 0s - loss: 0.2702 - loss_rmsle: 0.2702WARNING:tensorflow:Can save best model only with <function loss_rmsle at 0x000001A08D467158> available, skipping.\n",
      "3823/3823 [==============================] - 0s 70us/sample - loss: 0.2696 - loss_rmsle: 0.2688\n",
      "Epoch 158/400\n",
      "3800/3823 [============================>.] - ETA: 0s - loss: 0.2643 - loss_rmsle: 0.2643WARNING:tensorflow:Can save best model only with <function loss_rmsle at 0x000001A08D467158> available, skipping.\n",
      "3823/3823 [==============================] - 0s 71us/sample - loss: 0.2644 - loss_rmsle: 0.2644\n",
      "Epoch 159/400\n",
      "3800/3823 [============================>.] - ETA: 0s - loss: 0.2602 - loss_rmsle: 0.2602WARNING:tensorflow:Can save best model only with <function loss_rmsle at 0x000001A08D467158> available, skipping.\n",
      "3823/3823 [==============================] - 0s 72us/sample - loss: 0.2592 - loss_rmsle: 0.2580\n",
      "Epoch 160/400\n",
      "3750/3823 [============================>.] - ETA: 0s - loss: 0.2554 - loss_rmsle: 0.2554WARNING:tensorflow:Can save best model only with <function loss_rmsle at 0x000001A08D467158> available, skipping.\n",
      "3823/3823 [==============================] - 0s 72us/sample - loss: 0.2540 - loss_rmsle: 0.2537\n",
      "Epoch 161/400\n",
      "3400/3823 [=========================>....] - ETA: 0s - loss: 0.2538 - loss_rmsle: 0.2538WARNING:tensorflow:Can save best model only with <function loss_rmsle at 0x000001A08D467158> available, skipping.\n",
      "3823/3823 [==============================] - 0s 79us/sample - loss: 0.2489 - loss_rmsle: 0.2485\n",
      "Epoch 162/400\n",
      "3150/3823 [=======================>......] - ETA: 0s - loss: 0.2431 - loss_rmsle: 0.2431WARNING:tensorflow:Can save best model only with <function loss_rmsle at 0x000001A08D467158> available, skipping.\n",
      "3823/3823 [==============================] - 0s 69us/sample - loss: 0.2445 - loss_rmsle: 0.2443\n",
      "Epoch 163/400\n",
      "3800/3823 [============================>.] - ETA: 0s - loss: 0.2395 - loss_rmsle: 0.2395WARNING:tensorflow:Can save best model only with <function loss_rmsle at 0x000001A08D467158> available, skipping.\n",
      "3823/3823 [==============================] - 0s 71us/sample - loss: 0.2395 - loss_rmsle: 0.2396\n",
      "Epoch 164/400\n",
      "3100/3823 [=======================>......] - ETA: 0s - loss: 0.2386 - loss_rmsle: 0.2386WARNING:tensorflow:Can save best model only with <function loss_rmsle at 0x000001A08D467158> available, skipping.\n",
      "3823/3823 [==============================] - 0s 69us/sample - loss: 0.2338 - loss_rmsle: 0.2337\n",
      "Epoch 165/400\n",
      "3800/3823 [============================>.] - ETA: 0s - loss: 0.2289 - loss_rmsle: 0.2289WARNING:tensorflow:Can save best model only with <function loss_rmsle at 0x000001A08D467158> available, skipping.\n",
      "3823/3823 [==============================] - 0s 70us/sample - loss: 0.2288 - loss_rmsle: 0.2287\n",
      "Epoch 166/400\n",
      "3100/3823 [=======================>......] - ETA: 0s - loss: 0.2290 - loss_rmsle: 0.2290WARNING:tensorflow:Can save best model only with <function loss_rmsle at 0x000001A08D467158> available, skipping.\n",
      "3823/3823 [==============================] - 0s 70us/sample - loss: 0.2238 - loss_rmsle: 0.2247\n",
      "Epoch 167/400\n",
      "3450/3823 [==========================>...] - ETA: 0s - loss: 0.2216 - loss_rmsle: 0.2216WARNING:tensorflow:Can save best model only with <function loss_rmsle at 0x000001A08D467158> available, skipping.\n",
      "3823/3823 [==============================] - 0s 82us/sample - loss: 0.2225 - loss_rmsle: 0.2233\n",
      "Epoch 168/400\n",
      "3750/3823 [============================>.] - ETA: 0s - loss: 0.2138 - loss_rmsle: 0.2138WARNING:tensorflow:Can save best model only with <function loss_rmsle at 0x000001A08D467158> available, skipping.\n",
      "3823/3823 [==============================] - 0s 74us/sample - loss: 0.2146 - loss_rmsle: 0.2153\n",
      "Epoch 169/400\n",
      "3750/3823 [============================>.] - ETA: 0s - loss: 0.2126 - loss_rmsle: 0.2126WARNING:tensorflow:Can save best model only with <function loss_rmsle at 0x000001A08D467158> available, skipping.\n",
      "3823/3823 [==============================] - 0s 72us/sample - loss: 0.2127 - loss_rmsle: 0.2128\n",
      "Epoch 170/400\n",
      "3650/3823 [===========================>..] - ETA: 0s - loss: 0.2031 - loss_rmsle: 0.2031WARNING:tensorflow:Can save best model only with <function loss_rmsle at 0x000001A08D467158> available, skipping.\n",
      "3823/3823 [==============================] - 0s 73us/sample - loss: 0.2094 - loss_rmsle: 0.2100\n",
      "Epoch 171/400\n",
      "3700/3823 [============================>.] - ETA: 0s - loss: 0.2016 - loss_rmsle: 0.2016WARNING:tensorflow:Can save best model only with <function loss_rmsle at 0x000001A08D467158> available, skipping.\n",
      "3823/3823 [==============================] - 0s 73us/sample - loss: 0.2026 - loss_rmsle: 0.2017\n",
      "Epoch 172/400\n",
      "3650/3823 [===========================>..] - ETA: 0s - loss: 0.2002 - loss_rmsle: 0.2002WARNING:tensorflow:Can save best model only with <function loss_rmsle at 0x000001A08D467158> available, skipping.\n",
      "3823/3823 [==============================] - 0s 73us/sample - loss: 0.2004 - loss_rmsle: 0.2011\n",
      "Epoch 173/400\n",
      "3800/3823 [============================>.] - ETA: 0s - loss: 0.1929 - loss_rmsle: 0.1929WARNING:tensorflow:Can save best model only with <function loss_rmsle at 0x000001A08D467158> available, skipping.\n",
      "3823/3823 [==============================] - 0s 71us/sample - loss: 0.1937 - loss_rmsle: 0.1945\n",
      "Epoch 174/400\n",
      "3650/3823 [===========================>..] - ETA: 0s - loss: 0.1903 - loss_rmsle: 0.1903WARNING:tensorflow:Can save best model only with <function loss_rmsle at 0x000001A08D467158> available, skipping.\n",
      "3823/3823 [==============================] - 0s 76us/sample - loss: 0.1911 - loss_rmsle: 0.1918\n",
      "Epoch 175/400\n",
      "3750/3823 [============================>.] - ETA: 0s - loss: 0.1877 - loss_rmsle: 0.1877WARNING:tensorflow:Can save best model only with <function loss_rmsle at 0x000001A08D467158> available, skipping.\n",
      "3823/3823 [==============================] - 0s 73us/sample - loss: 0.1866 - loss_rmsle: 0.1857\n",
      "Epoch 176/400\n",
      "3700/3823 [============================>.] - ETA: 0s - loss: 0.1786 - loss_rmsle: 0.1786WARNING:tensorflow:Can save best model only with <function loss_rmsle at 0x000001A08D467158> available, skipping.\n",
      "3823/3823 [==============================] - 0s 71us/sample - loss: 0.1795 - loss_rmsle: 0.1788\n",
      "Epoch 177/400\n",
      "3550/3823 [==========================>...] - ETA: 0s - loss: 0.1750 - loss_rmsle: 0.1750WARNING:tensorflow:Can save best model only with <function loss_rmsle at 0x000001A08D467158> available, skipping.\n",
      "3823/3823 [==============================] - 0s 76us/sample - loss: 0.1778 - loss_rmsle: 0.1767\n",
      "Epoch 178/400\n",
      "3100/3823 [=======================>......] - ETA: 0s - loss: 0.1649 - loss_rmsle: 0.1649WARNING:tensorflow:Can save best model only with <function loss_rmsle at 0x000001A08D467158> available, skipping.\n",
      "3823/3823 [==============================] - 0s 70us/sample - loss: 0.1760 - loss_rmsle: 0.1753\n",
      "Epoch 179/400\n",
      "3100/3823 [=======================>......] - ETA: 0s - loss: 0.1717 - loss_rmsle: 0.1717WARNING:tensorflow:Can save best model only with <function loss_rmsle at 0x000001A08D467158> available, skipping.\n",
      "3823/3823 [==============================] - 0s 69us/sample - loss: 0.1663 - loss_rmsle: 0.1654\n",
      "Epoch 180/400\n",
      "3750/3823 [============================>.] - ETA: 0s - loss: 0.1655 - loss_rmsle: 0.1655WARNING:tensorflow:Can save best model only with <function loss_rmsle at 0x000001A08D467158> available, skipping.\n",
      "3823/3823 [==============================] - 0s 72us/sample - loss: 0.1664 - loss_rmsle: 0.1665\n",
      "Epoch 181/400\n",
      "3350/3823 [=========================>....] - ETA: 0s - loss: 0.1578 - loss_rmsle: 0.1578WARNING:tensorflow:Can save best model only with <function loss_rmsle at 0x000001A08D467158> available, skipping.\n",
      "3823/3823 [==============================] - 0s 81us/sample - loss: 0.1622 - loss_rmsle: 0.1635\n",
      "Epoch 182/400\n",
      "3100/3823 [=======================>......] - ETA: 0s - loss: 0.1524 - loss_rmsle: 0.1524WARNING:tensorflow:Can save best model only with <function loss_rmsle at 0x000001A08D467158> available, skipping.\n",
      "3823/3823 [==============================] - 0s 70us/sample - loss: 0.1575 - loss_rmsle: 0.1573\n",
      "Epoch 183/400\n",
      "3750/3823 [============================>.] - ETA: 0s - loss: 0.1558 - loss_rmsle: 0.1558WARNING:tensorflow:Can save best model only with <function loss_rmsle at 0x000001A08D467158> available, skipping.\n",
      "3823/3823 [==============================] - 0s 71us/sample - loss: 0.1566 - loss_rmsle: 0.1570\n",
      "Epoch 184/400\n",
      "3700/3823 [============================>.] - ETA: 0s - loss: 0.1470 - loss_rmsle: 0.1470WARNING:tensorflow:Can save best model only with <function loss_rmsle at 0x000001A08D467158> available, skipping.\n",
      "3823/3823 [==============================] - 0s 73us/sample - loss: 0.1467 - loss_rmsle: 0.1465\n",
      "Epoch 185/400\n",
      "3800/3823 [============================>.] - ETA: 0s - loss: 0.1417 - loss_rmsle: 0.1417WARNING:tensorflow:Can save best model only with <function loss_rmsle at 0x000001A08D467158> available, skipping.\n",
      "3823/3823 [==============================] - 0s 70us/sample - loss: 0.1428 - loss_rmsle: 0.1440\n",
      "Epoch 186/400\n",
      "3300/3823 [========================>.....] - ETA: 0s - loss: 0.1374 - loss_rmsle: 0.1374WARNING:tensorflow:Can save best model only with <function loss_rmsle at 0x000001A08D467158> available, skipping.\n",
      "3823/3823 [==============================] - 0s 81us/sample - loss: 0.1401 - loss_rmsle: 0.1393\n",
      "Epoch 187/400\n",
      "3650/3823 [===========================>..] - ETA: 0s - loss: 0.1470 - loss_rmsle: 0.1470WARNING:tensorflow:Can save best model only with <function loss_rmsle at 0x000001A08D467158> available, skipping.\n",
      "3823/3823 [==============================] - 0s 73us/sample - loss: 0.1498 - loss_rmsle: 0.1499\n",
      "Epoch 188/400\n",
      "3350/3823 [=========================>....] - ETA: 0s - loss: 0.1387 - loss_rmsle: 0.1387WARNING:tensorflow:Can save best model only with <function loss_rmsle at 0x000001A08D467158> available, skipping.\n",
      "3823/3823 [==============================] - 0s 79us/sample - loss: 0.1367 - loss_rmsle: 0.1365\n",
      "Epoch 189/400\n",
      "3400/3823 [=========================>....] - ETA: 0s - loss: 0.1367 - loss_rmsle: 0.1367WARNING:tensorflow:Can save best model only with <function loss_rmsle at 0x000001A08D467158> available, skipping.\n",
      "3823/3823 [==============================] - 0s 78us/sample - loss: 0.1356 - loss_rmsle: 0.1356\n",
      "Epoch 190/400\n",
      "3650/3823 [===========================>..] - ETA: 0s - loss: 0.1245 - loss_rmsle: 0.1245WARNING:tensorflow:Can save best model only with <function loss_rmsle at 0x000001A08D467158> available, skipping.\n",
      "3823/3823 [==============================] - 0s 74us/sample - loss: 0.1257 - loss_rmsle: 0.1271\n",
      "Epoch 191/400\n",
      "3650/3823 [===========================>..] - ETA: 0s - loss: 0.1265 - loss_rmsle: 0.1265WARNING:tensorflow:Can save best model only with <function loss_rmsle at 0x000001A08D467158> available, skipping.\n",
      "3823/3823 [==============================] - 0s 73us/sample - loss: 0.1261 - loss_rmsle: 0.1269\n",
      "Epoch 192/400\n",
      "3750/3823 [============================>.] - ETA: 0s - loss: 0.1219 - loss_rmsle: 0.1219WARNING:tensorflow:Can save best model only with <function loss_rmsle at 0x000001A08D467158> available, skipping.\n",
      "3823/3823 [==============================] - 0s 72us/sample - loss: 0.1229 - loss_rmsle: 0.1234\n",
      "Epoch 193/400\n",
      "3650/3823 [===========================>..] - ETA: 0s - loss: 0.1243 - loss_rmsle: 0.1243WARNING:tensorflow:Can save best model only with <function loss_rmsle at 0x000001A08D467158> available, skipping.\n",
      "3823/3823 [==============================] - 0s 73us/sample - loss: 0.1236 - loss_rmsle: 0.1239\n",
      "Epoch 194/400\n",
      "3700/3823 [============================>.] - ETA: 0s - loss: 0.1174 - loss_rmsle: 0.1174WARNING:tensorflow:Can save best model only with <function loss_rmsle at 0x000001A08D467158> available, skipping.\n",
      "3823/3823 [==============================] - 0s 72us/sample - loss: 0.1178 - loss_rmsle: 0.1176\n",
      "Epoch 195/400\n",
      "3550/3823 [==========================>...] - ETA: 0s - loss: 0.1209 - loss_rmsle: 0.1209WARNING:tensorflow:Can save best model only with <function loss_rmsle at 0x000001A08D467158> available, skipping.\n",
      "3823/3823 [==============================] - 0s 75us/sample - loss: 0.1210 - loss_rmsle: 0.1212\n",
      "Epoch 196/400\n",
      "3700/3823 [============================>.] - ETA: 0s - loss: 0.1161 - loss_rmsle: 0.1161WARNING:tensorflow:Can save best model only with <function loss_rmsle at 0x000001A08D467158> available, skipping.\n",
      "3823/3823 [==============================] - 0s 73us/sample - loss: 0.1162 - loss_rmsle: 0.1171\n",
      "Epoch 197/400\n",
      "3800/3823 [============================>.] - ETA: 0s - loss: 0.1151 - loss_rmsle: 0.1151WARNING:tensorflow:Can save best model only with <function loss_rmsle at 0x000001A08D467158> available, skipping.\n",
      "3823/3823 [==============================] - 0s 71us/sample - loss: 0.1145 - loss_rmsle: 0.1139\n",
      "Epoch 198/400\n",
      "3750/3823 [============================>.] - ETA: 0s - loss: 0.1118 - loss_rmsle: 0.1118WARNING:tensorflow:Can save best model only with <function loss_rmsle at 0x000001A08D467158> available, skipping.\n",
      "3823/3823 [==============================] - 0s 73us/sample - loss: 0.1123 - loss_rmsle: 0.1119\n",
      "Epoch 199/400\n",
      "3400/3823 [=========================>....] - ETA: 0s - loss: 0.1103 - loss_rmsle: 0.1103WARNING:tensorflow:Can save best model only with <function loss_rmsle at 0x000001A08D467158> available, skipping.\n",
      "3823/3823 [==============================] - 0s 78us/sample - loss: 0.1082 - loss_rmsle: 0.1083\n",
      "Epoch 200/400\n",
      "3100/3823 [=======================>......] - ETA: 0s - loss: 0.1070 - loss_rmsle: 0.1070WARNING:tensorflow:Can save best model only with <function loss_rmsle at 0x000001A08D467158> available, skipping.\n",
      "3823/3823 [==============================] - 0s 71us/sample - loss: 0.1091 - loss_rmsle: 0.1088\n",
      "Epoch 201/400\n",
      "3600/3823 [===========================>..] - ETA: 0s - loss: 0.1018 - loss_rmsle: 0.1018WARNING:tensorflow:Can save best model only with <function loss_rmsle at 0x000001A08D467158> available, skipping.\n",
      "3823/3823 [==============================] - 0s 74us/sample - loss: 0.1029 - loss_rmsle: 0.1027\n",
      "Epoch 202/400\n",
      "3100/3823 [=======================>......] - ETA: 0s - loss: 0.1039 - loss_rmsle: 0.1039WARNING:tensorflow:Can save best model only with <function loss_rmsle at 0x000001A08D467158> available, skipping.\n",
      "3823/3823 [==============================] - 0s 69us/sample - loss: 0.1048 - loss_rmsle: 0.1052\n",
      "Epoch 203/400\n",
      "3800/3823 [============================>.] - ETA: 0s - loss: 0.1025 - loss_rmsle: 0.1025WARNING:tensorflow:Can save best model only with <function loss_rmsle at 0x000001A08D467158> available, skipping.\n",
      "3823/3823 [==============================] - 0s 71us/sample - loss: 0.1044 - loss_rmsle: 0.1067\n",
      "Epoch 204/400\n",
      "3700/3823 [============================>.] - ETA: 0s - loss: 0.1051 - loss_rmsle: 0.1051WARNING:tensorflow:Can save best model only with <function loss_rmsle at 0x000001A08D467158> available, skipping.\n",
      "3823/3823 [==============================] - 0s 73us/sample - loss: 0.1058 - loss_rmsle: 0.1053\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1a08cdba5c0>"
      ]
     },
     "metadata": {},
     "execution_count": 31
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "pd.DataFrame(model0.history.history).plot()\r\n",
    "pd.DataFrame(model1.history.history).plot()\r\n",
    "pd.DataFrame(model2.history.history).plot()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "metadata": {},
     "execution_count": 65
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ],
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\r\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\r\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\r\n<!-- Created with matplotlib (https://matplotlib.org/) -->\r\n<svg height=\"248.518125pt\" version=\"1.1\" viewBox=\"0 0 378.465625 248.518125\" width=\"378.465625pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\r\n <metadata>\r\n  <rdf:RDF xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\r\n   <cc:Work>\r\n    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\r\n    <dc:date>2021-08-22T00:59:56.180645</dc:date>\r\n    <dc:format>image/svg+xml</dc:format>\r\n    <dc:creator>\r\n     <cc:Agent>\r\n      <dc:title>Matplotlib v3.3.2, https://matplotlib.org/</dc:title>\r\n     </cc:Agent>\r\n    </dc:creator>\r\n   </cc:Work>\r\n  </rdf:RDF>\r\n </metadata>\r\n <defs>\r\n  <style type=\"text/css\">*{stroke-linecap:butt;stroke-linejoin:round;}</style>\r\n </defs>\r\n <g id=\"figure_1\">\r\n  <g id=\"patch_1\">\r\n   <path d=\"M 0 248.518125 \r\nL 378.465625 248.518125 \r\nL 378.465625 0 \r\nL 0 0 \r\nz\r\n\" style=\"fill:none;\"/>\r\n  </g>\r\n  <g id=\"axes_1\">\r\n   <g id=\"patch_2\">\r\n    <path d=\"M 36.465625 224.64 \r\nL 371.265625 224.64 \r\nL 371.265625 7.2 \r\nL 36.465625 7.2 \r\nz\r\n\" style=\"fill:#ffffff;\"/>\r\n   </g>\r\n   <g id=\"matplotlib.axis_1\">\r\n    <g id=\"xtick_1\">\r\n     <g id=\"line2d_1\">\r\n      <defs>\r\n       <path d=\"M 0 0 \r\nL 0 3.5 \r\n\" id=\"ma3741dba8d\" style=\"stroke:#000000;stroke-width:0.8;\"/>\r\n      </defs>\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"51.683807\" xlink:href=\"#ma3741dba8d\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_1\">\r\n      <!-- 0 -->\r\n      <g transform=\"translate(48.502557 239.238437)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 31.78125 66.40625 \r\nQ 24.171875 66.40625 20.328125 58.90625 \r\nQ 16.5 51.421875 16.5 36.375 \r\nQ 16.5 21.390625 20.328125 13.890625 \r\nQ 24.171875 6.390625 31.78125 6.390625 \r\nQ 39.453125 6.390625 43.28125 13.890625 \r\nQ 47.125 21.390625 47.125 36.375 \r\nQ 47.125 51.421875 43.28125 58.90625 \r\nQ 39.453125 66.40625 31.78125 66.40625 \r\nz\r\nM 31.78125 74.21875 \r\nQ 44.046875 74.21875 50.515625 64.515625 \r\nQ 56.984375 54.828125 56.984375 36.375 \r\nQ 56.984375 17.96875 50.515625 8.265625 \r\nQ 44.046875 -1.421875 31.78125 -1.421875 \r\nQ 19.53125 -1.421875 13.0625 8.265625 \r\nQ 6.59375 17.96875 6.59375 36.375 \r\nQ 6.59375 54.828125 13.0625 64.515625 \r\nQ 19.53125 74.21875 31.78125 74.21875 \r\nz\r\n\" id=\"DejaVuSans-48\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_2\">\r\n     <g id=\"line2d_2\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"112.556534\" xlink:href=\"#ma3741dba8d\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_2\">\r\n      <!-- 2 -->\r\n      <g transform=\"translate(109.375284 239.238437)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 19.1875 8.296875 \r\nL 53.609375 8.296875 \r\nL 53.609375 0 \r\nL 7.328125 0 \r\nL 7.328125 8.296875 \r\nQ 12.9375 14.109375 22.625 23.890625 \r\nQ 32.328125 33.6875 34.8125 36.53125 \r\nQ 39.546875 41.84375 41.421875 45.53125 \r\nQ 43.3125 49.21875 43.3125 52.78125 \r\nQ 43.3125 58.59375 39.234375 62.25 \r\nQ 35.15625 65.921875 28.609375 65.921875 \r\nQ 23.96875 65.921875 18.8125 64.3125 \r\nQ 13.671875 62.703125 7.8125 59.421875 \r\nL 7.8125 69.390625 \r\nQ 13.765625 71.78125 18.9375 73 \r\nQ 24.125 74.21875 28.421875 74.21875 \r\nQ 39.75 74.21875 46.484375 68.546875 \r\nQ 53.21875 62.890625 53.21875 53.421875 \r\nQ 53.21875 48.921875 51.53125 44.890625 \r\nQ 49.859375 40.875 45.40625 35.40625 \r\nQ 44.1875 33.984375 37.640625 27.21875 \r\nQ 31.109375 20.453125 19.1875 8.296875 \r\nz\r\n\" id=\"DejaVuSans-50\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-50\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_3\">\r\n     <g id=\"line2d_3\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"173.429261\" xlink:href=\"#ma3741dba8d\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_3\">\r\n      <!-- 4 -->\r\n      <g transform=\"translate(170.248011 239.238437)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 37.796875 64.3125 \r\nL 12.890625 25.390625 \r\nL 37.796875 25.390625 \r\nz\r\nM 35.203125 72.90625 \r\nL 47.609375 72.90625 \r\nL 47.609375 25.390625 \r\nL 58.015625 25.390625 \r\nL 58.015625 17.1875 \r\nL 47.609375 17.1875 \r\nL 47.609375 0 \r\nL 37.796875 0 \r\nL 37.796875 17.1875 \r\nL 4.890625 17.1875 \r\nL 4.890625 26.703125 \r\nz\r\n\" id=\"DejaVuSans-52\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-52\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_4\">\r\n     <g id=\"line2d_4\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"234.301989\" xlink:href=\"#ma3741dba8d\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_4\">\r\n      <!-- 6 -->\r\n      <g transform=\"translate(231.120739 239.238437)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 33.015625 40.375 \r\nQ 26.375 40.375 22.484375 35.828125 \r\nQ 18.609375 31.296875 18.609375 23.390625 \r\nQ 18.609375 15.53125 22.484375 10.953125 \r\nQ 26.375 6.390625 33.015625 6.390625 \r\nQ 39.65625 6.390625 43.53125 10.953125 \r\nQ 47.40625 15.53125 47.40625 23.390625 \r\nQ 47.40625 31.296875 43.53125 35.828125 \r\nQ 39.65625 40.375 33.015625 40.375 \r\nz\r\nM 52.59375 71.296875 \r\nL 52.59375 62.3125 \r\nQ 48.875 64.0625 45.09375 64.984375 \r\nQ 41.3125 65.921875 37.59375 65.921875 \r\nQ 27.828125 65.921875 22.671875 59.328125 \r\nQ 17.53125 52.734375 16.796875 39.40625 \r\nQ 19.671875 43.65625 24.015625 45.921875 \r\nQ 28.375 48.1875 33.59375 48.1875 \r\nQ 44.578125 48.1875 50.953125 41.515625 \r\nQ 57.328125 34.859375 57.328125 23.390625 \r\nQ 57.328125 12.15625 50.6875 5.359375 \r\nQ 44.046875 -1.421875 33.015625 -1.421875 \r\nQ 20.359375 -1.421875 13.671875 8.265625 \r\nQ 6.984375 17.96875 6.984375 36.375 \r\nQ 6.984375 53.65625 15.1875 63.9375 \r\nQ 23.390625 74.21875 37.203125 74.21875 \r\nQ 40.921875 74.21875 44.703125 73.484375 \r\nQ 48.484375 72.75 52.59375 71.296875 \r\nz\r\n\" id=\"DejaVuSans-54\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-54\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_5\">\r\n     <g id=\"line2d_5\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"295.174716\" xlink:href=\"#ma3741dba8d\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_5\">\r\n      <!-- 8 -->\r\n      <g transform=\"translate(291.993466 239.238437)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 31.78125 34.625 \r\nQ 24.75 34.625 20.71875 30.859375 \r\nQ 16.703125 27.09375 16.703125 20.515625 \r\nQ 16.703125 13.921875 20.71875 10.15625 \r\nQ 24.75 6.390625 31.78125 6.390625 \r\nQ 38.8125 6.390625 42.859375 10.171875 \r\nQ 46.921875 13.96875 46.921875 20.515625 \r\nQ 46.921875 27.09375 42.890625 30.859375 \r\nQ 38.875 34.625 31.78125 34.625 \r\nz\r\nM 21.921875 38.8125 \r\nQ 15.578125 40.375 12.03125 44.71875 \r\nQ 8.5 49.078125 8.5 55.328125 \r\nQ 8.5 64.0625 14.71875 69.140625 \r\nQ 20.953125 74.21875 31.78125 74.21875 \r\nQ 42.671875 74.21875 48.875 69.140625 \r\nQ 55.078125 64.0625 55.078125 55.328125 \r\nQ 55.078125 49.078125 51.53125 44.71875 \r\nQ 48 40.375 41.703125 38.8125 \r\nQ 48.828125 37.15625 52.796875 32.3125 \r\nQ 56.78125 27.484375 56.78125 20.515625 \r\nQ 56.78125 9.90625 50.3125 4.234375 \r\nQ 43.84375 -1.421875 31.78125 -1.421875 \r\nQ 19.734375 -1.421875 13.25 4.234375 \r\nQ 6.78125 9.90625 6.78125 20.515625 \r\nQ 6.78125 27.484375 10.78125 32.3125 \r\nQ 14.796875 37.15625 21.921875 38.8125 \r\nz\r\nM 18.3125 54.390625 \r\nQ 18.3125 48.734375 21.84375 45.5625 \r\nQ 25.390625 42.390625 31.78125 42.390625 \r\nQ 38.140625 42.390625 41.71875 45.5625 \r\nQ 45.3125 48.734375 45.3125 54.390625 \r\nQ 45.3125 60.0625 41.71875 63.234375 \r\nQ 38.140625 66.40625 31.78125 66.40625 \r\nQ 25.390625 66.40625 21.84375 63.234375 \r\nQ 18.3125 60.0625 18.3125 54.390625 \r\nz\r\n\" id=\"DejaVuSans-56\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-56\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_6\">\r\n     <g id=\"line2d_6\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"356.047443\" xlink:href=\"#ma3741dba8d\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_6\">\r\n      <!-- 10 -->\r\n      <g transform=\"translate(349.684943 239.238437)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 12.40625 8.296875 \r\nL 28.515625 8.296875 \r\nL 28.515625 63.921875 \r\nL 10.984375 60.40625 \r\nL 10.984375 69.390625 \r\nL 28.421875 72.90625 \r\nL 38.28125 72.90625 \r\nL 38.28125 8.296875 \r\nL 54.390625 8.296875 \r\nL 54.390625 0 \r\nL 12.40625 0 \r\nz\r\n\" id=\"DejaVuSans-49\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-49\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n   </g>\r\n   <g id=\"matplotlib.axis_2\">\r\n    <g id=\"ytick_1\">\r\n     <g id=\"line2d_7\">\r\n      <defs>\r\n       <path d=\"M 0 0 \r\nL -3.5 0 \r\n\" id=\"me7f748a5ff\" style=\"stroke:#000000;stroke-width:0.8;\"/>\r\n      </defs>\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"36.465625\" xlink:href=\"#me7f748a5ff\" y=\"206.773929\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_7\">\r\n      <!-- 0.05 -->\r\n      <g transform=\"translate(7.2 210.573147)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 10.6875 12.40625 \r\nL 21 12.40625 \r\nL 21 0 \r\nL 10.6875 0 \r\nz\r\n\" id=\"DejaVuSans-46\"/>\r\n        <path d=\"M 10.796875 72.90625 \r\nL 49.515625 72.90625 \r\nL 49.515625 64.59375 \r\nL 19.828125 64.59375 \r\nL 19.828125 46.734375 \r\nQ 21.96875 47.46875 24.109375 47.828125 \r\nQ 26.265625 48.1875 28.421875 48.1875 \r\nQ 40.625 48.1875 47.75 41.5 \r\nQ 54.890625 34.8125 54.890625 23.390625 \r\nQ 54.890625 11.625 47.5625 5.09375 \r\nQ 40.234375 -1.421875 26.90625 -1.421875 \r\nQ 22.3125 -1.421875 17.546875 -0.640625 \r\nQ 12.796875 0.140625 7.71875 1.703125 \r\nL 7.71875 11.625 \r\nQ 12.109375 9.234375 16.796875 8.0625 \r\nQ 21.484375 6.890625 26.703125 6.890625 \r\nQ 35.15625 6.890625 40.078125 11.328125 \r\nQ 45.015625 15.765625 45.015625 23.390625 \r\nQ 45.015625 31 40.078125 35.4375 \r\nQ 35.15625 39.890625 26.703125 39.890625 \r\nQ 22.75 39.890625 18.8125 39.015625 \r\nQ 14.890625 38.140625 10.796875 36.28125 \r\nz\r\n\" id=\"DejaVuSans-53\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-53\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_2\">\r\n     <g id=\"line2d_8\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"36.465625\" xlink:href=\"#me7f748a5ff\" y=\"181.072809\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_8\">\r\n      <!-- 0.10 -->\r\n      <g transform=\"translate(7.2 184.872028)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-49\"/>\r\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_3\">\r\n     <g id=\"line2d_9\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"36.465625\" xlink:href=\"#me7f748a5ff\" y=\"155.371689\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_9\">\r\n      <!-- 0.15 -->\r\n      <g transform=\"translate(7.2 159.170908)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-49\"/>\r\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-53\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_4\">\r\n     <g id=\"line2d_10\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"36.465625\" xlink:href=\"#me7f748a5ff\" y=\"129.67057\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_10\">\r\n      <!-- 0.20 -->\r\n      <g transform=\"translate(7.2 133.469788)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-50\"/>\r\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_5\">\r\n     <g id=\"line2d_11\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"36.465625\" xlink:href=\"#me7f748a5ff\" y=\"103.96945\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_11\">\r\n      <!-- 0.25 -->\r\n      <g transform=\"translate(7.2 107.768669)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-50\"/>\r\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-53\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_6\">\r\n     <g id=\"line2d_12\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"36.465625\" xlink:href=\"#me7f748a5ff\" y=\"78.26833\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_12\">\r\n      <!-- 0.30 -->\r\n      <g transform=\"translate(7.2 82.067549)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 40.578125 39.3125 \r\nQ 47.65625 37.796875 51.625 33 \r\nQ 55.609375 28.21875 55.609375 21.1875 \r\nQ 55.609375 10.40625 48.1875 4.484375 \r\nQ 40.765625 -1.421875 27.09375 -1.421875 \r\nQ 22.515625 -1.421875 17.65625 -0.515625 \r\nQ 12.796875 0.390625 7.625 2.203125 \r\nL 7.625 11.71875 \r\nQ 11.71875 9.328125 16.59375 8.109375 \r\nQ 21.484375 6.890625 26.8125 6.890625 \r\nQ 36.078125 6.890625 40.9375 10.546875 \r\nQ 45.796875 14.203125 45.796875 21.1875 \r\nQ 45.796875 27.640625 41.28125 31.265625 \r\nQ 36.765625 34.90625 28.71875 34.90625 \r\nL 20.21875 34.90625 \r\nL 20.21875 43.015625 \r\nL 29.109375 43.015625 \r\nQ 36.375 43.015625 40.234375 45.921875 \r\nQ 44.09375 48.828125 44.09375 54.296875 \r\nQ 44.09375 59.90625 40.109375 62.90625 \r\nQ 36.140625 65.921875 28.71875 65.921875 \r\nQ 24.65625 65.921875 20.015625 65.03125 \r\nQ 15.375 64.15625 9.8125 62.3125 \r\nL 9.8125 71.09375 \r\nQ 15.4375 72.65625 20.34375 73.4375 \r\nQ 25.25 74.21875 29.59375 74.21875 \r\nQ 40.828125 74.21875 47.359375 69.109375 \r\nQ 53.90625 64.015625 53.90625 55.328125 \r\nQ 53.90625 49.265625 50.4375 45.09375 \r\nQ 46.96875 40.921875 40.578125 39.3125 \r\nz\r\n\" id=\"DejaVuSans-51\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-51\"/>\r\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_7\">\r\n     <g id=\"line2d_13\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"36.465625\" xlink:href=\"#me7f748a5ff\" y=\"52.56721\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_13\">\r\n      <!-- 0.35 -->\r\n      <g transform=\"translate(7.2 56.366429)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-51\"/>\r\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-53\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_8\">\r\n     <g id=\"line2d_14\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"36.465625\" xlink:href=\"#me7f748a5ff\" y=\"26.866091\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_14\">\r\n      <!-- 0.40 -->\r\n      <g transform=\"translate(7.2 30.66531)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-52\"/>\r\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n   </g>\r\n   <g id=\"line2d_15\">\r\n    <path clip-path=\"url(#p9dd4a9a4f1)\" d=\"M 51.683807 17.083636 \r\nL 82.12017 211.88659 \r\nL 112.556534 209.602217 \r\nL 142.992898 212.421266 \r\nL 173.429261 211.922249 \r\nL 203.865625 211.120839 \r\nL 234.301989 214.102059 \r\nL 264.738352 214.744273 \r\nL 295.174716 213.805062 \r\nL 325.61108 212.244287 \r\nL 356.047443 212.616194 \r\n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\r\n   </g>\r\n   <g id=\"line2d_16\">\r\n    <path clip-path=\"url(#p9dd4a9a4f1)\" d=\"M 51.683807 17.237644 \r\nL 82.12017 211.890035 \r\nL 112.556534 209.607757 \r\nL 142.992898 212.431893 \r\nL 173.429261 211.90158 \r\nL 203.865625 211.113129 \r\nL 234.301989 214.103668 \r\nL 264.738352 214.756364 \r\nL 295.174716 213.812012 \r\nL 325.61108 212.241682 \r\nL 356.047443 212.605476 \r\n\" style=\"fill:none;stroke:#ff7f0e;stroke-linecap:square;stroke-width:1.5;\"/>\r\n   </g>\r\n   <g id=\"patch_3\">\r\n    <path d=\"M 36.465625 224.64 \r\nL 36.465625 7.2 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_4\">\r\n    <path d=\"M 371.265625 224.64 \r\nL 371.265625 7.2 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_5\">\r\n    <path d=\"M 36.465625 224.64 \r\nL 371.265625 224.64 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_6\">\r\n    <path d=\"M 36.465625 7.2 \r\nL 371.265625 7.2 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"legend_1\">\r\n    <g id=\"patch_7\">\r\n     <path d=\"M 280.132812 44.834375 \r\nL 364.265625 44.834375 \r\nQ 366.265625 44.834375 366.265625 42.834375 \r\nL 366.265625 14.2 \r\nQ 366.265625 12.2 364.265625 12.2 \r\nL 280.132812 12.2 \r\nQ 278.132812 12.2 278.132812 14.2 \r\nL 278.132812 42.834375 \r\nQ 278.132812 44.834375 280.132812 44.834375 \r\nz\r\n\" style=\"fill:#ffffff;opacity:0.8;stroke:#cccccc;stroke-linejoin:miter;\"/>\r\n    </g>\r\n    <g id=\"line2d_17\">\r\n     <path d=\"M 282.132812 20.298437 \r\nL 302.132812 20.298437 \r\n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\r\n    </g>\r\n    <g id=\"line2d_18\"/>\r\n    <g id=\"text_15\">\r\n     <!-- loss -->\r\n     <g transform=\"translate(310.132812 23.798437)scale(0.1 -0.1)\">\r\n      <defs>\r\n       <path d=\"M 9.421875 75.984375 \r\nL 18.40625 75.984375 \r\nL 18.40625 0 \r\nL 9.421875 0 \r\nz\r\n\" id=\"DejaVuSans-108\"/>\r\n       <path d=\"M 30.609375 48.390625 \r\nQ 23.390625 48.390625 19.1875 42.75 \r\nQ 14.984375 37.109375 14.984375 27.296875 \r\nQ 14.984375 17.484375 19.15625 11.84375 \r\nQ 23.34375 6.203125 30.609375 6.203125 \r\nQ 37.796875 6.203125 41.984375 11.859375 \r\nQ 46.1875 17.53125 46.1875 27.296875 \r\nQ 46.1875 37.015625 41.984375 42.703125 \r\nQ 37.796875 48.390625 30.609375 48.390625 \r\nz\r\nM 30.609375 56 \r\nQ 42.328125 56 49.015625 48.375 \r\nQ 55.71875 40.765625 55.71875 27.296875 \r\nQ 55.71875 13.875 49.015625 6.21875 \r\nQ 42.328125 -1.421875 30.609375 -1.421875 \r\nQ 18.84375 -1.421875 12.171875 6.21875 \r\nQ 5.515625 13.875 5.515625 27.296875 \r\nQ 5.515625 40.765625 12.171875 48.375 \r\nQ 18.84375 56 30.609375 56 \r\nz\r\n\" id=\"DejaVuSans-111\"/>\r\n       <path d=\"M 44.28125 53.078125 \r\nL 44.28125 44.578125 \r\nQ 40.484375 46.53125 36.375 47.5 \r\nQ 32.28125 48.484375 27.875 48.484375 \r\nQ 21.1875 48.484375 17.84375 46.4375 \r\nQ 14.5 44.390625 14.5 40.28125 \r\nQ 14.5 37.15625 16.890625 35.375 \r\nQ 19.28125 33.59375 26.515625 31.984375 \r\nL 29.59375 31.296875 \r\nQ 39.15625 29.25 43.1875 25.515625 \r\nQ 47.21875 21.78125 47.21875 15.09375 \r\nQ 47.21875 7.46875 41.1875 3.015625 \r\nQ 35.15625 -1.421875 24.609375 -1.421875 \r\nQ 20.21875 -1.421875 15.453125 -0.5625 \r\nQ 10.6875 0.296875 5.421875 2 \r\nL 5.421875 11.28125 \r\nQ 10.40625 8.6875 15.234375 7.390625 \r\nQ 20.0625 6.109375 24.8125 6.109375 \r\nQ 31.15625 6.109375 34.5625 8.28125 \r\nQ 37.984375 10.453125 37.984375 14.40625 \r\nQ 37.984375 18.0625 35.515625 20.015625 \r\nQ 33.0625 21.96875 24.703125 23.78125 \r\nL 21.578125 24.515625 \r\nQ 13.234375 26.265625 9.515625 29.90625 \r\nQ 5.8125 33.546875 5.8125 39.890625 \r\nQ 5.8125 47.609375 11.28125 51.796875 \r\nQ 16.75 56 26.8125 56 \r\nQ 31.78125 56 36.171875 55.265625 \r\nQ 40.578125 54.546875 44.28125 53.078125 \r\nz\r\n\" id=\"DejaVuSans-115\"/>\r\n      </defs>\r\n      <use xlink:href=\"#DejaVuSans-108\"/>\r\n      <use x=\"27.783203\" xlink:href=\"#DejaVuSans-111\"/>\r\n      <use x=\"88.964844\" xlink:href=\"#DejaVuSans-115\"/>\r\n      <use x=\"141.064453\" xlink:href=\"#DejaVuSans-115\"/>\r\n     </g>\r\n    </g>\r\n    <g id=\"line2d_19\">\r\n     <path d=\"M 282.132812 34.976562 \r\nL 302.132812 34.976562 \r\n\" style=\"fill:none;stroke:#ff7f0e;stroke-linecap:square;stroke-width:1.5;\"/>\r\n    </g>\r\n    <g id=\"line2d_20\"/>\r\n    <g id=\"text_16\">\r\n     <!-- loss_rmsle -->\r\n     <g transform=\"translate(310.132812 38.476562)scale(0.1 -0.1)\">\r\n      <defs>\r\n       <path d=\"M 50.984375 -16.609375 \r\nL 50.984375 -23.578125 \r\nL -0.984375 -23.578125 \r\nL -0.984375 -16.609375 \r\nz\r\n\" id=\"DejaVuSans-95\"/>\r\n       <path d=\"M 41.109375 46.296875 \r\nQ 39.59375 47.171875 37.8125 47.578125 \r\nQ 36.03125 48 33.890625 48 \r\nQ 26.265625 48 22.1875 43.046875 \r\nQ 18.109375 38.09375 18.109375 28.8125 \r\nL 18.109375 0 \r\nL 9.078125 0 \r\nL 9.078125 54.6875 \r\nL 18.109375 54.6875 \r\nL 18.109375 46.1875 \r\nQ 20.953125 51.171875 25.484375 53.578125 \r\nQ 30.03125 56 36.53125 56 \r\nQ 37.453125 56 38.578125 55.875 \r\nQ 39.703125 55.765625 41.0625 55.515625 \r\nz\r\n\" id=\"DejaVuSans-114\"/>\r\n       <path d=\"M 52 44.1875 \r\nQ 55.375 50.25 60.0625 53.125 \r\nQ 64.75 56 71.09375 56 \r\nQ 79.640625 56 84.28125 50.015625 \r\nQ 88.921875 44.046875 88.921875 33.015625 \r\nL 88.921875 0 \r\nL 79.890625 0 \r\nL 79.890625 32.71875 \r\nQ 79.890625 40.578125 77.09375 44.375 \r\nQ 74.3125 48.1875 68.609375 48.1875 \r\nQ 61.625 48.1875 57.5625 43.546875 \r\nQ 53.515625 38.921875 53.515625 30.90625 \r\nL 53.515625 0 \r\nL 44.484375 0 \r\nL 44.484375 32.71875 \r\nQ 44.484375 40.625 41.703125 44.40625 \r\nQ 38.921875 48.1875 33.109375 48.1875 \r\nQ 26.21875 48.1875 22.15625 43.53125 \r\nQ 18.109375 38.875 18.109375 30.90625 \r\nL 18.109375 0 \r\nL 9.078125 0 \r\nL 9.078125 54.6875 \r\nL 18.109375 54.6875 \r\nL 18.109375 46.1875 \r\nQ 21.1875 51.21875 25.484375 53.609375 \r\nQ 29.78125 56 35.6875 56 \r\nQ 41.65625 56 45.828125 52.96875 \r\nQ 50 49.953125 52 44.1875 \r\nz\r\n\" id=\"DejaVuSans-109\"/>\r\n       <path d=\"M 56.203125 29.59375 \r\nL 56.203125 25.203125 \r\nL 14.890625 25.203125 \r\nQ 15.484375 15.921875 20.484375 11.0625 \r\nQ 25.484375 6.203125 34.421875 6.203125 \r\nQ 39.59375 6.203125 44.453125 7.46875 \r\nQ 49.3125 8.734375 54.109375 11.28125 \r\nL 54.109375 2.78125 \r\nQ 49.265625 0.734375 44.1875 -0.34375 \r\nQ 39.109375 -1.421875 33.890625 -1.421875 \r\nQ 20.796875 -1.421875 13.15625 6.1875 \r\nQ 5.515625 13.8125 5.515625 26.8125 \r\nQ 5.515625 40.234375 12.765625 48.109375 \r\nQ 20.015625 56 32.328125 56 \r\nQ 43.359375 56 49.78125 48.890625 \r\nQ 56.203125 41.796875 56.203125 29.59375 \r\nz\r\nM 47.21875 32.234375 \r\nQ 47.125 39.59375 43.09375 43.984375 \r\nQ 39.0625 48.390625 32.421875 48.390625 \r\nQ 24.90625 48.390625 20.390625 44.140625 \r\nQ 15.875 39.890625 15.1875 32.171875 \r\nz\r\n\" id=\"DejaVuSans-101\"/>\r\n      </defs>\r\n      <use xlink:href=\"#DejaVuSans-108\"/>\r\n      <use x=\"27.783203\" xlink:href=\"#DejaVuSans-111\"/>\r\n      <use x=\"88.964844\" xlink:href=\"#DejaVuSans-115\"/>\r\n      <use x=\"141.064453\" xlink:href=\"#DejaVuSans-115\"/>\r\n      <use x=\"193.164062\" xlink:href=\"#DejaVuSans-95\"/>\r\n      <use x=\"243.164062\" xlink:href=\"#DejaVuSans-114\"/>\r\n      <use x=\"282.527344\" xlink:href=\"#DejaVuSans-109\"/>\r\n      <use x=\"379.939453\" xlink:href=\"#DejaVuSans-115\"/>\r\n      <use x=\"432.039062\" xlink:href=\"#DejaVuSans-108\"/>\r\n      <use x=\"459.822266\" xlink:href=\"#DejaVuSans-101\"/>\r\n     </g>\r\n    </g>\r\n   </g>\r\n  </g>\r\n </g>\r\n <defs>\r\n  <clipPath id=\"p9dd4a9a4f1\">\r\n   <rect height=\"217.44\" width=\"334.8\" x=\"36.465625\" y=\"7.2\"/>\r\n  </clipPath>\r\n </defs>\r\n</svg>\r\n",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD4CAYAAADiry33AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8vihELAAAACXBIWXMAAAsTAAALEwEAmpwYAAAluUlEQVR4nO3de5CcdZ3v8fe3r3PPXDKTSWZymVy4BCJIBVZhN+LZg8LqMVrWWWBXXAGXogpc1zqgeHa9HOWUi26566nKQlGKSu26aLlwFpcIuJ5VVpStTNhgBARjCGQyk8wlc7/17Xv+6A42cZI8M5mZZ6b786rq6n4uv+f59gQ+8/Rvnv79zN0REZHSFQm7ABERWVgKehGREqegFxEpcQp6EZESp6AXESlxsbALmMnKlSt9w4YNYZchIrJs7N27t9/dm2fatiSDfsOGDXR2doZdhojIsmFmr55qm7puRERKnIJeRKTEKehFRErckuyjF5HlLZ1O09XVxdTUVNillJyKigra29uJx+OB2yjoRWTedXV1UVtby4YNGzCzsMspGe7OwMAAXV1ddHR0BG6nrhsRmXdTU1M0NTUp5OeZmdHU1DTrT0oKehFZEAr5hTGXn2vJBH0mneb737mfZ5/5t7BLERFZUkom6KPRKDte+BRTnX8fdikisgTU1NSEXcKSUTJBb5EIPbG1VI8eDLsUEZElpWSCHmC4uoOW6dfCLkNElhB358477+TCCy9k27ZtfPvb3wagp6eHHTt2cPHFF3PhhRfy7//+72SzWT70oQ+9vu/f/M3fhFz9/Cip2yuzjZtZPfwkw8NDrFhRH3Y5IgL8r+89zwvdI/N6zK1r6vjMf7sg0L4PP/ww+/bt47nnnqO/v59LL72UHTt28K1vfYt3vvOd/MVf/AXZbJaJiQn27dvHkSNH+MUvfgHA0NDQvNYdlpK6oq9YfT4APQf3h1yJiCwVP/nJT7j++uuJRqOsWrWKt73tbezZs4dLL72Ur3/963z2s59l//791NbWsnHjRg4ePMhHPvIRHn/8cerq6sIuf16U1BV94/oL4acwfPgFePPvhV2OiEDgK++F4u4zrt+xYwdPPfUUjz32GDfccAN33nknH/zgB3nuued44okn2LVrF9/5znd44IEHFrni+VdSV/StG84n60bm2EthlyIiS8SOHTv49re/TTabpa+vj6eeeorLLruMV199lZaWFv70T/+Um2++mWeffZb+/n5yuRzvf//7+fznP8+zzz4bdvnzItAVvZldDXwFiAJfdfe/OsV+lwLPANe6+3dn03Y+xJKVHIm2khj69UKdQkSWmfe973387Gc/46KLLsLM+OIXv0hrayvf/OY3+dKXvkQ8HqempoYHH3yQI0eOcOONN5LL5QD4whe+EHL188NO9bHm9R3MosDLwFVAF7AHuN7dX5hhvx8AU8AD7v7doG1Ptn37dp/rxCM//+I7qZ46yqZPPzen9iJy9l588UXOP//8sMsoWTP9fM1sr7tvn2n/IF03lwEH3P2gu6eAh4CdM+z3EeCfgN45tJ03Uys20ZY9QjqdXsjTiIgsG0GCvg04XLTcVVj3OjNrA94H3DfbtkXHuMXMOs2ss6+vL0BZM4u2nEuFpel+7VdzPoaISCkJEvQzjaBzcn/P3wKfcPfsHNrmV7rf7+7b3X17c/OM89sGUrd2KwADr+gWSxERCPbH2C5gbdFyO9B90j7bgYcKo6qtBP7AzDIB286r1k1vAmCy55cLeRoRkWUjSNDvAbaYWQdwBLgO+KPiHdz99RHwzewbwL+4+/81s9iZ2s632oZVDFJH5Li6bkREIEDQu3vGzG4HniB/i+QD7v68md1a2H5yv/wZ285P6ad2LLGWurFXFvo0IiLLQqD76N19N7D7pHUzBry7f+hMbRfaWM1GOo4/hbtr8gMRKXsl9c3YE7xpC00MM9B3LOxSRCQky2k8+iuvvJK5fncoiJIM+sq2/BcJjurOGxFZBJlMJuwSTqukBjU7oXnDNgBGu16A37kq5GpEytz374Kj83zR1boNrgk2moq78/GPf5zvf//7mBl/+Zd/ybXXXktPTw/XXnstIyMjZDIZ7r33Xi6//HJuvvlmOjs7MTNuuukmPvaxj8143CuvvJLLL7+cp59+mve85z1873vf481vfjN79+6lr6+PBx98kC984Qvs37+fa6+9lrvvvpvx8XH+8A//kK6uLrLZLJ/61Ke49tpr33DcJ598ks985jNMT0+zadMmvv71r5/1p5PSDPr2LaQ8hvdqcDORcreQ49EPDQ3x4x//GIDvfe97JBIJnnrqKb7yla+wc+dO9u7dS2NjI5s2beJjH/sYP/rRj1izZg2PPfYYAMPDw284Xn9/P3fffTf/+q//SnV1Nffccw9f/vKX+fSnP31WP4OSDPpILEZ3rI3KEQ1uJhK6gFfeC+V049HfdNNNpNNp3vve93LxxRe/YTz6d73rXbzjHe847bFPvhp/z3veA8C2bdu44IILWL16NQAbN27k8OHDbNu2jTvuuINPfOITvPvd7+b3fu+Nw6k/88wzvPDCC1xxxRUApFIp3vrWt571z6Ak++gBBqs2sHLq1bDLEJGQnWk8+ra2Nm644QYefPBBGhoaeO6557jyyivZtWsXH/7wh0977Orq6jcsJ5NJACKRyOuvTyxnMhnOOecc9u7dy7Zt2/jkJz/J5z73ud+q9aqrrmLfvn3s27ePF154ga997WtzedtvULJBn67fzOrcMSYnJsIuRURCtJTGo+/u7qaqqooPfOAD3HHHHb91/Le85S08/fTTHDhwAICJiQlefvnlsz5vSXbdAMRXnUfscI5Dr7zA5gtmHLlTRMrAUhqPfv/+/dx5551EIhHi8Tj33nvvG7Y3NzfzjW98g+uvv57p6WkA7r77bs4555yzOu8Zx6MPw9mMR3/CK/t/Qsc/vYs9l32FS//gQ/NTmIgEovHoF9ZCjEe/LK3emL/FMqVpBUWkzJVs101F9Qp6rYn44IGwSxGRZey2227j6aeffsO6j370o9x4440hVTR7JRv0AH3J9awY1+BmImEolbGmdu3aFXYJbzCX7vaS7boBmKjbSFumi1w2F3YpImWloqKCgYGBOYWSnJq7MzAwQEVFxazalfQVva08h5re79LdfYg1azeGXY5I2Whvb6erq4uzmRZUZlZRUUF7e/us2pR00Ne0b4UXoO+VXyjoRRZRPB6no6PjzDvKoijprptVHfk7b8aPvBByJSIi4SnpoK9ftY5xKmBA0wqKSPkKFPRmdrWZvWRmB8zsrhm27zSzn5vZPjPrNLPfLdp2yMz2n9g2n8Wfse5IhJ7YWmpGDy7maUVElpQz9tGbWRTYBVwFdAF7zOxRdy/uD/kh8Ki7u5m9CfgOcF7R9re7e/881h3YcM1G2ob2hnFqEZElIcgV/WXAAXc/6O4p4CFgZ/EO7j7mv7mPqhpYMvdUZRs200o/w8ODYZciIhKKIEHfBhwuWu4qrHsDM3ufmf0SeAy4qWiTA0+a2V4zu+VUJzGzWwrdPp3zeUtWcnX+g0XPr38xb8cUEVlOggT9TF9t+60rdnd/xN3PA94LfL5o0xXufglwDXCbme2Y6STufr+7b3f37c3NzQHKCqapMK3g8OHn5+2YIiLLSZCg7wLWFi23A92n2tndnwI2mdnKwnJ34bkXeIR8V9CiaV1/HhmPkNG0giJSpoIE/R5gi5l1mFkCuA54tHgHM9tshUEtzOwSIAEMmFm1mdUW1lcD7wAWtQ8llqzkaLSV5JCmFRSR8nTGu27cPWNmtwNPAFHgAXd/3sxuLWy/D3g/8EEzSwOTwLWFO3BWAY8UfgfEgG+5++ML9F5OaaBiPY2Thxb7tCIiS0KgIRDcfTew+6R19xW9vge4Z4Z2B4GLzrLGszZdv4nzjnSSTqeJx+NhlyMisqhK+puxJ0RbziVpabpfPfu5F0VElpuyCPq69q0ADBzSLZYiUn7KIuhbN+VvsZzs+WXIlYiILL6yCPrahlUcp47ocQ1uJiLlpyyCHuBYYh21YxrcTETKT9kE/VjtRlanD2tqMxEpO2UT9N60hUZGGOg7GnYpIiKLqmyCvmrN+QAcPbg/5EpERBZX2QR9c2FawdEuTSsoIuWlfIK+bTPTHsf79KUpESkvZRP0kViM7lgblSMa3ExEykvZBD3AUNUGVk69GnYZIiKLqqyCPlW/mTW5o0xOTIRdiojIoimroE+0nkvUnO6Dmm1KRMpHWQV9/boLABh8TUEvIuWjrIJ+9cYLAZg+psHNRKR8BAp6M7vazF4yswNmdtcM23ea2c/NbJ+ZdZrZ7wZtu5gqqldwzFYSHzwQZhkiIovqjEFvZlFgF3ANsBW43sy2nrTbD4GL3P1i4Cbgq7Nou6j6kuuoHz8UZgkiIosqyBX9ZcABdz/o7ingIWBn8Q7uPua/GS2sGvCgbRfbRN0m2jKHyWVzYZYhIrJoggR9G3C4aLmrsO4NzOx9ZvZL4DHyV/WB2xba31Lo9uns6+sLUvucWPM5VNsUR7tfWbBziIgsJUGC3mZY91tj/br7I+5+HvBe4POzaVtof7+7b3f37c3NzQHKmpuatnzPUd8rmlZQRMpDkKDvAtYWLbcD3afa2d2fAjaZ2crZtl0MqzbmBzebOKLBzUSkPAQJ+j3AFjPrMLMEcB3waPEOZrbZzKzw+hIgAQwEabvYGlrWMkYl9GtaQREpD7Ez7eDuGTO7HXgCiAIPuPvzZnZrYft9wPuBD5pZGpgEri38cXbGtgv0XgKxSISe2FqqRzWtoIiUhzMGPYC77wZ2n7TuvqLX9wD3BG0btpGaDtqHOsMuQ0RkUZTVN2NPyDZsZhUDDA8dD7sUEZEFV5ZBn1x9HgA9B3XnjYiUvrIM+sb1+TFvhg/rzhsRKX1lGfStHVvJeIRsrwY3E5HSV5ZBH09U0BNdTXJI0wqKSOkry6AHGKhYT8OkphUUkdJXtkE/vWIT7dkjpNPpsEsREVlQZRv00ZZzSViG7ldfCrsUEZEFVbZBX7c2P7jZwCHdYikipa1sg751U35ws6ke3XkjIqWtbIO+rqGFAVYQGdDgZiJS2so26AF6E+uoG9cEJCJS2so66MdqO2hNv8ZvZkEUESk9ZR30uaYtNDLKQG9P2KWIiCyYsg76qjX5O2+OvrI/5EpERBZOWQd9c0d+cLOxLg1uJiKlq6yDvqVtM1MeJ9f3ctiliIgsmEBBb2ZXm9lLZnbAzO6aYfsfm9nPC4+fmtlFRdsOmdl+M9tnZktqWqdILEZ3rI3KYU0rKCKl64xTCZpZFNgFXAV0AXvM7FF3L+7veAV4m7sPmtk1wP3A7xRtf7u7989j3fNmqKqD5jF9aUpESleQK/rLgAPuftDdU8BDwM7iHdz9p+4+WFh8Bmif3zIXTrp+M2tyR5mcmAi7FBGRBREk6NuAw0XLXYV1p3Iz8P2iZQeeNLO9ZnbLqRqZ2S1m1mlmnX19fQHKmh/x1nOJmtN98PlFO6eIyGIKEvQ2w7oZv2FkZm8nH/SfKFp9hbtfAlwD3GZmO2Zq6+73u/t2d9/e3NwcoKz5Ub/uAgAGX1PQi0hpChL0XcDaouV2oPvknczsTcBXgZ3uPnBivbt3F557gUfIdwUtGWs25m+xTB19MeRKREQWRpCg3wNsMbMOM0sA1wGPFu9gZuuAh4Eb3P3lovXVZlZ74jXwDmBJjQtcUV3HUWsmPngg7FJERBbEGe+6cfeMmd0OPAFEgQfc/Xkzu7Ww/T7g00AT8HdmBpBx9+3AKuCRwroY8C13f3xB3slZ6Euuo37iUNhliIgsiDMGPYC77wZ2n7TuvqLXHwY+PEO7g8BFJ69faibqNrHx2D+Ty+aIRMv6O2QiUoKUaoCtPIdqm+bYEX1xSkRKj4IeqG0/H4BeTSsoIiVIQQ+0bHwTABNHdOeNiJQeBT3Q2NLOKJVYvwY3E5HSo6AHLBKhJ7aW6lH10YtI6VHQF4xUd9CSei3sMkRE5p2CviDbtIVVHGd46HjYpYiIzCsFfUGy9TwAeg5qWkERKS0K+oKm9fkxb0YOa1pBESktCvqC1o7zyXiETO9LYZciIjKvFPQF8UQFPdHVJIc0uJmIlBYFfZGBivU0Tr4adhkiIvNKQV9kesUm2rLdpNOpsEsREZk3Cvoi0ZZzSViG7kPqpxeR0qGgL1JXmFZwQIObiUgJUdAXad20DYCpnl+GXImIyPxR0Bepq29mgHqix38VdikiIvMmUNCb2dVm9pKZHTCzu2bY/sdm9vPC46dmdlHQtkvNscQ66sZfCbsMEZF5c8agN7MosAu4BtgKXG9mW0/a7RXgbe7+JuDzwP2zaLukjNV20Jo+jLuHXYqIyLwIckV/GXDA3Q+6ewp4CNhZvIO7/9TdBwuLzwDtQdsuNd60hQZGGejtCbsUEZF5ESTo24DDRctdhXWncjPw/dm2NbNbzKzTzDr7+voClLUwqtbkpxU8dvDnodUgIjKfggS9zbBuxn4NM3s7+aD/xGzbuvv97r7d3bc3NzcHKGthNHfk77wZPaLBzUSkNMQC7NMFrC1abge6T97JzN4EfBW4xt0HZtN2KWlp38yUx/E+TSsoIqUhyBX9HmCLmXWYWQK4Dni0eAczWwc8DNzg7i/Ppu1SE4lG6Y61UzmsaQVFpDSc8Yre3TNmdjvwBBAFHnD3583s1sL2+4BPA03A35kZQKbQDTNj2wV6L/NmsGoDq8ZeDLsMEZF5EaTrBnffDew+ad19Ra8/DHw4aNulLl2/mdUjP2JyYpzKquqwyxEROSv6ZuwMEq3nEjWn++CS//AhInJGCvoZ1K/LTys4+JqCXkSWPwX9DNZsygd96qgGNxOR5U9BP4OKqlp6rJn4oKYVFJHlT0F/Cv3J9dRPHAq7DBGRs6agP4WJuo2syRwml82GXYqIyFlR0J+CNZ9LtU1z7Ii+OCUiy5uC/hRq2vKDm/W9omkFRWR5U9CfwqrCtILjR/QNWRFZ3hT0p9DY3M4IVdiABjcTkeVNQX8KFolwNLaWmlFNKygiy5uC/jRGajpoSb0WdhkiImdFQX8amcYttHCc4aHjYZciIjJnCvrTqFh9HgBHf61pBUVk+VLQn0bT+vyYN8OHNa2giCxfCvrTaN1wPmmPkul9KexSRETmLFDQm9nVZvaSmR0ws7tm2H6emf3MzKbN7I6Tth0ys/1mts/MOuer8MUQTyTpibZSMfTrsEsREZmzM84wZWZRYBdwFfnJvveY2aPuXtyfcRz4M+C9pzjM2929/yxrDcXxivU0Th0KuwwRkTkLckV/GXDA3Q+6ewp4CNhZvIO797r7HiC9ADWGaqp+M2uy3aTTqbBLERGZkyBB3wYcLlruKqwLyoEnzWyvmd0ym+KWgljLOSQsS88hTUIiIstTkKC3Gdb5LM5xhbtfAlwD3GZmO2Y8idktZtZpZp19fX2zOPzCqlt7AQADhzS4mYgsT0GCvgtYW7TcDnQHPYG7dxeee4FHyHcFzbTf/e6+3d23Nzc3Bz38gmstDG421aMrehFZnoIE/R5gi5l1mFkCuA54NMjBzazazGpPvAbeASyrS+O6+mYGqCdy/FdhlyIiMidnvOvG3TNmdjvwBBAFHnD3583s1sL2+8ysFegE6oCcmf05sBVYCTxiZifO9S13f3xB3skCOpZYR924BjcTkeXpjEEP4O67gd0nrbuv6PVR8l06JxsBLjqbApeC0dqNnDfwAzyXwyL6jpmILC9KrSBWbmEF4wz09YRdiYjIrCnoA6hanZ9W8NhBDW4mIsuPgj6A5o35O29Gj2hwMxFZfhT0AbS0bWLSE3iv7rwRkeVHQR9AJBqlO9ZO5YgGNxOR5UdBH9BQ1Qaap18NuwwRkVlT0AeUbtjM6lwvkxPjYZciIjIrCvqA4q3nEjGn++Cy+mKviIiCPqiGdflpBQdfez7kSkREZkdBH9CajReQcyN1VIObicjyoqAPqKKqlqORZuKDuvNGRJYXBf0s9CfXUz+hwc1EZHlR0M/CRN1G2jJd5LLZsEsREQlMQT8LkeZzqLJpjh05GHYpIiKBKehnoaZtKwB9r+gWSxFZPhT0s7CqMK3ghAY3E5FlREE/C43NbYxQjQ28HHYpIiKBBQp6M7vazF4yswNmdtcM288zs5+Z2bSZ3TGbtsuJRSL0xNZSPao7b0Rk+Thj0JtZFNgFXEN+HtjrzWzrSbsdB/4M+Os5tF1WRmo6WJV6LewyREQCC3JFfxlwwN0PunsKeAjYWbyDu/e6+x4gPdu2y022cQvNDDI8NBB2KSIigQQJ+jbgcNFyV2FdEIHbmtktZtZpZp19fX0BD7/4KlafB8DRX2taQRFZHoIEvc2wzgMeP3Bbd7/f3be7+/bm5uaAh198TesvAGD4sO68EZHlIUjQdwFri5bbge6Axz+btktS64bzSXuUbO9LYZciIhJIkKDfA2wxsw4zSwDXAY8GPP7ZtF2S4okk3dHVJIc0uJmILA+xM+3g7hkzux14AogCD7j782Z2a2H7fWbWCnQCdUDOzP4c2OruIzO1XaD3smiOV66nafJQ2GWIiARyxqAHcPfdwO6T1t1X9Poo+W6ZQG2Xu+kVm1g99gzp1DTxRDLsckRETkvfjJ2DaMu5JCxLz6uahERElj4F/RysWJu/82bgkAY3E5GlT0E/B62Fwc2menRFLyJLn4J+DurqV9JPPdHjB8IuRUTkjBT0c3QssY66MQ1uJiJLn4J+jsZqN7I68xqey4VdiojIaSno52rlFlYwzkDfsv6ir4iUAQX9HFWtPh+AYwf3h1yJiMjpKejnqHlj/s6bsS4NbiYiS5uCfo5a2jYx4Um8T9MKisjSpqCfo0g0SnesncoRDW4mIkubgv4sDFVtoHla0wqKyNKmoD8L6YbNtOZ6mRwfC7sUEZFTUtCfhUTruUTM6T647EdeFpESpqA/Cw3rLgRg8DUNbiYiS5eC/iys2XgBOTfSxzS4mYgsXQr6s1BRVcPRSAvxQd15IyJLV6CgN7OrzewlMztgZnfNsN3M7P8Utv/czC4p2nbIzPab2T4z65zP4peCvuQ66icOhV2GiMgpnTHozSwK7AKuAbYC15vZ1pN2uwbYUnjcAtx70va3u/vF7r797EteWiZXbGJNpotcNht2KSIiMwpyRX8ZcMDdD7p7CngI2HnSPjuBBz3vGaDezFbPc61Lkq08hyqb5liXum9EZGkKEvRtwOGi5a7CuqD7OPCkme01s1tOdRIzu8XMOs2ss6+vL0BZS0Nte/7DTZ+mFRSRJSpI0NsM63wW+1zh7peQ7965zcx2zHQSd7/f3be7+/bm5uYAZS0NqwqDm00ceTHkSkREZhYLsE8XsLZouR04eRD2U+7j7ieee83sEfJdQU/NteClprF5DcNUYwNnP7hZNpNhZKifkYGjTAz3MTXST3q0n+z4AIwPEJkaJJ4aIpkeoiozQm1umBofZ9yqGI40MB5vZKpiJZnKZqhpIVbXSkXDaqqb1lC/so36plVEotF5eNcispwECfo9wBYz6wCOANcBf3TSPo8Ct5vZQ8DvAMPu3mNm1UDE3UcLr98BfG7+yg+fRSL0xNZRPfrGaQWnJicYOd7L2OAxJob7mB7pIzM6QG5iACYHiU4NkkwNkkwPU50dodZHqPNxGsxpmOE8aY8yYrWMRmqZiK1gqHItfclt5BIriKRGSUz1U5kaYOXIYRqHhkhaesZj9NsKhqONTCQamU6uJFPVgtWsIr5iFZWNa6hpWkN9y1rq6uqxSIncfetONj1NanqS9PQ06dQU6fQUmdQUmelpspn8umxqilwmRS49RS49/fprz6TwzDSeTWHxCuL1bVQ1raFu5VoaW9dSUVUb9jsUOa0zBr27Z8zsduAJIAo84O7Pm9mthe33AbuBPwAOABPAjYXmq4BHzOzEub7l7o/P+7sI2UhNB28a/AEHPn8J1dkR6nyEapumAmiZYf8JTzJidYxF65iK1dFbuZruZANe2YhVNRKraSJeu5LKFc1U16+irqmF6pp6miIRmgLU47kcoyODDPUeZrS/m6mhHtLDPfhYL9HxPpLT/dSkBlgzeYCGwSFi9tvTIU55nOORBkaijUwkmkhXrCRX1YzVrSKxYjWVDatJ1qwgl0mTzWTwbIpcJk0um8azGXLZNLlMGs9l8MK6E8/kCsu5DGQzkMtguTSey2LZNHgGy+Ufv3mdxQqvI54hkksTzaWI5tJEPE3spEecDHHSxD1NwrJEgcrCY76NUslgpJHR2EomK5pf/+UZXbGaysY2apvbaVi1jpraEvrlucx5Lsf09CTjo0NMjQ0zNTbE9MQI6YkRMlOj4DksEoNIjEgkikUjmMWwaBSLRIlEYkSi0cJyYZ9IjGis8BzN72PRKJFolGgkRiQWJRqNYdEY0cIj8vpz/rgLxdxP7m4P3/bt272zc/nccr//xw8T/8mXmI7VkEo0kK1ogMpGItWNxGpWkqxrpqq+mZr6FuqaWkhWVIdd8uty2Swjx48x1NfF+ED+l0Jm+BiMHSM22Udyqp/q9HHqc4M0MLKgtWQ8QpYomcIjaxGyxMgSJWvRwnOMHFEyFiNrCTKROB6Jk40kyEYSeCROLpLAowmIxvFoEqKJ/COWhFiSSCyBxRJEYgkisQoi8fy6aKKCaCJJLFZ4TlQQTySJJSuIJypJJJJMjo8y1HuYsf4upgZ7yI30YGNHiU/0UpXqpy4zQGNukEpL/db7m/AkxyMNjMaamEyuJFW1Cq9pzXexNa6hZmU7DS1rWdHYol8IM8hms4yPDTNZCOap8RHSE8OkJkbITI7iUyPkpkdhegxLjRFJjxNLjxHLjpPITJDMTVCRm6CKCap8irgtvVui+2ik+bOvnHnHGZjZ3lPdwq6gl8AyqWmGBnoY6TvC+EA36alRItE4Fo0TicZ+8zpWeERjRGNxItF4PkhjcaLx/PpYLEkkFicejxGNJYjF4kQiEQqf/pY1z+UYGR5k8NhrjPUfZvJ4N5nhHhg7Rnyil8rpPurS/TTkBqmxyd9qn/IYA9bAcKyJicRKUlXNeGUzxJNYJA7RBBaNY7E4Fj3xSyv/OhKLE40licQTRGMJovH8L7RYPEE0liSW+M1zPF54jsaJRIP/YnF30ukM09MTpKYm811iUxOkU5NkpifJpqbIpCbJpibJpabIpifx9DS5wjOZKSwzDdkpLDuNZaaJZKeJ5KaJZFPEctNEc/nnitwElT5BlU9SbVOB6su6MW6VTFLJVKSK6Ugl09FqMrH8IxevJpeohWQNlqglWllLrLKWWOUK4lV1JKvqsEiEXC5DLpPFcxly2Qyey5LLZvHciXXZwroM7lnIZnHP74Nn8GwOcvl1nsvmXxc/e/6Tav51DnIZiFdx+Yf+d+B/i2IKepElanx0mOPHDjPad5iJwi8EH+0hNtFLxVQftekB6nPHqWfhhsLOuZEmSoYYGaKkLUaWGBmLkSFGhBwJTxEnRdLTJEjPy9XwlMdJWYIUcdKWeP2RiSTIWIJsJEkmXkM2VkUuXoMnaqCijkiyhkhFLfHKOuJVdSSqV1BRvYKKmhVU19RTUVVTlp+IThf0Qf4YKyILpLp2BdW1K2DzhafdL5NOkUmnSKdTZFLTZNPTpNPTZNMpspkUmVSKXDZFNj2d/1tJJlX0SOPZ6cJzCjLpwt9L0pBNQTaN5dKQS+f/RpJLE8mlIZf/W4hblFw0iUeT+S6xWEWhG6wCiyexeAWRWCWRREW++yuefx1LVBJPVuafKypJJKtIVFSSrKgiFk9SYUbFIv2cy52CXmQZiMXz3S8KRpmL8vt8IyJSZhT0IiIlTkEvIlLiFPQiIiVOQS8iUuIU9CIiJU5BLyJS4hT0IiIlbkkOgWBmfcCrc2y+Euifx3KWA73n0ldu7xf0nmdrvbvPOGvTkgz6s2FmnaU4Cfnp6D2XvnJ7v6D3PJ/UdSMiUuIU9CIiJa4Ug/7+sAsIgd5z6Su39wt6z/Om5ProRUTkjUrxil5ERIoo6EVESlzJBL2ZXW1mL5nZATO7K+x6FpqZrTWzfzOzF83seTP7aNg1LRYzi5rZf5rZv4Rdy2Iws3oz+66Z/bLw7/3WsGtaaGb2scJ/178ws380s5Kbc8XMHjCzXjP7RdG6RjP7gZn9qvDcMB/nKomgN7MosAu4BtgKXG9mW8OtasFlgP/h7ucDbwFuK4P3fMJHgRfDLmIRfQV43N3PAy6ixN+7mbUBfwZsd/cLgShwXbhVLYhvAFeftO4u4IfuvgX4YWH5rJVE0AOXAQfc/aC7p4CHgJ0h17Sg3L3H3Z8tvB4l/z9/W7hVLTwzawfeBXw17FoWg5nVATuArwG4e8rdh0ItanHEgEoziwFVQHfI9cw7d38KOH7S6p3ANwuvvwm8dz7OVSpB3wYcLlruogxC7wQz2wC8GfiPkEtZDH8LfBzIhVzHYtkI9AFfL3RXfdXMqsMuaiG5+xHgr4HXgB5g2N2fDLeqRbPK3XsgfzEHtMzHQUsl6G2GdWVx36iZ1QD/BPy5u4+EXc9CMrN3A73uvjfsWhZRDLgEuNfd3wyMM08f55eqQr/0TqADWANUm9kHwq1qeSuVoO8C1hYtt1OCH/VOZmZx8iH/D+7+cNj1LIIrgPeY2SHy3XP/xcz+PtySFlwX0OXuJz6tfZd88Jey/wq84u597p4GHgYuD7mmxXLMzFYDFJ575+OgpRL0e4AtZtZhZgnyf7h5NOSaFpSZGfl+2xfd/cth17MY3P2T7t7u7hvI/xv/P3cv6Ss9dz8KHDazcwurfh94IcSSFsNrwFvMrKrw3/nvU+J/gC7yKPAnhdd/AvzzfBw0Nh8HCZu7Z8zsduAJ8n+hf8Ddnw+5rIV2BXADsN/M9hXW/U933x1eSbJAPgL8Q+Ei5iBwY8j1LCh3/w8z+y7wLPm7y/6TEhwOwcz+EbgSWGlmXcBngL8CvmNmN5P/hfff5+VcGgJBRKS0lUrXjYiInIKCXkSkxCnoRURKnIJeRKTEKehFREqcgl5EpMQp6EVEStz/B6t1nEHrTyaSAAAAAElFTkSuQmCC"
     },
     "metadata": {
      "needs_background": "light"
     }
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ],
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\r\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\r\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\r\n<!-- Created with matplotlib (https://matplotlib.org/) -->\r\n<svg height=\"248.518125pt\" version=\"1.1\" viewBox=\"0 0 372.103125 248.518125\" width=\"372.103125pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\r\n <metadata>\r\n  <rdf:RDF xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\r\n   <cc:Work>\r\n    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\r\n    <dc:date>2021-08-22T00:59:56.347628</dc:date>\r\n    <dc:format>image/svg+xml</dc:format>\r\n    <dc:creator>\r\n     <cc:Agent>\r\n      <dc:title>Matplotlib v3.3.2, https://matplotlib.org/</dc:title>\r\n     </cc:Agent>\r\n    </dc:creator>\r\n   </cc:Work>\r\n  </rdf:RDF>\r\n </metadata>\r\n <defs>\r\n  <style type=\"text/css\">*{stroke-linecap:butt;stroke-linejoin:round;}</style>\r\n </defs>\r\n <g id=\"figure_1\">\r\n  <g id=\"patch_1\">\r\n   <path d=\"M 0 248.518125 \r\nL 372.103125 248.518125 \r\nL 372.103125 0 \r\nL 0 0 \r\nz\r\n\" style=\"fill:none;\"/>\r\n  </g>\r\n  <g id=\"axes_1\">\r\n   <g id=\"patch_2\">\r\n    <path d=\"M 30.103125 224.64 \r\nL 364.903125 224.64 \r\nL 364.903125 7.2 \r\nL 30.103125 7.2 \r\nz\r\n\" style=\"fill:#ffffff;\"/>\r\n   </g>\r\n   <g id=\"matplotlib.axis_1\">\r\n    <g id=\"xtick_1\">\r\n     <g id=\"line2d_1\">\r\n      <defs>\r\n       <path d=\"M 0 0 \r\nL 0 3.5 \r\n\" id=\"mba984698c6\" style=\"stroke:#000000;stroke-width:0.8;\"/>\r\n      </defs>\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"45.321307\" xlink:href=\"#mba984698c6\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_1\">\r\n      <!-- 0 -->\r\n      <g transform=\"translate(42.140057 239.238437)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 31.78125 66.40625 \r\nQ 24.171875 66.40625 20.328125 58.90625 \r\nQ 16.5 51.421875 16.5 36.375 \r\nQ 16.5 21.390625 20.328125 13.890625 \r\nQ 24.171875 6.390625 31.78125 6.390625 \r\nQ 39.453125 6.390625 43.28125 13.890625 \r\nQ 47.125 21.390625 47.125 36.375 \r\nQ 47.125 51.421875 43.28125 58.90625 \r\nQ 39.453125 66.40625 31.78125 66.40625 \r\nz\r\nM 31.78125 74.21875 \r\nQ 44.046875 74.21875 50.515625 64.515625 \r\nQ 56.984375 54.828125 56.984375 36.375 \r\nQ 56.984375 17.96875 50.515625 8.265625 \r\nQ 44.046875 -1.421875 31.78125 -1.421875 \r\nQ 19.53125 -1.421875 13.0625 8.265625 \r\nQ 6.59375 17.96875 6.59375 36.375 \r\nQ 6.59375 54.828125 13.0625 64.515625 \r\nQ 19.53125 74.21875 31.78125 74.21875 \r\nz\r\n\" id=\"DejaVuSans-48\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_2\">\r\n     <g id=\"line2d_2\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"106.254968\" xlink:href=\"#mba984698c6\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_2\">\r\n      <!-- 200 -->\r\n      <g transform=\"translate(96.711218 239.238437)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 19.1875 8.296875 \r\nL 53.609375 8.296875 \r\nL 53.609375 0 \r\nL 7.328125 0 \r\nL 7.328125 8.296875 \r\nQ 12.9375 14.109375 22.625 23.890625 \r\nQ 32.328125 33.6875 34.8125 36.53125 \r\nQ 39.546875 41.84375 41.421875 45.53125 \r\nQ 43.3125 49.21875 43.3125 52.78125 \r\nQ 43.3125 58.59375 39.234375 62.25 \r\nQ 35.15625 65.921875 28.609375 65.921875 \r\nQ 23.96875 65.921875 18.8125 64.3125 \r\nQ 13.671875 62.703125 7.8125 59.421875 \r\nL 7.8125 69.390625 \r\nQ 13.765625 71.78125 18.9375 73 \r\nQ 24.125 74.21875 28.421875 74.21875 \r\nQ 39.75 74.21875 46.484375 68.546875 \r\nQ 53.21875 62.890625 53.21875 53.421875 \r\nQ 53.21875 48.921875 51.53125 44.890625 \r\nQ 49.859375 40.875 45.40625 35.40625 \r\nQ 44.1875 33.984375 37.640625 27.21875 \r\nQ 31.109375 20.453125 19.1875 8.296875 \r\nz\r\n\" id=\"DejaVuSans-50\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-50\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_3\">\r\n     <g id=\"line2d_3\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"167.188629\" xlink:href=\"#mba984698c6\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_3\">\r\n      <!-- 400 -->\r\n      <g transform=\"translate(157.644879 239.238437)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 37.796875 64.3125 \r\nL 12.890625 25.390625 \r\nL 37.796875 25.390625 \r\nz\r\nM 35.203125 72.90625 \r\nL 47.609375 72.90625 \r\nL 47.609375 25.390625 \r\nL 58.015625 25.390625 \r\nL 58.015625 17.1875 \r\nL 47.609375 17.1875 \r\nL 47.609375 0 \r\nL 37.796875 0 \r\nL 37.796875 17.1875 \r\nL 4.890625 17.1875 \r\nL 4.890625 26.703125 \r\nz\r\n\" id=\"DejaVuSans-52\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-52\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_4\">\r\n     <g id=\"line2d_4\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"228.12229\" xlink:href=\"#mba984698c6\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_4\">\r\n      <!-- 600 -->\r\n      <g transform=\"translate(218.57854 239.238437)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 33.015625 40.375 \r\nQ 26.375 40.375 22.484375 35.828125 \r\nQ 18.609375 31.296875 18.609375 23.390625 \r\nQ 18.609375 15.53125 22.484375 10.953125 \r\nQ 26.375 6.390625 33.015625 6.390625 \r\nQ 39.65625 6.390625 43.53125 10.953125 \r\nQ 47.40625 15.53125 47.40625 23.390625 \r\nQ 47.40625 31.296875 43.53125 35.828125 \r\nQ 39.65625 40.375 33.015625 40.375 \r\nz\r\nM 52.59375 71.296875 \r\nL 52.59375 62.3125 \r\nQ 48.875 64.0625 45.09375 64.984375 \r\nQ 41.3125 65.921875 37.59375 65.921875 \r\nQ 27.828125 65.921875 22.671875 59.328125 \r\nQ 17.53125 52.734375 16.796875 39.40625 \r\nQ 19.671875 43.65625 24.015625 45.921875 \r\nQ 28.375 48.1875 33.59375 48.1875 \r\nQ 44.578125 48.1875 50.953125 41.515625 \r\nQ 57.328125 34.859375 57.328125 23.390625 \r\nQ 57.328125 12.15625 50.6875 5.359375 \r\nQ 44.046875 -1.421875 33.015625 -1.421875 \r\nQ 20.359375 -1.421875 13.671875 8.265625 \r\nQ 6.984375 17.96875 6.984375 36.375 \r\nQ 6.984375 53.65625 15.1875 63.9375 \r\nQ 23.390625 74.21875 37.203125 74.21875 \r\nQ 40.921875 74.21875 44.703125 73.484375 \r\nQ 48.484375 72.75 52.59375 71.296875 \r\nz\r\n\" id=\"DejaVuSans-54\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-54\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_5\">\r\n     <g id=\"line2d_5\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"289.055951\" xlink:href=\"#mba984698c6\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_5\">\r\n      <!-- 800 -->\r\n      <g transform=\"translate(279.512201 239.238437)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 31.78125 34.625 \r\nQ 24.75 34.625 20.71875 30.859375 \r\nQ 16.703125 27.09375 16.703125 20.515625 \r\nQ 16.703125 13.921875 20.71875 10.15625 \r\nQ 24.75 6.390625 31.78125 6.390625 \r\nQ 38.8125 6.390625 42.859375 10.171875 \r\nQ 46.921875 13.96875 46.921875 20.515625 \r\nQ 46.921875 27.09375 42.890625 30.859375 \r\nQ 38.875 34.625 31.78125 34.625 \r\nz\r\nM 21.921875 38.8125 \r\nQ 15.578125 40.375 12.03125 44.71875 \r\nQ 8.5 49.078125 8.5 55.328125 \r\nQ 8.5 64.0625 14.71875 69.140625 \r\nQ 20.953125 74.21875 31.78125 74.21875 \r\nQ 42.671875 74.21875 48.875 69.140625 \r\nQ 55.078125 64.0625 55.078125 55.328125 \r\nQ 55.078125 49.078125 51.53125 44.71875 \r\nQ 48 40.375 41.703125 38.8125 \r\nQ 48.828125 37.15625 52.796875 32.3125 \r\nQ 56.78125 27.484375 56.78125 20.515625 \r\nQ 56.78125 9.90625 50.3125 4.234375 \r\nQ 43.84375 -1.421875 31.78125 -1.421875 \r\nQ 19.734375 -1.421875 13.25 4.234375 \r\nQ 6.78125 9.90625 6.78125 20.515625 \r\nQ 6.78125 27.484375 10.78125 32.3125 \r\nQ 14.796875 37.15625 21.921875 38.8125 \r\nz\r\nM 18.3125 54.390625 \r\nQ 18.3125 48.734375 21.84375 45.5625 \r\nQ 25.390625 42.390625 31.78125 42.390625 \r\nQ 38.140625 42.390625 41.71875 45.5625 \r\nQ 45.3125 48.734375 45.3125 54.390625 \r\nQ 45.3125 60.0625 41.71875 63.234375 \r\nQ 38.140625 66.40625 31.78125 66.40625 \r\nQ 25.390625 66.40625 21.84375 63.234375 \r\nQ 18.3125 60.0625 18.3125 54.390625 \r\nz\r\n\" id=\"DejaVuSans-56\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-56\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_6\">\r\n     <g id=\"line2d_6\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"349.989611\" xlink:href=\"#mba984698c6\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_6\">\r\n      <!-- 1000 -->\r\n      <g transform=\"translate(337.264611 239.238437)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 12.40625 8.296875 \r\nL 28.515625 8.296875 \r\nL 28.515625 63.921875 \r\nL 10.984375 60.40625 \r\nL 10.984375 69.390625 \r\nL 28.421875 72.90625 \r\nL 38.28125 72.90625 \r\nL 38.28125 8.296875 \r\nL 54.390625 8.296875 \r\nL 54.390625 0 \r\nL 12.40625 0 \r\nz\r\n\" id=\"DejaVuSans-49\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-49\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"190.869141\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n   </g>\r\n   <g id=\"matplotlib.axis_2\">\r\n    <g id=\"ytick_1\">\r\n     <g id=\"line2d_7\">\r\n      <defs>\r\n       <path d=\"M 0 0 \r\nL -3.5 0 \r\n\" id=\"ma449df70b1\" style=\"stroke:#000000;stroke-width:0.8;\"/>\r\n      </defs>\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"30.103125\" xlink:href=\"#ma449df70b1\" y=\"220.377934\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_7\">\r\n      <!-- 0.0 -->\r\n      <g transform=\"translate(7.2 224.177153)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 10.6875 12.40625 \r\nL 21 12.40625 \r\nL 21 0 \r\nL 10.6875 0 \r\nz\r\n\" id=\"DejaVuSans-46\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_2\">\r\n     <g id=\"line2d_8\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"30.103125\" xlink:href=\"#ma449df70b1\" y=\"194.598692\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_8\">\r\n      <!-- 0.2 -->\r\n      <g transform=\"translate(7.2 198.39791)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-50\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_3\">\r\n     <g id=\"line2d_9\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"30.103125\" xlink:href=\"#ma449df70b1\" y=\"168.819449\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_9\">\r\n      <!-- 0.4 -->\r\n      <g transform=\"translate(7.2 172.618668)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-52\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_4\">\r\n     <g id=\"line2d_10\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"30.103125\" xlink:href=\"#ma449df70b1\" y=\"143.040206\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_10\">\r\n      <!-- 0.6 -->\r\n      <g transform=\"translate(7.2 146.839425)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-54\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_5\">\r\n     <g id=\"line2d_11\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"30.103125\" xlink:href=\"#ma449df70b1\" y=\"117.260964\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_11\">\r\n      <!-- 0.8 -->\r\n      <g transform=\"translate(7.2 121.060182)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-56\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_6\">\r\n     <g id=\"line2d_12\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"30.103125\" xlink:href=\"#ma449df70b1\" y=\"91.481721\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_12\">\r\n      <!-- 1.0 -->\r\n      <g transform=\"translate(7.2 95.28094)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-49\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_7\">\r\n     <g id=\"line2d_13\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"30.103125\" xlink:href=\"#ma449df70b1\" y=\"65.702478\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_13\">\r\n      <!-- 1.2 -->\r\n      <g transform=\"translate(7.2 69.501697)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-49\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-50\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_8\">\r\n     <g id=\"line2d_14\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"30.103125\" xlink:href=\"#ma449df70b1\" y=\"39.923236\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_14\">\r\n      <!-- 1.4 -->\r\n      <g transform=\"translate(7.2 43.722454)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-49\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-52\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_9\">\r\n     <g id=\"line2d_15\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"30.103125\" xlink:href=\"#ma449df70b1\" y=\"14.143993\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_15\">\r\n      <!-- 1.6 -->\r\n      <g transform=\"translate(7.2 17.943212)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-49\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-54\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n   </g>\r\n   <g id=\"line2d_16\">\r\n    <path clip-path=\"url(#p6ff2e85cbd)\" d=\"M 45.321307 17.083636 \r\nL 45.625975 103.683884 \r\nL 46.235312 167.659527 \r\nL 46.53998 187.834223 \r\nL 46.844648 199.623596 \r\nL 47.453985 210.68733 \r\nL 47.758653 213.025835 \r\nL 48.063322 212.08932 \r\nL 48.36799 212.600016 \r\nL 48.977326 212.105294 \r\nL 49.281995 212.323561 \r\nL 49.586663 212.333932 \r\nL 49.891331 212.984905 \r\nL 50.196 213.356557 \r\nL 50.500668 213.115863 \r\nL 50.805336 211.660786 \r\nL 51.110005 211.721227 \r\nL 51.414673 212.860763 \r\nL 51.719341 212.831439 \r\nL 52.02401 213.519322 \r\nL 52.328678 211.973406 \r\nL 52.633346 214.10382 \r\nL 52.938014 211.99519 \r\nL 53.242683 211.965317 \r\nL 53.547351 213.67135 \r\nL 53.852019 212.019023 \r\nL 54.156688 213.240923 \r\nL 54.461356 213.202421 \r\nL 54.766024 213.451083 \r\nL 55.070693 211.242046 \r\nL 55.375361 212.212989 \r\nL 55.680029 211.063983 \r\nL 55.984697 213.293694 \r\nL 56.289366 212.525139 \r\nL 56.594034 212.182069 \r\nL 56.898702 212.705891 \r\nL 57.203371 213.885134 \r\nL 57.508039 211.249316 \r\nL 57.812707 212.788035 \r\nL 58.117376 211.621425 \r\nL 58.422044 213.43272 \r\nL 58.726712 213.457288 \r\nL 59.031381 212.436965 \r\nL 59.336049 212.202843 \r\nL 59.640717 212.394654 \r\nL 59.945385 213.424058 \r\nL 60.250054 212.62224 \r\nL 60.554722 213.246457 \r\nL 60.85939 212.973949 \r\nL 61.164059 212.941074 \r\nL 61.468727 212.497529 \r\nL 61.773395 213.341226 \r\nL 62.078064 212.138694 \r\nL 62.382732 212.709416 \r\nL 62.6874 212.20655 \r\nL 62.992068 212.597758 \r\nL 63.296737 213.999771 \r\nL 63.601405 212.688989 \r\nL 63.906073 212.602517 \r\nL 64.210742 212.355541 \r\nL 64.51541 213.130619 \r\nL 64.820078 212.83613 \r\nL 65.124747 212.814675 \r\nL 65.429415 213.357476 \r\nL 65.734083 212.913513 \r\nL 66.038752 212.976539 \r\nL 66.648088 211.235884 \r\nL 66.952756 213.475049 \r\nL 67.562093 212.39069 \r\nL 67.866761 212.997093 \r\nL 68.17143 212.23972 \r\nL 68.476098 212.833713 \r\nL 68.780766 213.131089 \r\nL 69.085435 213.773263 \r\nL 69.390103 212.352527 \r\nL 69.694771 212.527465 \r\nL 69.999439 213.482379 \r\nL 70.304108 213.307424 \r\nL 70.608776 213.43914 \r\nL 71.218113 211.629012 \r\nL 71.522781 213.727908 \r\nL 71.827449 212.072821 \r\nL 72.132118 212.358641 \r\nL 72.436786 212.891021 \r\nL 72.741454 211.529698 \r\nL 73.046123 213.748774 \r\nL 73.350791 211.846837 \r\nL 73.655459 212.572985 \r\nL 73.960127 211.341679 \r\nL 74.264796 212.165363 \r\nL 74.569464 212.186373 \r\nL 74.874132 211.616266 \r\nL 75.178801 212.292292 \r\nL 75.483469 212.24834 \r\nL 75.788137 213.241971 \r\nL 76.397474 212.092952 \r\nL 76.702142 213.125649 \r\nL 77.006811 212.803033 \r\nL 77.311479 213.134831 \r\nL 77.616147 212.452553 \r\nL 77.920815 212.971648 \r\nL 78.225484 212.085091 \r\nL 78.83482 213.619975 \r\nL 79.444157 211.964852 \r\nL 79.748825 214.296661 \r\nL 80.053494 211.214768 \r\nL 80.358162 212.559719 \r\nL 80.967498 212.798553 \r\nL 81.272167 213.539852 \r\nL 81.576835 212.666204 \r\nL 81.881503 212.879691 \r\nL 82.186172 211.513578 \r\nL 82.49084 211.783149 \r\nL 82.795508 213.06906 \r\nL 83.100177 212.635084 \r\nL 83.404845 213.082933 \r\nL 83.709513 212.556173 \r\nL 84.014182 213.222433 \r\nL 84.31885 212.679764 \r\nL 84.623518 213.730152 \r\nL 84.928186 212.169308 \r\nL 85.232855 211.650048 \r\nL 85.537523 212.167566 \r\nL 86.451528 212.632064 \r\nL 86.756196 213.170744 \r\nL 87.060865 212.054506 \r\nL 87.365533 213.304329 \r\nL 87.670201 212.308679 \r\nL 87.974869 213.029175 \r\nL 88.584206 211.640625 \r\nL 88.888874 211.712366 \r\nL 89.193543 213.118551 \r\nL 89.498211 212.899411 \r\nL 89.802879 213.333747 \r\nL 90.107548 212.499579 \r\nL 90.412216 212.382977 \r\nL 90.716884 213.485416 \r\nL 91.326221 211.865636 \r\nL 91.630889 212.631281 \r\nL 91.935557 211.495256 \r\nL 92.240226 212.546394 \r\nL 92.544894 212.745941 \r\nL 93.154231 211.840699 \r\nL 93.458899 213.598665 \r\nL 93.763567 213.436294 \r\nL 94.068236 212.567552 \r\nL 94.372904 212.139635 \r\nL 94.677572 212.996883 \r\nL 94.98224 212.776181 \r\nL 95.286909 213.056603 \r\nL 95.591577 213.582148 \r\nL 95.896245 213.348549 \r\nL 96.200914 212.652425 \r\nL 96.505582 213.094541 \r\nL 96.81025 212.292989 \r\nL 97.114919 213.036903 \r\nL 97.419587 214.335429 \r\nL 97.724255 212.975084 \r\nL 98.028924 213.431308 \r\nL 98.333592 212.737251 \r\nL 98.63826 213.937223 \r\nL 99.247597 211.973768 \r\nL 99.552265 212.786966 \r\nL 100.161602 213.795865 \r\nL 100.46627 212.79917 \r\nL 100.770938 212.477324 \r\nL 101.075607 211.965335 \r\nL 101.380275 214.018996 \r\nL 101.684943 213.334904 \r\nL 101.989611 211.70858 \r\nL 102.29428 213.608774 \r\nL 102.598948 212.406287 \r\nL 102.903616 212.067112 \r\nL 103.208285 211.314549 \r\nL 103.817621 212.355298 \r\nL 104.12229 212.620866 \r\nL 104.426958 213.40138 \r\nL 104.731626 212.784276 \r\nL 105.036295 213.008963 \r\nL 105.340963 213.637578 \r\nL 105.645631 213.55182 \r\nL 106.254968 211.56838 \r\nL 106.559636 212.881565 \r\nL 106.864304 211.892056 \r\nL 107.168973 211.72432 \r\nL 107.473641 212.904214 \r\nL 107.778309 213.482631 \r\nL 108.082978 213.044367 \r\nL 108.387646 213.578822 \r\nL 108.692314 212.867989 \r\nL 108.996982 212.771163 \r\nL 109.301651 212.472818 \r\nL 109.606319 212.825686 \r\nL 109.910987 212.795045 \r\nL 110.215656 212.133485 \r\nL 110.520324 212.92321 \r\nL 110.824992 211.98688 \r\nL 111.129661 214.300174 \r\nL 111.434329 213.781709 \r\nL 111.738997 211.998648 \r\nL 112.043666 212.00323 \r\nL 112.348334 211.36761 \r\nL 112.653002 212.554603 \r\nL 112.95767 212.889554 \r\nL 113.262339 212.626561 \r\nL 113.567007 211.990359 \r\nL 113.871675 212.000685 \r\nL 114.176344 213.246378 \r\nL 114.481012 213.649992 \r\nL 114.78568 211.990814 \r\nL 115.090349 213.270142 \r\nL 115.699685 212.462886 \r\nL 116.004354 211.694807 \r\nL 116.309022 212.351595 \r\nL 116.61369 212.139249 \r\nL 116.918358 213.629851 \r\nL 117.223027 212.241816 \r\nL 117.832363 213.006508 \r\nL 118.137032 212.408297 \r\nL 118.4417 213.911055 \r\nL 118.746368 212.032019 \r\nL 119.051037 212.329254 \r\nL 119.355705 213.623577 \r\nL 119.965041 212.425305 \r\nL 120.26971 212.340289 \r\nL 120.574378 213.621323 \r\nL 120.879046 212.96321 \r\nL 121.183715 213.648302 \r\nL 121.488383 213.056057 \r\nL 121.793051 213.394669 \r\nL 122.09772 212.49041 \r\nL 122.402388 213.098244 \r\nL 122.707056 212.263157 \r\nL 123.011725 212.406687 \r\nL 123.316393 212.41974 \r\nL 123.925729 213.082154 \r\nL 124.230398 212.460334 \r\nL 124.535066 212.185349 \r\nL 124.839734 213.166195 \r\nL 125.144403 212.275669 \r\nL 126.058408 211.454239 \r\nL 126.363076 212.93202 \r\nL 126.667744 213.112524 \r\nL 126.972412 212.294354 \r\nL 127.277081 213.981938 \r\nL 127.581749 211.687496 \r\nL 127.886417 213.220255 \r\nL 128.191086 212.574166 \r\nL 128.495754 212.40921 \r\nL 128.800422 213.670891 \r\nL 129.105091 214.117967 \r\nL 129.409759 213.6018 \r\nL 129.714427 211.762951 \r\nL 130.323764 213.493882 \r\nL 130.628432 211.992623 \r\nL 130.9331 212.402078 \r\nL 131.237769 211.536334 \r\nL 131.542437 211.769219 \r\nL 131.847105 213.239633 \r\nL 132.151774 213.880702 \r\nL 132.456442 211.987138 \r\nL 132.76111 213.193756 \r\nL 133.065779 212.611673 \r\nL 133.370447 212.852938 \r\nL 133.675115 213.344809 \r\nL 133.979783 211.90936 \r\nL 134.284452 212.230548 \r\nL 134.58912 212.146315 \r\nL 134.893788 213.246278 \r\nL 135.198457 212.429512 \r\nL 135.503125 212.824153 \r\nL 135.807793 212.629026 \r\nL 136.112462 213.245748 \r\nL 136.41713 212.765228 \r\nL 136.721798 212.52309 \r\nL 137.331135 211.788151 \r\nL 137.635803 212.729088 \r\nL 137.940471 213.01364 \r\nL 138.24514 211.028334 \r\nL 138.549808 211.592608 \r\nL 138.854476 213.59735 \r\nL 139.159145 214.078072 \r\nL 139.768481 211.838435 \r\nL 140.07315 213.248205 \r\nL 140.377818 212.91739 \r\nL 140.682486 211.592464 \r\nL 140.987154 213.173955 \r\nL 141.291823 212.080337 \r\nL 141.596491 212.513941 \r\nL 141.901159 212.483913 \r\nL 142.205828 212.263772 \r\nL 142.510496 213.547082 \r\nL 142.815164 212.168105 \r\nL 143.119833 211.941148 \r\nL 143.729169 213.220123 \r\nL 144.033838 211.86554 \r\nL 144.338506 213.379341 \r\nL 144.643174 212.349503 \r\nL 145.252511 212.770283 \r\nL 145.557179 212.186533 \r\nL 145.861847 213.870978 \r\nL 146.166516 212.976435 \r\nL 146.471184 211.66803 \r\nL 146.775852 212.515857 \r\nL 147.080521 212.8338 \r\nL 147.385189 212.436172 \r\nL 147.689857 213.123542 \r\nL 147.994525 213.180629 \r\nL 148.299194 212.510037 \r\nL 148.603862 213.192195 \r\nL 148.90853 212.552747 \r\nL 149.213199 214.435685 \r\nL 149.517867 213.104391 \r\nL 149.822535 213.970436 \r\nL 150.127204 212.196582 \r\nL 150.73654 212.40833 \r\nL 151.041209 213.456537 \r\nL 151.345877 212.663627 \r\nL 151.650545 213.09628 \r\nL 151.955213 211.873903 \r\nL 152.259882 214.269582 \r\nL 152.56455 214.068201 \r\nL 152.869218 214.054129 \r\nL 153.478555 212.5116 \r\nL 153.783223 211.92023 \r\nL 154.087892 210.953404 \r\nL 154.39256 213.100213 \r\nL 154.697228 211.614375 \r\nL 155.001896 210.914756 \r\nL 155.306565 213.208302 \r\nL 155.611233 211.506008 \r\nL 155.915901 212.954369 \r\nL 156.22057 212.23148 \r\nL 156.525238 212.19855 \r\nL 156.829906 211.844304 \r\nL 157.134575 212.334234 \r\nL 157.439243 213.343972 \r\nL 157.743911 212.600621 \r\nL 158.04858 212.939513 \r\nL 158.353248 212.393432 \r\nL 158.657916 213.493842 \r\nL 158.962584 211.655816 \r\nL 159.267253 212.432428 \r\nL 159.571921 212.759117 \r\nL 159.876589 212.69549 \r\nL 160.181258 213.063389 \r\nL 160.485926 212.999461 \r\nL 160.790594 213.628256 \r\nL 161.095263 211.908472 \r\nL 161.399931 212.210746 \r\nL 161.704599 212.034887 \r\nL 162.009268 211.188216 \r\nL 162.313936 213.682361 \r\nL 162.618604 212.960473 \r\nL 162.923272 213.167949 \r\nL 163.227941 213.082389 \r\nL 163.532609 212.631699 \r\nL 163.837277 213.522222 \r\nL 164.141946 213.493171 \r\nL 164.446614 214.327758 \r\nL 164.751282 212.658979 \r\nL 165.055951 213.075902 \r\nL 165.360619 212.678875 \r\nL 165.665287 211.593485 \r\nL 165.969955 211.871042 \r\nL 166.274624 212.443676 \r\nL 166.579292 212.736733 \r\nL 166.88396 212.625185 \r\nL 167.188629 211.269804 \r\nL 167.797965 213.897299 \r\nL 168.102634 213.577644 \r\nL 168.407302 212.944045 \r\nL 168.71197 212.953663 \r\nL 169.016639 212.018753 \r\nL 169.321307 211.946762 \r\nL 169.625975 212.726917 \r\nL 169.930643 212.494926 \r\nL 170.235312 211.941852 \r\nL 170.53998 213.956579 \r\nL 170.844648 213.769555 \r\nL 171.149317 214.427932 \r\nL 171.453985 212.887938 \r\nL 171.758653 212.797863 \r\nL 172.063322 213.454116 \r\nL 172.36799 212.570015 \r\nL 172.672658 212.981551 \r\nL 172.977326 211.362142 \r\nL 173.281995 211.217293 \r\nL 173.586663 212.956777 \r\nL 173.891331 212.726648 \r\nL 174.196 212.282434 \r\nL 174.500668 213.078079 \r\nL 174.805336 212.606366 \r\nL 175.110005 213.106992 \r\nL 175.414673 212.706971 \r\nL 175.719341 213.159445 \r\nL 176.02401 213.014831 \r\nL 176.328678 213.30585 \r\nL 176.633346 213.859071 \r\nL 177.242683 211.942111 \r\nL 177.547351 213.074077 \r\nL 177.852019 212.761421 \r\nL 178.156688 212.270712 \r\nL 178.461356 213.175713 \r\nL 178.766024 212.625701 \r\nL 179.070693 213.517042 \r\nL 179.680029 211.752487 \r\nL 180.289366 213.276372 \r\nL 180.594034 212.310848 \r\nL 180.898702 213.135995 \r\nL 181.203371 213.150364 \r\nL 181.508039 211.979545 \r\nL 181.812707 212.017284 \r\nL 182.117376 211.567046 \r\nL 182.422044 212.710036 \r\nL 183.031381 211.664497 \r\nL 183.336049 211.960567 \r\nL 183.640717 213.271313 \r\nL 183.945385 211.920095 \r\nL 184.250054 212.848193 \r\nL 184.554722 213.124647 \r\nL 184.85939 213.82655 \r\nL 185.164059 212.218939 \r\nL 185.468727 212.398441 \r\nL 185.773395 212.156559 \r\nL 186.078064 212.270368 \r\nL 186.382732 212.834878 \r\nL 186.6874 212.456891 \r\nL 186.992068 212.640396 \r\nL 187.296737 212.958243 \r\nL 187.601405 212.493604 \r\nL 187.906073 213.4459 \r\nL 188.210742 211.685151 \r\nL 188.51541 213.057925 \r\nL 188.820078 212.442202 \r\nL 189.124747 213.214641 \r\nL 189.429415 213.626054 \r\nL 190.038752 212.755335 \r\nL 190.34342 212.99921 \r\nL 190.648088 212.78291 \r\nL 190.952756 213.061775 \r\nL 191.257425 213.088383 \r\nL 191.562093 212.227634 \r\nL 191.866761 211.841043 \r\nL 192.17143 212.670341 \r\nL 192.476098 212.257737 \r\nL 192.780766 212.454816 \r\nL 193.085435 213.371688 \r\nL 193.390103 212.948442 \r\nL 193.694771 211.870537 \r\nL 193.999439 212.729902 \r\nL 194.304108 212.817976 \r\nL 194.608776 212.731582 \r\nL 195.218113 211.484747 \r\nL 195.522781 212.527818 \r\nL 195.827449 212.242933 \r\nL 196.132118 213.049548 \r\nL 196.436786 212.904696 \r\nL 196.741454 210.721381 \r\nL 197.046123 213.815677 \r\nL 197.350791 213.222653 \r\nL 197.655459 213.34179 \r\nL 197.960127 213.729342 \r\nL 198.264796 212.339949 \r\nL 198.569464 213.849612 \r\nL 198.874132 211.743556 \r\nL 199.178801 210.94276 \r\nL 199.483469 212.777957 \r\nL 199.788137 212.93958 \r\nL 200.092806 214.101761 \r\nL 200.397474 213.495525 \r\nL 200.702142 213.260772 \r\nL 201.006811 212.67392 \r\nL 201.311479 213.268727 \r\nL 201.616147 210.985328 \r\nL 201.920815 212.492337 \r\nL 202.225484 213.158957 \r\nL 202.530152 212.908446 \r\nL 202.83482 213.715842 \r\nL 203.139489 212.466365 \r\nL 204.053494 213.321983 \r\nL 204.358162 212.683001 \r\nL 204.66283 211.293015 \r\nL 204.967498 212.534535 \r\nL 205.272167 211.809919 \r\nL 205.576835 213.2088 \r\nL 205.881503 212.916113 \r\nL 206.186172 212.997013 \r\nL 206.49084 212.721318 \r\nL 206.795508 212.953626 \r\nL 207.100177 212.249803 \r\nL 207.404845 212.726869 \r\nL 207.709513 212.316235 \r\nL 208.014182 213.28788 \r\nL 208.31885 211.907556 \r\nL 208.623518 212.783614 \r\nL 208.928186 214.050735 \r\nL 209.232855 212.27297 \r\nL 209.537523 214.044638 \r\nL 209.842191 212.283062 \r\nL 210.14686 213.771986 \r\nL 210.451528 214.423038 \r\nL 210.756196 212.293902 \r\nL 211.060865 212.507026 \r\nL 211.365533 212.344698 \r\nL 211.974869 213.661573 \r\nL 212.279538 211.806968 \r\nL 212.888874 213.195556 \r\nL 213.193543 212.960952 \r\nL 213.498211 212.920503 \r\nL 214.107548 211.092743 \r\nL 214.412216 211.265566 \r\nL 214.716884 212.383205 \r\nL 215.021553 212.710878 \r\nL 215.326221 212.348237 \r\nL 215.630889 213.02478 \r\nL 216.240226 212.867499 \r\nL 216.544894 211.984849 \r\nL 216.849562 213.182813 \r\nL 217.154231 212.947625 \r\nL 217.458899 212.003599 \r\nL 217.763567 213.254988 \r\nL 218.068236 213.053759 \r\nL 218.372904 212.288336 \r\nL 218.677572 213.2195 \r\nL 218.98224 211.424901 \r\nL 219.286909 212.255949 \r\nL 219.591577 213.568192 \r\nL 219.896245 213.251098 \r\nL 220.200914 212.514329 \r\nL 220.81025 212.853365 \r\nL 221.724255 211.982029 \r\nL 222.028924 212.845011 \r\nL 222.333592 212.288491 \r\nL 222.942928 213.736038 \r\nL 223.247597 213.036947 \r\nL 223.552265 213.196369 \r\nL 223.856933 212.636232 \r\nL 224.161602 213.581519 \r\nL 224.770938 212.664651 \r\nL 225.075607 213.090923 \r\nL 225.380275 212.808974 \r\nL 225.684943 211.655986 \r\nL 225.989611 213.485785 \r\nL 226.598948 212.320162 \r\nL 226.903616 212.274603 \r\nL 227.512953 213.170232 \r\nL 227.817621 212.132189 \r\nL 228.12229 212.641263 \r\nL 228.426958 213.795002 \r\nL 228.731626 211.475946 \r\nL 229.036295 212.862841 \r\nL 229.340963 212.931067 \r\nL 229.645631 211.836571 \r\nL 230.254968 212.948196 \r\nL 230.559636 212.117496 \r\nL 230.864304 212.259006 \r\nL 231.168973 213.130931 \r\nL 231.473641 213.575627 \r\nL 231.778309 212.492017 \r\nL 232.082978 213.840734 \r\nL 232.387646 212.177747 \r\nL 232.692314 212.0376 \r\nL 232.996982 211.162416 \r\nL 233.301651 213.226366 \r\nL 233.606319 212.579833 \r\nL 233.910987 213.672568 \r\nL 234.215656 212.513863 \r\nL 234.520324 213.237559 \r\nL 234.824992 212.105775 \r\nL 235.129661 212.065427 \r\nL 235.434329 212.865932 \r\nL 235.738997 211.289203 \r\nL 236.043666 211.692213 \r\nL 236.348334 212.847988 \r\nL 236.653002 212.517487 \r\nL 237.262339 212.791655 \r\nL 237.567007 212.617764 \r\nL 237.871675 214.197251 \r\nL 238.176344 211.622463 \r\nL 238.481012 211.190367 \r\nL 238.78568 212.815264 \r\nL 239.090349 212.833164 \r\nL 239.395017 212.200069 \r\nL 240.004354 214.756364 \r\nL 240.309022 212.879166 \r\nL 240.61369 213.864996 \r\nL 240.918358 213.556448 \r\nL 241.223027 212.326393 \r\nL 241.527695 212.222498 \r\nL 241.832363 212.982062 \r\nL 242.137032 212.387067 \r\nL 242.4417 211.368618 \r\nL 243.051037 212.40475 \r\nL 243.355705 212.958362 \r\nL 243.660373 211.780709 \r\nL 243.965041 212.398723 \r\nL 244.26971 212.1096 \r\nL 244.574378 213.739169 \r\nL 245.488383 211.884834 \r\nL 245.793051 214.496231 \r\nL 246.09772 211.985286 \r\nL 246.402388 212.673036 \r\nL 246.707056 212.507852 \r\nL 247.011725 212.741519 \r\nL 247.316393 213.443966 \r\nL 248.230398 212.909742 \r\nL 248.535066 212.466743 \r\nL 248.839734 213.67557 \r\nL 249.144403 212.636535 \r\nL 249.449071 212.479116 \r\nL 250.058408 212.574691 \r\nL 250.363076 213.86562 \r\nL 250.667744 213.279023 \r\nL 251.277081 211.340594 \r\nL 251.581749 212.221584 \r\nL 252.191086 213.013745 \r\nL 252.495754 212.772317 \r\nL 252.800422 213.352753 \r\nL 253.105091 211.594466 \r\nL 253.409759 211.72192 \r\nL 253.714427 212.717881 \r\nL 254.019096 213.209533 \r\nL 254.323764 212.288702 \r\nL 254.628432 213.667275 \r\nL 254.9331 212.355492 \r\nL 255.237769 213.720812 \r\nL 255.542437 212.347094 \r\nL 255.847105 211.929146 \r\nL 256.151774 212.080264 \r\nL 256.456442 212.768952 \r\nL 256.76111 214.272495 \r\nL 257.065779 211.640011 \r\nL 257.370447 212.979973 \r\nL 257.675115 213.544254 \r\nL 257.979783 211.470825 \r\nL 258.284452 212.027099 \r\nL 258.58912 213.007856 \r\nL 258.893788 211.430729 \r\nL 259.198457 212.879636 \r\nL 259.503125 213.047841 \r\nL 259.807793 213.601309 \r\nL 260.112462 211.584087 \r\nL 260.41713 213.259725 \r\nL 260.721798 212.298173 \r\nL 261.026467 212.018421 \r\nL 261.940471 212.438481 \r\nL 262.24514 213.789833 \r\nL 262.549808 212.894419 \r\nL 262.854476 212.434218 \r\nL 263.159145 212.393236 \r\nL 263.463813 212.787174 \r\nL 263.768481 212.143523 \r\nL 264.07315 213.092933 \r\nL 264.682486 212.995681 \r\nL 264.987154 214.065111 \r\nL 265.291823 212.477564 \r\nL 265.596491 212.305676 \r\nL 265.901159 212.862793 \r\nL 266.205828 212.494142 \r\nL 266.510496 213.05115 \r\nL 266.815164 212.896995 \r\nL 267.119833 213.569346 \r\nL 267.424501 211.024158 \r\nL 267.729169 212.462619 \r\nL 268.033838 211.127559 \r\nL 268.338506 213.205753 \r\nL 268.643174 212.346048 \r\nL 268.947842 212.955692 \r\nL 269.252511 212.590384 \r\nL 269.557179 213.218912 \r\nL 269.861847 211.976257 \r\nL 270.166516 213.52983 \r\nL 270.471184 213.379258 \r\nL 270.775852 211.810488 \r\nL 271.080521 211.942311 \r\nL 271.385189 211.7764 \r\nL 271.689857 212.439851 \r\nL 271.994525 212.413994 \r\nL 272.299194 210.394337 \r\nL 272.603862 211.740205 \r\nL 272.90853 212.014249 \r\nL 273.213199 213.303304 \r\nL 273.517867 213.070227 \r\nL 273.822535 212.292768 \r\nL 274.127204 213.246841 \r\nL 274.431872 211.850362 \r\nL 274.73654 213.307151 \r\nL 275.041209 211.239234 \r\nL 275.345877 212.387174 \r\nL 275.650545 212.357132 \r\nL 275.955213 212.70084 \r\nL 276.259882 213.810271 \r\nL 276.56455 212.645678 \r\nL 276.869218 212.52165 \r\nL 277.173887 213.337258 \r\nL 277.478555 212.607312 \r\nL 278.087892 212.365913 \r\nL 278.39256 213.742478 \r\nL 278.697228 212.748749 \r\nL 279.001896 213.621666 \r\nL 279.306565 212.429219 \r\nL 279.611233 212.158 \r\nL 279.915901 212.175006 \r\nL 280.22057 212.399809 \r\nL 280.525238 211.964087 \r\nL 280.829906 213.202956 \r\nL 281.134575 213.581177 \r\nL 281.439243 211.467542 \r\nL 282.353248 213.922395 \r\nL 282.962584 211.913708 \r\nL 283.267253 211.610502 \r\nL 283.571921 213.300229 \r\nL 283.876589 212.741256 \r\nL 284.181258 213.498294 \r\nL 284.485926 213.168322 \r\nL 284.790594 212.368914 \r\nL 285.095263 212.986093 \r\nL 285.399931 212.176033 \r\nL 285.704599 212.866858 \r\nL 286.009268 211.972723 \r\nL 286.313936 212.457205 \r\nL 286.618604 213.219991 \r\nL 286.923272 211.910462 \r\nL 287.227941 213.487709 \r\nL 287.532609 213.151234 \r\nL 287.837277 213.282694 \r\nL 288.141946 212.897294 \r\nL 288.446614 213.082734 \r\nL 288.751282 211.902505 \r\nL 289.055951 212.749076 \r\nL 289.360619 211.938923 \r\nL 289.665287 213.143551 \r\nL 289.969955 212.858838 \r\nL 290.274624 214.268752 \r\nL 290.579292 212.564981 \r\nL 290.88396 212.417467 \r\nL 291.188629 212.583217 \r\nL 291.493297 213.72016 \r\nL 292.102634 211.750762 \r\nL 292.407302 212.05484 \r\nL 292.71197 213.252645 \r\nL 293.016639 213.260942 \r\nL 293.321307 211.770507 \r\nL 293.625975 212.45439 \r\nL 293.930643 211.460236 \r\nL 294.235312 211.901651 \r\nL 294.53998 211.751182 \r\nL 294.844648 211.452027 \r\nL 295.149317 212.517803 \r\nL 295.453985 212.431167 \r\nL 295.758653 211.958061 \r\nL 296.063322 212.086812 \r\nL 296.36799 212.409208 \r\nL 296.977326 212.48207 \r\nL 297.281995 212.223129 \r\nL 297.586663 212.140943 \r\nL 297.891331 212.20189 \r\nL 298.196 212.945298 \r\nL 298.500668 211.938886 \r\nL 298.805336 214.020899 \r\nL 299.110005 212.617008 \r\nL 299.414673 212.500747 \r\nL 299.719341 212.597713 \r\nL 300.02401 212.28711 \r\nL 300.633346 212.57809 \r\nL 300.938014 212.35237 \r\nL 301.242683 211.305431 \r\nL 301.547351 212.467891 \r\nL 301.852019 212.447938 \r\nL 302.156688 211.949411 \r\nL 302.461356 213.922078 \r\nL 302.766024 212.834146 \r\nL 303.070693 213.090509 \r\nL 303.375361 212.18949 \r\nL 303.680029 211.798842 \r\nL 303.984697 213.344854 \r\nL 304.594034 211.855369 \r\nL 304.898702 212.262051 \r\nL 305.203371 213.058522 \r\nL 305.508039 211.479682 \r\nL 305.812707 211.763994 \r\nL 306.117376 213.305911 \r\nL 306.422044 211.485876 \r\nL 306.726712 211.616127 \r\nL 307.031381 212.539124 \r\nL 307.336049 212.400957 \r\nL 307.640717 212.024328 \r\nL 307.945385 212.901778 \r\nL 308.250054 213.261251 \r\nL 308.554722 212.381872 \r\nL 308.85939 212.047404 \r\nL 309.164059 211.390575 \r\nL 309.468727 213.003341 \r\nL 309.773395 212.749096 \r\nL 310.078064 213.413312 \r\nL 310.382732 212.444052 \r\nL 310.6874 212.162283 \r\nL 310.992068 213.532435 \r\nL 311.296737 212.347968 \r\nL 311.601405 212.70752 \r\nL 311.906073 212.722803 \r\nL 312.210742 212.475856 \r\nL 312.820078 212.797984 \r\nL 313.124747 213.66808 \r\nL 313.429415 212.947856 \r\nL 313.734083 211.05556 \r\nL 314.038752 212.042202 \r\nL 314.648088 212.134974 \r\nL 314.952756 212.601654 \r\nL 315.257425 212.38504 \r\nL 315.562093 212.746 \r\nL 315.866761 211.877187 \r\nL 316.17143 213.477877 \r\nL 316.476098 213.098762 \r\nL 316.780766 212.120169 \r\nL 317.085435 212.913367 \r\nL 317.390103 212.799399 \r\nL 317.694771 212.107815 \r\nL 317.999439 213.005418 \r\nL 318.304108 213.280543 \r\nL 318.608776 213.372628 \r\nL 318.913444 212.492856 \r\nL 319.218113 213.243193 \r\nL 319.522781 213.279296 \r\nL 319.827449 213.698457 \r\nL 320.132118 213.701048 \r\nL 321.046123 211.929289 \r\nL 321.350791 211.962403 \r\nL 321.655459 213.474234 \r\nL 321.960127 212.257661 \r\nL 322.264796 213.442662 \r\nL 322.569464 211.188372 \r\nL 322.874132 212.312072 \r\nL 323.178801 212.867755 \r\nL 323.483469 212.824522 \r\nL 323.788137 211.953714 \r\nL 324.092806 212.598856 \r\nL 324.397474 212.474369 \r\nL 324.702142 213.782491 \r\nL 325.006811 211.409501 \r\nL 325.311479 212.469724 \r\nL 325.616147 212.008395 \r\nL 325.920815 213.461346 \r\nL 326.225484 213.263033 \r\nL 326.530152 211.926592 \r\nL 326.83482 212.594159 \r\nL 327.139489 212.938226 \r\nL 327.444157 211.584037 \r\nL 327.748825 213.188457 \r\nL 328.053494 212.909599 \r\nL 328.358162 211.712116 \r\nL 328.66283 212.38205 \r\nL 328.967498 212.284288 \r\nL 329.272167 212.405967 \r\nL 329.576835 211.713759 \r\nL 330.49084 213.036918 \r\nL 330.795508 212.89783 \r\nL 331.100177 212.542705 \r\nL 331.404845 211.843056 \r\nL 331.709513 213.595582 \r\nL 332.014182 212.683123 \r\nL 332.31885 211.237159 \r\nL 332.623518 212.733802 \r\nL 332.928186 212.156568 \r\nL 333.232855 212.222657 \r\nL 333.537523 212.667862 \r\nL 333.842191 210.761591 \r\nL 334.14686 213.431369 \r\nL 334.451528 211.770582 \r\nL 334.756196 211.813025 \r\nL 335.060865 212.52287 \r\nL 335.365533 212.934361 \r\nL 335.670201 211.964915 \r\nL 335.974869 212.061743 \r\nL 336.584206 212.748298 \r\nL 336.888874 211.209062 \r\nL 337.193543 212.48656 \r\nL 337.498211 212.944871 \r\nL 337.802879 213.026951 \r\nL 338.107548 213.4561 \r\nL 338.412216 213.40826 \r\nL 338.716884 213.107246 \r\nL 339.021553 212.637085 \r\nL 339.935557 212.526643 \r\nL 340.240226 213.440356 \r\nL 340.849562 211.728522 \r\nL 341.154231 211.342917 \r\nL 341.458899 212.035148 \r\nL 341.763567 213.83481 \r\nL 342.068236 213.524195 \r\nL 342.372904 212.006956 \r\nL 342.98224 213.196628 \r\nL 343.286909 212.574431 \r\nL 343.591577 213.370603 \r\nL 343.896245 213.701157 \r\nL 344.200914 212.686397 \r\nL 344.505582 212.119861 \r\nL 344.81025 211.991429 \r\nL 345.114919 213.964892 \r\nL 345.419587 212.69975 \r\nL 345.724255 212.983118 \r\nL 346.333592 211.761702 \r\nL 346.63826 212.120263 \r\nL 346.942928 212.964953 \r\nL 347.247597 211.569246 \r\nL 347.552265 214.087653 \r\nL 348.161602 212.180824 \r\nL 348.770938 212.959761 \r\nL 349.075607 212.968684 \r\nL 349.380275 212.051217 \r\nL 349.684943 213.072049 \r\nL 349.684943 213.072049 \r\n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\r\n   </g>\r\n   <g id=\"line2d_17\">\r\n    <path clip-path=\"url(#p6ff2e85cbd)\" d=\"M 45.321307 17.126366 \r\nL 45.625975 103.70387 \r\nL 46.235312 167.668413 \r\nL 46.53998 187.850053 \r\nL 46.844648 199.630309 \r\nL 47.453985 210.688938 \r\nL 47.758653 213.012092 \r\nL 48.063322 212.089344 \r\nL 48.36799 212.596891 \r\nL 48.977326 212.111182 \r\nL 49.281995 212.317484 \r\nL 49.586663 212.325054 \r\nL 49.891331 212.985852 \r\nL 50.196 213.357693 \r\nL 50.500668 213.11067 \r\nL 50.805336 211.662422 \r\nL 51.110005 211.725203 \r\nL 51.414673 212.861695 \r\nL 51.719341 212.836966 \r\nL 52.02401 213.519371 \r\nL 52.328678 211.974917 \r\nL 52.633346 214.107958 \r\nL 52.938014 211.999562 \r\nL 53.242683 211.967818 \r\nL 53.547351 213.672661 \r\nL 53.852019 212.008761 \r\nL 54.156688 213.245811 \r\nL 54.461356 213.206778 \r\nL 54.766024 213.446957 \r\nL 55.070693 211.243405 \r\nL 55.375361 212.212761 \r\nL 55.680029 211.068406 \r\nL 55.984697 213.291969 \r\nL 56.289366 212.529129 \r\nL 56.594034 212.181568 \r\nL 56.898702 212.709559 \r\nL 57.203371 213.883342 \r\nL 57.508039 211.247213 \r\nL 57.812707 212.789757 \r\nL 58.117376 211.618203 \r\nL 58.422044 213.434179 \r\nL 58.726712 213.462398 \r\nL 59.031381 212.440103 \r\nL 59.336049 212.20306 \r\nL 59.640717 212.400626 \r\nL 59.945385 213.428827 \r\nL 60.250054 212.625005 \r\nL 60.554722 213.243692 \r\nL 60.85939 212.966591 \r\nL 61.164059 212.945009 \r\nL 61.468727 212.49695 \r\nL 61.773395 213.342879 \r\nL 62.078064 212.145075 \r\nL 62.382732 212.710009 \r\nL 62.6874 212.2121 \r\nL 62.992068 212.599278 \r\nL 63.296737 213.998968 \r\nL 63.601405 212.691865 \r\nL 63.906073 212.601406 \r\nL 64.210742 212.356764 \r\nL 64.51541 213.131953 \r\nL 64.820078 212.841722 \r\nL 65.124747 212.8149 \r\nL 65.429415 213.356686 \r\nL 65.734083 212.915839 \r\nL 66.038752 212.971723 \r\nL 66.648088 211.236694 \r\nL 66.952756 213.479061 \r\nL 67.562093 212.38518 \r\nL 67.866761 212.988139 \r\nL 68.17143 212.228371 \r\nL 68.476098 212.825886 \r\nL 68.780766 213.133491 \r\nL 69.085435 213.773971 \r\nL 69.390103 212.355877 \r\nL 69.694771 212.531449 \r\nL 69.999439 213.482275 \r\nL 70.304108 213.30267 \r\nL 70.608776 213.443283 \r\nL 71.218113 211.620443 \r\nL 71.522781 213.722808 \r\nL 71.827449 212.078162 \r\nL 72.132118 212.357546 \r\nL 72.436786 212.890483 \r\nL 72.741454 211.532507 \r\nL 73.046123 213.739634 \r\nL 73.350791 211.848495 \r\nL 73.655459 212.571834 \r\nL 73.960127 211.345163 \r\nL 74.264796 212.170974 \r\nL 74.569464 212.180524 \r\nL 74.874132 211.621275 \r\nL 75.178801 212.291551 \r\nL 75.483469 212.246041 \r\nL 75.788137 213.245722 \r\nL 76.397474 212.092789 \r\nL 76.702142 213.130744 \r\nL 77.006811 212.80631 \r\nL 77.311479 213.13509 \r\nL 77.616147 212.456894 \r\nL 77.920815 212.971147 \r\nL 78.225484 212.084949 \r\nL 78.83482 213.61873 \r\nL 79.444157 211.963874 \r\nL 79.748825 214.290385 \r\nL 80.053494 211.216552 \r\nL 80.358162 212.561664 \r\nL 80.967498 212.802449 \r\nL 81.272167 213.545171 \r\nL 81.576835 212.659226 \r\nL 81.881503 212.883764 \r\nL 82.186172 211.508549 \r\nL 82.49084 211.782466 \r\nL 82.795508 213.070729 \r\nL 83.100177 212.63258 \r\nL 83.404845 213.079946 \r\nL 83.709513 212.560899 \r\nL 84.014182 213.221188 \r\nL 84.31885 212.679608 \r\nL 84.623518 213.728555 \r\nL 84.928186 212.17029 \r\nL 85.232855 211.650214 \r\nL 85.537523 212.164113 \r\nL 86.451528 212.620503 \r\nL 86.756196 213.176128 \r\nL 87.060865 212.056557 \r\nL 87.365533 213.307489 \r\nL 87.670201 212.305606 \r\nL 87.974869 213.022149 \r\nL 88.584206 211.637209 \r\nL 88.888874 211.702473 \r\nL 89.193543 213.123142 \r\nL 89.498211 212.901472 \r\nL 89.802879 213.331181 \r\nL 90.107548 212.500842 \r\nL 90.412216 212.387971 \r\nL 90.716884 213.488989 \r\nL 91.326221 211.86716 \r\nL 91.630889 212.62729 \r\nL 91.935557 211.497347 \r\nL 92.240226 212.551359 \r\nL 92.544894 212.74921 \r\nL 93.154231 211.838076 \r\nL 93.458899 213.602214 \r\nL 93.763567 213.44163 \r\nL 94.068236 212.572136 \r\nL 94.372904 212.144181 \r\nL 94.677572 212.997567 \r\nL 94.98224 212.774226 \r\nL 95.286909 213.057598 \r\nL 95.591577 213.584962 \r\nL 95.896245 213.347851 \r\nL 96.200914 212.655721 \r\nL 96.505582 213.096103 \r\nL 96.81025 212.294245 \r\nL 97.114919 213.040844 \r\nL 97.419587 214.336809 \r\nL 97.724255 212.974717 \r\nL 98.028924 213.429989 \r\nL 98.333592 212.733457 \r\nL 98.63826 213.929267 \r\nL 99.247597 211.9715 \r\nL 99.552265 212.792455 \r\nL 100.161602 213.789445 \r\nL 100.46627 212.797927 \r\nL 100.770938 212.473524 \r\nL 101.075607 211.962551 \r\nL 101.380275 214.019645 \r\nL 101.684943 213.335337 \r\nL 101.989611 211.706208 \r\nL 102.29428 213.602399 \r\nL 102.598948 212.405862 \r\nL 102.903616 212.056353 \r\nL 103.208285 211.32113 \r\nL 103.817621 212.353705 \r\nL 104.12229 212.624438 \r\nL 104.426958 213.401085 \r\nL 104.731626 212.785021 \r\nL 105.036295 213.009517 \r\nL 105.340963 213.635625 \r\nL 105.645631 213.550042 \r\nL 106.254968 211.559609 \r\nL 106.559636 212.880456 \r\nL 106.864304 211.897553 \r\nL 107.168973 211.717142 \r\nL 107.473641 212.903215 \r\nL 107.778309 213.482043 \r\nL 108.082978 213.04629 \r\nL 108.387646 213.581884 \r\nL 108.692314 212.870657 \r\nL 108.996982 212.773966 \r\nL 109.301651 212.464981 \r\nL 109.606319 212.820963 \r\nL 109.910987 212.799946 \r\nL 110.215656 212.135625 \r\nL 110.520324 212.926432 \r\nL 110.824992 211.982562 \r\nL 111.129661 214.300443 \r\nL 111.434329 213.783518 \r\nL 111.738997 211.996168 \r\nL 112.043666 212.006875 \r\nL 112.348334 211.372429 \r\nL 112.653002 212.555219 \r\nL 112.95767 212.893791 \r\nL 113.262339 212.627607 \r\nL 113.567007 211.995306 \r\nL 113.871675 211.998429 \r\nL 114.176344 213.248131 \r\nL 114.481012 213.6497 \r\nL 114.78568 211.992952 \r\nL 115.090349 213.273243 \r\nL 115.699685 212.468885 \r\nL 116.004354 211.69473 \r\nL 116.309022 212.357091 \r\nL 116.61369 212.138567 \r\nL 116.918358 213.62948 \r\nL 117.223027 212.241132 \r\nL 117.832363 213.007893 \r\nL 118.137032 212.406268 \r\nL 118.4417 213.906092 \r\nL 118.746368 212.037076 \r\nL 119.051037 212.33522 \r\nL 119.355705 213.627736 \r\nL 119.965041 212.425658 \r\nL 120.26971 212.338018 \r\nL 120.574378 213.620585 \r\nL 120.879046 212.966565 \r\nL 121.183715 213.647345 \r\nL 121.488383 213.051689 \r\nL 121.793051 213.397521 \r\nL 122.09772 212.488306 \r\nL 122.402388 213.099236 \r\nL 122.707056 212.259912 \r\nL 123.011725 212.410509 \r\nL 123.316393 212.419944 \r\nL 123.925729 213.076869 \r\nL 124.230398 212.465522 \r\nL 124.535066 212.18409 \r\nL 124.839734 213.162968 \r\nL 125.144403 212.276323 \r\nL 126.058408 211.460904 \r\nL 126.363076 212.930739 \r\nL 126.667744 213.114712 \r\nL 126.972412 212.291232 \r\nL 127.277081 213.986553 \r\nL 127.581749 211.691707 \r\nL 127.886417 213.21462 \r\nL 128.191086 212.574241 \r\nL 128.495754 212.407339 \r\nL 128.800422 213.669833 \r\nL 129.105091 214.122705 \r\nL 129.409759 213.606186 \r\nL 129.714427 211.76744 \r\nL 130.323764 213.488377 \r\nL 130.628432 211.999231 \r\nL 130.9331 212.389883 \r\nL 131.237769 211.532665 \r\nL 131.542437 211.760368 \r\nL 131.847105 213.241379 \r\nL 132.151774 213.869067 \r\nL 132.456442 211.993303 \r\nL 132.76111 213.197594 \r\nL 133.065779 212.61445 \r\nL 133.370447 212.852627 \r\nL 133.675115 213.343248 \r\nL 133.979783 211.913603 \r\nL 134.284452 212.236422 \r\nL 134.58912 212.149677 \r\nL 134.893788 213.247454 \r\nL 135.198457 212.431794 \r\nL 135.503125 212.824454 \r\nL 135.807793 212.635065 \r\nL 136.112462 213.242648 \r\nL 136.41713 212.768835 \r\nL 136.721798 212.524312 \r\nL 137.331135 211.786495 \r\nL 137.635803 212.733737 \r\nL 137.940471 213.019447 \r\nL 138.24514 211.031365 \r\nL 138.549808 211.594256 \r\nL 138.854476 213.598374 \r\nL 139.159145 214.079562 \r\nL 139.768481 211.840987 \r\nL 140.07315 213.241944 \r\nL 140.377818 212.911371 \r\nL 140.682486 211.59936 \r\nL 140.987154 213.17947 \r\nL 141.291823 212.084608 \r\nL 141.596491 212.517073 \r\nL 141.901159 212.482046 \r\nL 142.205828 212.261432 \r\nL 142.510496 213.543762 \r\nL 142.815164 212.171518 \r\nL 143.119833 211.944611 \r\nL 143.729169 213.22527 \r\nL 144.033838 211.867633 \r\nL 144.338506 213.382593 \r\nL 144.643174 212.355757 \r\nL 145.252511 212.77497 \r\nL 145.557179 212.188365 \r\nL 145.861847 213.865543 \r\nL 146.166516 212.981125 \r\nL 146.471184 211.666897 \r\nL 146.775852 212.518061 \r\nL 147.080521 212.835154 \r\nL 147.385189 212.426433 \r\nL 147.689857 213.123218 \r\nL 147.994525 213.184953 \r\nL 148.299194 212.51315 \r\nL 148.603862 213.190471 \r\nL 148.90853 212.555524 \r\nL 149.213199 214.43782 \r\nL 149.517867 213.109753 \r\nL 149.822535 213.971916 \r\nL 150.127204 212.199517 \r\nL 150.73654 212.401423 \r\nL 151.041209 213.458166 \r\nL 151.345877 212.661001 \r\nL 151.650545 213.094962 \r\nL 151.955213 211.865098 \r\nL 152.259882 214.273968 \r\nL 152.56455 214.068244 \r\nL 152.869218 214.056219 \r\nL 153.478555 212.50602 \r\nL 153.783223 211.921444 \r\nL 154.087892 210.952197 \r\nL 154.39256 213.102512 \r\nL 154.697228 211.620449 \r\nL 155.001896 210.912173 \r\nL 155.306565 213.206339 \r\nL 155.611233 211.507329 \r\nL 155.915901 212.946715 \r\nL 156.22057 212.230687 \r\nL 156.525238 212.199591 \r\nL 156.829906 211.834396 \r\nL 157.134575 212.337054 \r\nL 157.439243 213.343106 \r\nL 157.743911 212.596456 \r\nL 158.04858 212.932161 \r\nL 158.353248 212.390486 \r\nL 158.657916 213.497807 \r\nL 158.962584 211.65481 \r\nL 159.267253 212.434515 \r\nL 159.571921 212.75643 \r\nL 159.876589 212.698795 \r\nL 160.181258 213.068958 \r\nL 160.485926 212.993757 \r\nL 160.790594 213.632267 \r\nL 161.095263 211.899554 \r\nL 161.399931 212.209857 \r\nL 161.704599 212.031698 \r\nL 162.009268 211.194005 \r\nL 162.313936 213.679788 \r\nL 162.618604 212.964747 \r\nL 162.923272 213.170153 \r\nL 163.227941 213.07766 \r\nL 163.532609 212.635536 \r\nL 163.837277 213.523248 \r\nL 164.141946 213.496859 \r\nL 164.446614 214.332188 \r\nL 164.751282 212.659339 \r\nL 165.055951 213.079745 \r\nL 165.360619 212.67325 \r\nL 165.665287 211.59981 \r\nL 165.969955 211.861811 \r\nL 166.274624 212.44605 \r\nL 166.579292 212.737685 \r\nL 166.88396 212.628496 \r\nL 167.188629 211.269926 \r\nL 167.797965 213.89675 \r\nL 168.102634 213.579695 \r\nL 168.407302 212.946653 \r\nL 168.71197 212.947148 \r\nL 169.016639 212.022634 \r\nL 169.321307 211.943821 \r\nL 169.625975 212.731424 \r\nL 169.930643 212.486158 \r\nL 170.235312 211.938089 \r\nL 170.53998 213.958558 \r\nL 170.844648 213.758868 \r\nL 171.149317 214.425646 \r\nL 171.453985 212.891408 \r\nL 171.758653 212.802684 \r\nL 172.063322 213.444512 \r\nL 172.36799 212.568641 \r\nL 172.672658 212.98555 \r\nL 172.977326 211.365502 \r\nL 173.281995 211.2172 \r\nL 173.586663 212.962166 \r\nL 173.891331 212.73178 \r\nL 174.196 212.284374 \r\nL 174.500668 213.082156 \r\nL 174.805336 212.611718 \r\nL 175.110005 213.096437 \r\nL 175.414673 212.695996 \r\nL 175.719341 213.156714 \r\nL 176.02401 213.010384 \r\nL 176.328678 213.306356 \r\nL 176.633346 213.862848 \r\nL 177.242683 211.937963 \r\nL 177.547351 213.07501 \r\nL 177.852019 212.753435 \r\nL 178.156688 212.27016 \r\nL 178.461356 213.168161 \r\nL 178.766024 212.622853 \r\nL 179.070693 213.522075 \r\nL 179.680029 211.745852 \r\nL 180.289366 213.27696 \r\nL 180.594034 212.30832 \r\nL 180.898702 213.13671 \r\nL 181.203371 213.148444 \r\nL 181.508039 211.982385 \r\nL 181.812707 212.008872 \r\nL 182.117376 211.569556 \r\nL 182.422044 212.710942 \r\nL 183.031381 211.661492 \r\nL 183.336049 211.959523 \r\nL 183.640717 213.268019 \r\nL 183.945385 211.924394 \r\nL 184.250054 212.844333 \r\nL 184.554722 213.128288 \r\nL 184.85939 213.823122 \r\nL 185.164059 212.221488 \r\nL 185.468727 212.403077 \r\nL 185.773395 212.149521 \r\nL 186.078064 212.276675 \r\nL 186.382732 212.837255 \r\nL 186.6874 212.447825 \r\nL 187.296737 212.961635 \r\nL 187.601405 212.493624 \r\nL 187.906073 213.445615 \r\nL 188.210742 211.67332 \r\nL 188.51541 213.053369 \r\nL 188.820078 212.441687 \r\nL 189.124747 213.218446 \r\nL 189.429415 213.619233 \r\nL 190.038752 212.757092 \r\nL 190.34342 213.000972 \r\nL 190.648088 212.788322 \r\nL 190.952756 213.06611 \r\nL 191.257425 213.083082 \r\nL 191.562093 212.229617 \r\nL 191.866761 211.839582 \r\nL 192.17143 212.670218 \r\nL 192.476098 212.245961 \r\nL 192.780766 212.458494 \r\nL 193.085435 213.365663 \r\nL 193.390103 212.950744 \r\nL 193.694771 211.876923 \r\nL 193.999439 212.728608 \r\nL 194.304108 212.81767 \r\nL 194.608776 212.736309 \r\nL 195.218113 211.483274 \r\nL 195.522781 212.526184 \r\nL 195.827449 212.244547 \r\nL 196.132118 213.054376 \r\nL 196.436786 212.907821 \r\nL 196.741454 210.724974 \r\nL 197.046123 213.816878 \r\nL 197.350791 213.226559 \r\nL 197.655459 213.338765 \r\nL 197.960127 213.720864 \r\nL 198.264796 212.342991 \r\nL 198.569464 213.840711 \r\nL 198.874132 211.742486 \r\nL 199.178801 210.949734 \r\nL 199.483469 212.780164 \r\nL 199.788137 212.942932 \r\nL 200.092806 214.104395 \r\nL 200.397474 213.498162 \r\nL 200.702142 213.255255 \r\nL 201.006811 212.670403 \r\nL 201.311479 213.267223 \r\nL 201.616147 210.979941 \r\nL 201.920815 212.493356 \r\nL 202.225484 213.156918 \r\nL 202.530152 212.91115 \r\nL 202.83482 213.71709 \r\nL 203.139489 212.462744 \r\nL 204.053494 213.325691 \r\nL 204.358162 212.688263 \r\nL 204.66283 211.293745 \r\nL 204.967498 212.526714 \r\nL 205.272167 211.810903 \r\nL 205.576835 213.212653 \r\nL 205.881503 212.916969 \r\nL 206.186172 212.994724 \r\nL 206.49084 212.726784 \r\nL 206.795508 212.954288 \r\nL 207.100177 212.255545 \r\nL 207.404845 212.721587 \r\nL 207.709513 212.317857 \r\nL 208.014182 213.292619 \r\nL 208.31885 211.906982 \r\nL 208.623518 212.788477 \r\nL 208.928186 214.055506 \r\nL 209.232855 212.273974 \r\nL 209.537523 214.046558 \r\nL 209.842191 212.282788 \r\nL 210.14686 213.769504 \r\nL 210.451528 214.426124 \r\nL 210.756196 212.299003 \r\nL 211.060865 212.511664 \r\nL 211.365533 212.34851 \r\nL 211.974869 213.659428 \r\nL 212.279538 211.797601 \r\nL 212.888874 213.196306 \r\nL 213.193543 212.961241 \r\nL 213.498211 212.922892 \r\nL 214.107548 211.084861 \r\nL 214.412216 211.264314 \r\nL 214.716884 212.383155 \r\nL 215.021553 212.709262 \r\nL 215.326221 212.339891 \r\nL 215.630889 213.027856 \r\nL 216.240226 212.862342 \r\nL 216.544894 211.985944 \r\nL 216.849562 213.181697 \r\nL 217.154231 212.947409 \r\nL 217.458899 212.007112 \r\nL 217.763567 213.260254 \r\nL 218.068236 213.057765 \r\nL 218.372904 212.288852 \r\nL 218.677572 213.201998 \r\nL 218.98224 211.423214 \r\nL 219.286909 212.256774 \r\nL 219.591577 213.570243 \r\nL 219.896245 213.250322 \r\nL 220.200914 212.515401 \r\nL 220.81025 212.839416 \r\nL 221.724255 211.988654 \r\nL 222.028924 212.846677 \r\nL 222.333592 212.290019 \r\nL 222.942928 213.731794 \r\nL 223.247597 213.037907 \r\nL 223.552265 213.198703 \r\nL 223.856933 212.63844 \r\nL 224.161602 213.58542 \r\nL 224.770938 212.6655 \r\nL 225.075607 213.083434 \r\nL 225.380275 212.813043 \r\nL 225.684943 211.650595 \r\nL 225.989611 213.479822 \r\nL 226.598948 212.322089 \r\nL 226.903616 212.265173 \r\nL 227.512953 213.172494 \r\nL 227.817621 212.127958 \r\nL 228.12229 212.642823 \r\nL 228.426958 213.782936 \r\nL 228.731626 211.478345 \r\nL 229.036295 212.862359 \r\nL 229.340963 212.928537 \r\nL 229.645631 211.833533 \r\nL 230.254968 212.940674 \r\nL 230.559636 212.116542 \r\nL 230.864304 212.257937 \r\nL 231.168973 213.129643 \r\nL 231.473641 213.576241 \r\nL 231.778309 212.490713 \r\nL 232.082978 213.843818 \r\nL 232.387646 212.181507 \r\nL 232.692314 212.034517 \r\nL 232.996982 211.15976 \r\nL 233.301651 213.220615 \r\nL 233.606319 212.572264 \r\nL 233.910987 213.660252 \r\nL 234.215656 212.505583 \r\nL 234.520324 213.237961 \r\nL 234.824992 212.105024 \r\nL 235.129661 212.065595 \r\nL 235.434329 212.867615 \r\nL 235.738997 211.295049 \r\nL 236.043666 211.698007 \r\nL 236.348334 212.841201 \r\nL 236.653002 212.511385 \r\nL 237.262339 212.786925 \r\nL 237.567007 212.619493 \r\nL 237.871675 214.189156 \r\nL 238.176344 211.615343 \r\nL 238.481012 211.194308 \r\nL 238.78568 212.802587 \r\nL 239.090349 212.835085 \r\nL 239.395017 212.190509 \r\nL 240.004354 214.755754 \r\nL 240.309022 212.884877 \r\nL 240.61369 213.868021 \r\nL 240.918358 213.561758 \r\nL 241.223027 212.325669 \r\nL 241.527695 212.223696 \r\nL 241.832363 212.986605 \r\nL 242.137032 212.391587 \r\nL 242.4417 211.373627 \r\nL 243.051037 212.406331 \r\nL 243.355705 212.959587 \r\nL 243.660373 211.773761 \r\nL 243.965041 212.404366 \r\nL 244.26971 212.107423 \r\nL 244.574378 213.724245 \r\nL 245.488383 211.879467 \r\nL 245.793051 214.494806 \r\nL 246.09772 211.978768 \r\nL 246.402388 212.67355 \r\nL 246.707056 212.507323 \r\nL 247.011725 212.746762 \r\nL 247.316393 213.446457 \r\nL 248.230398 212.912597 \r\nL 248.535066 212.471961 \r\nL 248.839734 213.676282 \r\nL 249.144403 212.640423 \r\nL 249.449071 212.479858 \r\nL 250.058408 212.571843 \r\nL 250.363076 213.862498 \r\nL 250.667744 213.27897 \r\nL 251.277081 211.339929 \r\nL 251.581749 212.221567 \r\nL 252.191086 213.019381 \r\nL 252.495754 212.764179 \r\nL 252.800422 213.358086 \r\nL 253.105091 211.591754 \r\nL 253.409759 211.719034 \r\nL 253.714427 212.708299 \r\nL 254.019096 213.208631 \r\nL 254.323764 212.291746 \r\nL 254.628432 213.672169 \r\nL 254.9331 212.352422 \r\nL 255.237769 213.718136 \r\nL 255.542437 212.337772 \r\nL 255.847105 211.93012 \r\nL 256.151774 212.086592 \r\nL 256.456442 212.766527 \r\nL 256.76111 214.277138 \r\nL 257.065779 211.641946 \r\nL 257.370447 212.976019 \r\nL 257.675115 213.546743 \r\nL 257.979783 211.474822 \r\nL 258.284452 212.026892 \r\nL 258.58912 213.010275 \r\nL 258.893788 211.433773 \r\nL 259.198457 212.876769 \r\nL 259.503125 213.042434 \r\nL 259.807793 213.605087 \r\nL 260.112462 211.588555 \r\nL 260.41713 213.253698 \r\nL 260.721798 212.300252 \r\nL 261.026467 212.021059 \r\nL 261.940471 212.428198 \r\nL 262.24514 213.789825 \r\nL 262.549808 212.893602 \r\nL 262.854476 212.427166 \r\nL 263.159145 212.385025 \r\nL 263.463813 212.788079 \r\nL 263.768481 212.144271 \r\nL 264.07315 213.09358 \r\nL 264.682486 212.99473 \r\nL 264.987154 214.066751 \r\nL 265.291823 212.480074 \r\nL 265.596491 212.310046 \r\nL 265.901159 212.865201 \r\nL 266.205828 212.492757 \r\nL 266.510496 213.053447 \r\nL 266.815164 212.892371 \r\nL 267.119833 213.568312 \r\nL 267.424501 211.029009 \r\nL 267.729169 212.459241 \r\nL 268.033838 211.117314 \r\nL 268.338506 213.19638 \r\nL 268.643174 212.348656 \r\nL 268.947842 212.957222 \r\nL 269.252511 212.590087 \r\nL 269.557179 213.212003 \r\nL 269.861847 211.979429 \r\nL 270.166516 213.526682 \r\nL 270.471184 213.372963 \r\nL 270.775852 211.799247 \r\nL 271.080521 211.945578 \r\nL 271.385189 211.78084 \r\nL 271.689857 212.423772 \r\nL 271.994525 212.413994 \r\nL 272.299194 210.39953 \r\nL 272.603862 211.744699 \r\nL 272.90853 212.02011 \r\nL 273.213199 213.304191 \r\nL 273.517867 213.073806 \r\nL 273.822535 212.290993 \r\nL 274.127204 213.242774 \r\nL 274.431872 211.855878 \r\nL 274.73654 213.303306 \r\nL 275.041209 211.244742 \r\nL 275.345877 212.38155 \r\nL 275.650545 212.351434 \r\nL 275.955213 212.702069 \r\nL 276.259882 213.812761 \r\nL 276.56455 212.644607 \r\nL 276.869218 212.521328 \r\nL 277.173887 213.339096 \r\nL 277.478555 212.61045 \r\nL 278.087892 212.361627 \r\nL 278.39256 213.729294 \r\nL 278.697228 212.754273 \r\nL 279.001896 213.621204 \r\nL 279.306565 212.433824 \r\nL 279.611233 212.157658 \r\nL 279.915901 212.168592 \r\nL 280.22057 212.401181 \r\nL 280.525238 211.963488 \r\nL 280.829906 213.208315 \r\nL 281.134575 213.583737 \r\nL 281.439243 211.465048 \r\nL 282.353248 213.919868 \r\nL 282.962584 211.918463 \r\nL 283.267253 211.614319 \r\nL 283.571921 213.299975 \r\nL 283.876589 212.738794 \r\nL 284.181258 213.497709 \r\nL 284.485926 213.173443 \r\nL 284.790594 212.368522 \r\nL 285.095263 212.990959 \r\nL 285.399931 212.178408 \r\nL 285.704599 212.860596 \r\nL 286.009268 211.97255 \r\nL 286.313936 212.450152 \r\nL 286.618604 213.218638 \r\nL 286.923272 211.915128 \r\nL 287.227941 213.486705 \r\nL 287.532609 213.156887 \r\nL 287.837277 213.283091 \r\nL 288.141946 212.900195 \r\nL 288.446614 213.087427 \r\nL 288.751282 211.903928 \r\nL 289.055951 212.749196 \r\nL 289.360619 211.936855 \r\nL 289.665287 213.145069 \r\nL 289.969955 212.862971 \r\nL 290.274624 214.266665 \r\nL 290.579292 212.569227 \r\nL 290.88396 212.406536 \r\nL 291.188629 212.584461 \r\nL 291.493297 213.720938 \r\nL 292.102634 211.754906 \r\nL 292.407302 212.059232 \r\nL 292.71197 213.252022 \r\nL 293.016639 213.250644 \r\nL 293.321307 211.77695 \r\nL 293.625975 212.454281 \r\nL 293.930643 211.465329 \r\nL 294.235312 211.898215 \r\nL 294.53998 211.745075 \r\nL 294.844648 211.444642 \r\nL 295.149317 212.519162 \r\nL 295.453985 212.436988 \r\nL 295.758653 211.961257 \r\nL 296.063322 212.091789 \r\nL 296.36799 212.409708 \r\nL 296.977326 212.478593 \r\nL 297.281995 212.221629 \r\nL 297.586663 212.145677 \r\nL 297.891331 212.200282 \r\nL 298.196 212.936027 \r\nL 298.500668 211.938289 \r\nL 298.805336 214.020798 \r\nL 299.110005 212.619883 \r\nL 299.414673 212.502659 \r\nL 299.719341 212.598834 \r\nL 300.02401 212.281395 \r\nL 300.633346 212.582646 \r\nL 300.938014 212.354933 \r\nL 301.242683 211.299774 \r\nL 301.547351 212.469865 \r\nL 301.852019 212.45409 \r\nL 302.156688 211.953609 \r\nL 302.461356 213.923309 \r\nL 302.766024 212.819804 \r\nL 303.070693 213.087498 \r\nL 303.375361 212.190521 \r\nL 303.680029 211.798365 \r\nL 303.984697 213.343208 \r\nL 304.594034 211.850979 \r\nL 304.898702 212.262565 \r\nL 305.203371 213.055134 \r\nL 305.508039 211.484529 \r\nL 305.812707 211.76615 \r\nL 306.117376 213.310516 \r\nL 306.422044 211.479451 \r\nL 306.726712 211.617708 \r\nL 307.031381 212.535077 \r\nL 307.336049 212.397454 \r\nL 307.640717 212.030556 \r\nL 307.945385 212.901953 \r\nL 308.250054 213.266095 \r\nL 308.554722 212.386755 \r\nL 308.85939 212.053035 \r\nL 309.164059 211.392034 \r\nL 309.468727 212.992105 \r\nL 309.773395 212.750565 \r\nL 310.078064 213.414146 \r\nL 310.382732 212.440617 \r\nL 310.6874 212.155001 \r\nL 310.992068 213.537685 \r\nL 311.296737 212.346953 \r\nL 311.601405 212.713113 \r\nL 311.906073 212.718044 \r\nL 312.210742 212.476088 \r\nL 312.820078 212.800149 \r\nL 313.124747 213.6596 \r\nL 313.429415 212.947873 \r\nL 313.734083 211.047872 \r\nL 314.038752 212.0408 \r\nL 314.648088 212.135052 \r\nL 314.952756 212.607193 \r\nL 315.257425 212.388409 \r\nL 315.562093 212.743008 \r\nL 315.866761 211.88365 \r\nL 316.17143 213.470456 \r\nL 316.476098 213.096977 \r\nL 316.780766 212.114496 \r\nL 317.085435 212.91543 \r\nL 317.390103 212.804705 \r\nL 317.694771 212.113797 \r\nL 317.999439 213.001272 \r\nL 318.304108 213.277002 \r\nL 318.608776 213.374812 \r\nL 318.913444 212.491318 \r\nL 319.218113 213.240866 \r\nL 319.522781 213.28394 \r\nL 319.827449 213.696092 \r\nL 320.132118 213.695662 \r\nL 321.046123 211.927807 \r\nL 321.350791 211.965361 \r\nL 321.655459 213.472505 \r\nL 321.960127 212.259902 \r\nL 322.264796 213.447075 \r\nL 322.569464 211.189022 \r\nL 322.874132 212.304781 \r\nL 323.178801 212.864807 \r\nL 323.483469 212.830345 \r\nL 323.788137 211.952285 \r\nL 324.092806 212.597821 \r\nL 324.397474 212.470362 \r\nL 324.702142 213.777026 \r\nL 325.006811 211.408301 \r\nL 325.311479 212.475937 \r\nL 325.616147 212.002562 \r\nL 325.920815 213.460677 \r\nL 326.225484 213.267894 \r\nL 326.530152 211.918996 \r\nL 326.83482 212.592972 \r\nL 327.139489 212.929117 \r\nL 327.444157 211.586409 \r\nL 327.748825 213.187578 \r\nL 328.053494 212.910272 \r\nL 328.358162 211.712284 \r\nL 328.66283 212.371279 \r\nL 328.967498 212.283176 \r\nL 329.272167 212.409471 \r\nL 329.576835 211.719237 \r\nL 330.49084 213.022299 \r\nL 330.795508 212.900018 \r\nL 331.100177 212.544597 \r\nL 331.404845 211.837421 \r\nL 331.709513 213.59269 \r\nL 332.014182 212.677388 \r\nL 332.31885 211.218574 \r\nL 332.623518 212.727774 \r\nL 332.928186 212.159442 \r\nL 333.232855 212.2276 \r\nL 333.537523 212.671938 \r\nL 333.842191 210.754734 \r\nL 334.14686 213.435466 \r\nL 334.451528 211.767139 \r\nL 334.756196 211.805861 \r\nL 335.060865 212.52867 \r\nL 335.365533 212.931687 \r\nL 335.670201 211.960101 \r\nL 335.974869 212.061222 \r\nL 336.584206 212.751109 \r\nL 336.888874 211.205621 \r\nL 337.193543 212.485141 \r\nL 337.498211 212.94836 \r\nL 337.802879 213.023817 \r\nL 338.107548 213.46062 \r\nL 338.412216 213.412546 \r\nL 338.716884 213.104737 \r\nL 339.021553 212.630938 \r\nL 339.935557 212.523939 \r\nL 340.240226 213.437183 \r\nL 340.849562 211.72898 \r\nL 341.154231 211.344493 \r\nL 341.458899 212.039601 \r\nL 341.763567 213.838105 \r\nL 342.068236 213.512092 \r\nL 342.372904 212.01343 \r\nL 342.98224 213.201683 \r\nL 343.286909 212.57343 \r\nL 343.591577 213.375636 \r\nL 343.896245 213.704764 \r\nL 344.200914 212.683495 \r\nL 344.505582 212.105614 \r\nL 344.81025 211.987707 \r\nL 345.114919 213.969499 \r\nL 345.419587 212.702214 \r\nL 345.724255 212.985354 \r\nL 346.333592 211.75725 \r\nL 346.63826 212.124743 \r\nL 346.942928 212.967071 \r\nL 347.247597 211.571522 \r\nL 347.552265 214.084817 \r\nL 348.161602 212.186427 \r\nL 348.770938 212.961222 \r\nL 349.075607 212.968487 \r\nL 349.380275 212.043332 \r\nL 349.684943 213.075215 \r\nL 349.684943 213.075215 \r\n\" style=\"fill:none;stroke:#ff7f0e;stroke-linecap:square;stroke-width:1.5;\"/>\r\n   </g>\r\n   <g id=\"patch_3\">\r\n    <path d=\"M 30.103125 224.64 \r\nL 30.103125 7.2 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_4\">\r\n    <path d=\"M 364.903125 224.64 \r\nL 364.903125 7.2 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_5\">\r\n    <path d=\"M 30.103125 224.64 \r\nL 364.903125 224.64 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_6\">\r\n    <path d=\"M 30.103125 7.2 \r\nL 364.903125 7.2 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"legend_1\">\r\n    <g id=\"patch_7\">\r\n     <path d=\"M 273.770313 44.834375 \r\nL 357.903125 44.834375 \r\nQ 359.903125 44.834375 359.903125 42.834375 \r\nL 359.903125 14.2 \r\nQ 359.903125 12.2 357.903125 12.2 \r\nL 273.770313 12.2 \r\nQ 271.770313 12.2 271.770313 14.2 \r\nL 271.770313 42.834375 \r\nQ 271.770313 44.834375 273.770313 44.834375 \r\nz\r\n\" style=\"fill:#ffffff;opacity:0.8;stroke:#cccccc;stroke-linejoin:miter;\"/>\r\n    </g>\r\n    <g id=\"line2d_18\">\r\n     <path d=\"M 275.770313 20.298437 \r\nL 295.770313 20.298437 \r\n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\r\n    </g>\r\n    <g id=\"line2d_19\"/>\r\n    <g id=\"text_16\">\r\n     <!-- loss -->\r\n     <g transform=\"translate(303.770313 23.798437)scale(0.1 -0.1)\">\r\n      <defs>\r\n       <path d=\"M 9.421875 75.984375 \r\nL 18.40625 75.984375 \r\nL 18.40625 0 \r\nL 9.421875 0 \r\nz\r\n\" id=\"DejaVuSans-108\"/>\r\n       <path d=\"M 30.609375 48.390625 \r\nQ 23.390625 48.390625 19.1875 42.75 \r\nQ 14.984375 37.109375 14.984375 27.296875 \r\nQ 14.984375 17.484375 19.15625 11.84375 \r\nQ 23.34375 6.203125 30.609375 6.203125 \r\nQ 37.796875 6.203125 41.984375 11.859375 \r\nQ 46.1875 17.53125 46.1875 27.296875 \r\nQ 46.1875 37.015625 41.984375 42.703125 \r\nQ 37.796875 48.390625 30.609375 48.390625 \r\nz\r\nM 30.609375 56 \r\nQ 42.328125 56 49.015625 48.375 \r\nQ 55.71875 40.765625 55.71875 27.296875 \r\nQ 55.71875 13.875 49.015625 6.21875 \r\nQ 42.328125 -1.421875 30.609375 -1.421875 \r\nQ 18.84375 -1.421875 12.171875 6.21875 \r\nQ 5.515625 13.875 5.515625 27.296875 \r\nQ 5.515625 40.765625 12.171875 48.375 \r\nQ 18.84375 56 30.609375 56 \r\nz\r\n\" id=\"DejaVuSans-111\"/>\r\n       <path d=\"M 44.28125 53.078125 \r\nL 44.28125 44.578125 \r\nQ 40.484375 46.53125 36.375 47.5 \r\nQ 32.28125 48.484375 27.875 48.484375 \r\nQ 21.1875 48.484375 17.84375 46.4375 \r\nQ 14.5 44.390625 14.5 40.28125 \r\nQ 14.5 37.15625 16.890625 35.375 \r\nQ 19.28125 33.59375 26.515625 31.984375 \r\nL 29.59375 31.296875 \r\nQ 39.15625 29.25 43.1875 25.515625 \r\nQ 47.21875 21.78125 47.21875 15.09375 \r\nQ 47.21875 7.46875 41.1875 3.015625 \r\nQ 35.15625 -1.421875 24.609375 -1.421875 \r\nQ 20.21875 -1.421875 15.453125 -0.5625 \r\nQ 10.6875 0.296875 5.421875 2 \r\nL 5.421875 11.28125 \r\nQ 10.40625 8.6875 15.234375 7.390625 \r\nQ 20.0625 6.109375 24.8125 6.109375 \r\nQ 31.15625 6.109375 34.5625 8.28125 \r\nQ 37.984375 10.453125 37.984375 14.40625 \r\nQ 37.984375 18.0625 35.515625 20.015625 \r\nQ 33.0625 21.96875 24.703125 23.78125 \r\nL 21.578125 24.515625 \r\nQ 13.234375 26.265625 9.515625 29.90625 \r\nQ 5.8125 33.546875 5.8125 39.890625 \r\nQ 5.8125 47.609375 11.28125 51.796875 \r\nQ 16.75 56 26.8125 56 \r\nQ 31.78125 56 36.171875 55.265625 \r\nQ 40.578125 54.546875 44.28125 53.078125 \r\nz\r\n\" id=\"DejaVuSans-115\"/>\r\n      </defs>\r\n      <use xlink:href=\"#DejaVuSans-108\"/>\r\n      <use x=\"27.783203\" xlink:href=\"#DejaVuSans-111\"/>\r\n      <use x=\"88.964844\" xlink:href=\"#DejaVuSans-115\"/>\r\n      <use x=\"141.064453\" xlink:href=\"#DejaVuSans-115\"/>\r\n     </g>\r\n    </g>\r\n    <g id=\"line2d_20\">\r\n     <path d=\"M 275.770313 34.976562 \r\nL 295.770313 34.976562 \r\n\" style=\"fill:none;stroke:#ff7f0e;stroke-linecap:square;stroke-width:1.5;\"/>\r\n    </g>\r\n    <g id=\"line2d_21\"/>\r\n    <g id=\"text_17\">\r\n     <!-- loss_rmsle -->\r\n     <g transform=\"translate(303.770313 38.476562)scale(0.1 -0.1)\">\r\n      <defs>\r\n       <path d=\"M 50.984375 -16.609375 \r\nL 50.984375 -23.578125 \r\nL -0.984375 -23.578125 \r\nL -0.984375 -16.609375 \r\nz\r\n\" id=\"DejaVuSans-95\"/>\r\n       <path d=\"M 41.109375 46.296875 \r\nQ 39.59375 47.171875 37.8125 47.578125 \r\nQ 36.03125 48 33.890625 48 \r\nQ 26.265625 48 22.1875 43.046875 \r\nQ 18.109375 38.09375 18.109375 28.8125 \r\nL 18.109375 0 \r\nL 9.078125 0 \r\nL 9.078125 54.6875 \r\nL 18.109375 54.6875 \r\nL 18.109375 46.1875 \r\nQ 20.953125 51.171875 25.484375 53.578125 \r\nQ 30.03125 56 36.53125 56 \r\nQ 37.453125 56 38.578125 55.875 \r\nQ 39.703125 55.765625 41.0625 55.515625 \r\nz\r\n\" id=\"DejaVuSans-114\"/>\r\n       <path d=\"M 52 44.1875 \r\nQ 55.375 50.25 60.0625 53.125 \r\nQ 64.75 56 71.09375 56 \r\nQ 79.640625 56 84.28125 50.015625 \r\nQ 88.921875 44.046875 88.921875 33.015625 \r\nL 88.921875 0 \r\nL 79.890625 0 \r\nL 79.890625 32.71875 \r\nQ 79.890625 40.578125 77.09375 44.375 \r\nQ 74.3125 48.1875 68.609375 48.1875 \r\nQ 61.625 48.1875 57.5625 43.546875 \r\nQ 53.515625 38.921875 53.515625 30.90625 \r\nL 53.515625 0 \r\nL 44.484375 0 \r\nL 44.484375 32.71875 \r\nQ 44.484375 40.625 41.703125 44.40625 \r\nQ 38.921875 48.1875 33.109375 48.1875 \r\nQ 26.21875 48.1875 22.15625 43.53125 \r\nQ 18.109375 38.875 18.109375 30.90625 \r\nL 18.109375 0 \r\nL 9.078125 0 \r\nL 9.078125 54.6875 \r\nL 18.109375 54.6875 \r\nL 18.109375 46.1875 \r\nQ 21.1875 51.21875 25.484375 53.609375 \r\nQ 29.78125 56 35.6875 56 \r\nQ 41.65625 56 45.828125 52.96875 \r\nQ 50 49.953125 52 44.1875 \r\nz\r\n\" id=\"DejaVuSans-109\"/>\r\n       <path d=\"M 56.203125 29.59375 \r\nL 56.203125 25.203125 \r\nL 14.890625 25.203125 \r\nQ 15.484375 15.921875 20.484375 11.0625 \r\nQ 25.484375 6.203125 34.421875 6.203125 \r\nQ 39.59375 6.203125 44.453125 7.46875 \r\nQ 49.3125 8.734375 54.109375 11.28125 \r\nL 54.109375 2.78125 \r\nQ 49.265625 0.734375 44.1875 -0.34375 \r\nQ 39.109375 -1.421875 33.890625 -1.421875 \r\nQ 20.796875 -1.421875 13.15625 6.1875 \r\nQ 5.515625 13.8125 5.515625 26.8125 \r\nQ 5.515625 40.234375 12.765625 48.109375 \r\nQ 20.015625 56 32.328125 56 \r\nQ 43.359375 56 49.78125 48.890625 \r\nQ 56.203125 41.796875 56.203125 29.59375 \r\nz\r\nM 47.21875 32.234375 \r\nQ 47.125 39.59375 43.09375 43.984375 \r\nQ 39.0625 48.390625 32.421875 48.390625 \r\nQ 24.90625 48.390625 20.390625 44.140625 \r\nQ 15.875 39.890625 15.1875 32.171875 \r\nz\r\n\" id=\"DejaVuSans-101\"/>\r\n      </defs>\r\n      <use xlink:href=\"#DejaVuSans-108\"/>\r\n      <use x=\"27.783203\" xlink:href=\"#DejaVuSans-111\"/>\r\n      <use x=\"88.964844\" xlink:href=\"#DejaVuSans-115\"/>\r\n      <use x=\"141.064453\" xlink:href=\"#DejaVuSans-115\"/>\r\n      <use x=\"193.164062\" xlink:href=\"#DejaVuSans-95\"/>\r\n      <use x=\"243.164062\" xlink:href=\"#DejaVuSans-114\"/>\r\n      <use x=\"282.527344\" xlink:href=\"#DejaVuSans-109\"/>\r\n      <use x=\"379.939453\" xlink:href=\"#DejaVuSans-115\"/>\r\n      <use x=\"432.039062\" xlink:href=\"#DejaVuSans-108\"/>\r\n      <use x=\"459.822266\" xlink:href=\"#DejaVuSans-101\"/>\r\n     </g>\r\n    </g>\r\n   </g>\r\n  </g>\r\n </g>\r\n <defs>\r\n  <clipPath id=\"p6ff2e85cbd\">\r\n   <rect height=\"217.44\" width=\"334.8\" x=\"30.103125\" y=\"7.2\"/>\r\n  </clipPath>\r\n </defs>\r\n</svg>\r\n",
      "image/png": "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"
     },
     "metadata": {
      "needs_background": "light"
     }
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ],
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\r\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\r\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\r\n<!-- Created with matplotlib (https://matplotlib.org/) -->\r\n<svg height=\"251.178341pt\" version=\"1.1\" viewBox=\"0 0 362.5625 251.178341\" width=\"362.5625pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\r\n <metadata>\r\n  <rdf:RDF xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\r\n   <cc:Work>\r\n    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\r\n    <dc:date>2021-08-22T00:59:56.499271</dc:date>\r\n    <dc:format>image/svg+xml</dc:format>\r\n    <dc:creator>\r\n     <cc:Agent>\r\n      <dc:title>Matplotlib v3.3.2, https://matplotlib.org/</dc:title>\r\n     </cc:Agent>\r\n    </dc:creator>\r\n   </cc:Work>\r\n  </rdf:RDF>\r\n </metadata>\r\n <defs>\r\n  <style type=\"text/css\">*{stroke-linecap:butt;stroke-linejoin:round;}</style>\r\n </defs>\r\n <g id=\"figure_1\">\r\n  <g id=\"patch_1\">\r\n   <path d=\"M 0 251.178341 \r\nL 362.5625 251.178341 \r\nL 362.5625 0 \r\nL 0 0 \r\nz\r\n\" style=\"fill:none;\"/>\r\n  </g>\r\n  <g id=\"axes_1\">\r\n   <g id=\"patch_2\">\r\n    <path d=\"M 20.5625 227.300216 \r\nL 355.3625 227.300216 \r\nL 355.3625 9.860216 \r\nL 20.5625 9.860216 \r\nz\r\n\" style=\"fill:#ffffff;\"/>\r\n   </g>\r\n   <g id=\"matplotlib.axis_1\">\r\n    <g id=\"xtick_1\">\r\n     <g id=\"line2d_1\">\r\n      <defs>\r\n       <path d=\"M 0 0 \r\nL 0 3.5 \r\n\" id=\"m78606af453\" style=\"stroke:#000000;stroke-width:0.8;\"/>\r\n      </defs>\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"35.780682\" xlink:href=\"#m78606af453\" y=\"227.300216\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_1\">\r\n      <!-- 0 -->\r\n      <g transform=\"translate(32.599432 241.898654)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 31.78125 66.40625 \r\nQ 24.171875 66.40625 20.328125 58.90625 \r\nQ 16.5 51.421875 16.5 36.375 \r\nQ 16.5 21.390625 20.328125 13.890625 \r\nQ 24.171875 6.390625 31.78125 6.390625 \r\nQ 39.453125 6.390625 43.28125 13.890625 \r\nQ 47.125 21.390625 47.125 36.375 \r\nQ 47.125 51.421875 43.28125 58.90625 \r\nQ 39.453125 66.40625 31.78125 66.40625 \r\nz\r\nM 31.78125 74.21875 \r\nQ 44.046875 74.21875 50.515625 64.515625 \r\nQ 56.984375 54.828125 56.984375 36.375 \r\nQ 56.984375 17.96875 50.515625 8.265625 \r\nQ 44.046875 -1.421875 31.78125 -1.421875 \r\nQ 19.53125 -1.421875 13.0625 8.265625 \r\nQ 6.59375 17.96875 6.59375 36.375 \r\nQ 6.59375 54.828125 13.0625 64.515625 \r\nQ 19.53125 74.21875 31.78125 74.21875 \r\nz\r\n\" id=\"DejaVuSans-48\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_2\">\r\n     <g id=\"line2d_2\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"96.714343\" xlink:href=\"#m78606af453\" y=\"227.300216\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_2\">\r\n      <!-- 200 -->\r\n      <g transform=\"translate(87.170593 241.898654)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 19.1875 8.296875 \r\nL 53.609375 8.296875 \r\nL 53.609375 0 \r\nL 7.328125 0 \r\nL 7.328125 8.296875 \r\nQ 12.9375 14.109375 22.625 23.890625 \r\nQ 32.328125 33.6875 34.8125 36.53125 \r\nQ 39.546875 41.84375 41.421875 45.53125 \r\nQ 43.3125 49.21875 43.3125 52.78125 \r\nQ 43.3125 58.59375 39.234375 62.25 \r\nQ 35.15625 65.921875 28.609375 65.921875 \r\nQ 23.96875 65.921875 18.8125 64.3125 \r\nQ 13.671875 62.703125 7.8125 59.421875 \r\nL 7.8125 69.390625 \r\nQ 13.765625 71.78125 18.9375 73 \r\nQ 24.125 74.21875 28.421875 74.21875 \r\nQ 39.75 74.21875 46.484375 68.546875 \r\nQ 53.21875 62.890625 53.21875 53.421875 \r\nQ 53.21875 48.921875 51.53125 44.890625 \r\nQ 49.859375 40.875 45.40625 35.40625 \r\nQ 44.1875 33.984375 37.640625 27.21875 \r\nQ 31.109375 20.453125 19.1875 8.296875 \r\nz\r\n\" id=\"DejaVuSans-50\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-50\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_3\">\r\n     <g id=\"line2d_3\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"157.648004\" xlink:href=\"#m78606af453\" y=\"227.300216\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_3\">\r\n      <!-- 400 -->\r\n      <g transform=\"translate(148.104254 241.898654)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 37.796875 64.3125 \r\nL 12.890625 25.390625 \r\nL 37.796875 25.390625 \r\nz\r\nM 35.203125 72.90625 \r\nL 47.609375 72.90625 \r\nL 47.609375 25.390625 \r\nL 58.015625 25.390625 \r\nL 58.015625 17.1875 \r\nL 47.609375 17.1875 \r\nL 47.609375 0 \r\nL 37.796875 0 \r\nL 37.796875 17.1875 \r\nL 4.890625 17.1875 \r\nL 4.890625 26.703125 \r\nz\r\n\" id=\"DejaVuSans-52\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-52\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_4\">\r\n     <g id=\"line2d_4\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"218.581665\" xlink:href=\"#m78606af453\" y=\"227.300216\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_4\">\r\n      <!-- 600 -->\r\n      <g transform=\"translate(209.037915 241.898654)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 33.015625 40.375 \r\nQ 26.375 40.375 22.484375 35.828125 \r\nQ 18.609375 31.296875 18.609375 23.390625 \r\nQ 18.609375 15.53125 22.484375 10.953125 \r\nQ 26.375 6.390625 33.015625 6.390625 \r\nQ 39.65625 6.390625 43.53125 10.953125 \r\nQ 47.40625 15.53125 47.40625 23.390625 \r\nQ 47.40625 31.296875 43.53125 35.828125 \r\nQ 39.65625 40.375 33.015625 40.375 \r\nz\r\nM 52.59375 71.296875 \r\nL 52.59375 62.3125 \r\nQ 48.875 64.0625 45.09375 64.984375 \r\nQ 41.3125 65.921875 37.59375 65.921875 \r\nQ 27.828125 65.921875 22.671875 59.328125 \r\nQ 17.53125 52.734375 16.796875 39.40625 \r\nQ 19.671875 43.65625 24.015625 45.921875 \r\nQ 28.375 48.1875 33.59375 48.1875 \r\nQ 44.578125 48.1875 50.953125 41.515625 \r\nQ 57.328125 34.859375 57.328125 23.390625 \r\nQ 57.328125 12.15625 50.6875 5.359375 \r\nQ 44.046875 -1.421875 33.015625 -1.421875 \r\nQ 20.359375 -1.421875 13.671875 8.265625 \r\nQ 6.984375 17.96875 6.984375 36.375 \r\nQ 6.984375 53.65625 15.1875 63.9375 \r\nQ 23.390625 74.21875 37.203125 74.21875 \r\nQ 40.921875 74.21875 44.703125 73.484375 \r\nQ 48.484375 72.75 52.59375 71.296875 \r\nz\r\n\" id=\"DejaVuSans-54\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-54\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_5\">\r\n     <g id=\"line2d_5\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"279.515326\" xlink:href=\"#m78606af453\" y=\"227.300216\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_5\">\r\n      <!-- 800 -->\r\n      <g transform=\"translate(269.971576 241.898654)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 31.78125 34.625 \r\nQ 24.75 34.625 20.71875 30.859375 \r\nQ 16.703125 27.09375 16.703125 20.515625 \r\nQ 16.703125 13.921875 20.71875 10.15625 \r\nQ 24.75 6.390625 31.78125 6.390625 \r\nQ 38.8125 6.390625 42.859375 10.171875 \r\nQ 46.921875 13.96875 46.921875 20.515625 \r\nQ 46.921875 27.09375 42.890625 30.859375 \r\nQ 38.875 34.625 31.78125 34.625 \r\nz\r\nM 21.921875 38.8125 \r\nQ 15.578125 40.375 12.03125 44.71875 \r\nQ 8.5 49.078125 8.5 55.328125 \r\nQ 8.5 64.0625 14.71875 69.140625 \r\nQ 20.953125 74.21875 31.78125 74.21875 \r\nQ 42.671875 74.21875 48.875 69.140625 \r\nQ 55.078125 64.0625 55.078125 55.328125 \r\nQ 55.078125 49.078125 51.53125 44.71875 \r\nQ 48 40.375 41.703125 38.8125 \r\nQ 48.828125 37.15625 52.796875 32.3125 \r\nQ 56.78125 27.484375 56.78125 20.515625 \r\nQ 56.78125 9.90625 50.3125 4.234375 \r\nQ 43.84375 -1.421875 31.78125 -1.421875 \r\nQ 19.734375 -1.421875 13.25 4.234375 \r\nQ 6.78125 9.90625 6.78125 20.515625 \r\nQ 6.78125 27.484375 10.78125 32.3125 \r\nQ 14.796875 37.15625 21.921875 38.8125 \r\nz\r\nM 18.3125 54.390625 \r\nQ 18.3125 48.734375 21.84375 45.5625 \r\nQ 25.390625 42.390625 31.78125 42.390625 \r\nQ 38.140625 42.390625 41.71875 45.5625 \r\nQ 45.3125 48.734375 45.3125 54.390625 \r\nQ 45.3125 60.0625 41.71875 63.234375 \r\nQ 38.140625 66.40625 31.78125 66.40625 \r\nQ 25.390625 66.40625 21.84375 63.234375 \r\nQ 18.3125 60.0625 18.3125 54.390625 \r\nz\r\n\" id=\"DejaVuSans-56\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-56\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_6\">\r\n     <g id=\"line2d_6\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"340.448986\" xlink:href=\"#m78606af453\" y=\"227.300216\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_6\">\r\n      <!-- 1000 -->\r\n      <g transform=\"translate(327.723986 241.898654)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 12.40625 8.296875 \r\nL 28.515625 8.296875 \r\nL 28.515625 63.921875 \r\nL 10.984375 60.40625 \r\nL 10.984375 69.390625 \r\nL 28.421875 72.90625 \r\nL 38.28125 72.90625 \r\nL 38.28125 8.296875 \r\nL 54.390625 8.296875 \r\nL 54.390625 0 \r\nL 12.40625 0 \r\nz\r\n\" id=\"DejaVuSans-49\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-49\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"190.869141\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n   </g>\r\n   <g id=\"matplotlib.axis_2\">\r\n    <g id=\"ytick_1\">\r\n     <g id=\"line2d_7\">\r\n      <defs>\r\n       <path d=\"M 0 0 \r\nL -3.5 0 \r\n\" id=\"mc7764e66bb\" style=\"stroke:#000000;stroke-width:0.8;\"/>\r\n      </defs>\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"20.5625\" xlink:href=\"#mc7764e66bb\" y=\"219.870021\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_7\">\r\n      <!-- 0 -->\r\n      <g transform=\"translate(7.2 223.66924)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_2\">\r\n     <g id=\"line2d_8\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"20.5625\" xlink:href=\"#mc7764e66bb\" y=\"178.095861\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_8\">\r\n      <!-- 1 -->\r\n      <g transform=\"translate(7.2 181.895079)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-49\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_3\">\r\n     <g id=\"line2d_9\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"20.5625\" xlink:href=\"#mc7764e66bb\" y=\"136.3217\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_9\">\r\n      <!-- 2 -->\r\n      <g transform=\"translate(7.2 140.120919)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-50\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_4\">\r\n     <g id=\"line2d_10\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"20.5625\" xlink:href=\"#mc7764e66bb\" y=\"94.54754\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_10\">\r\n      <!-- 3 -->\r\n      <g transform=\"translate(7.2 98.346758)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 40.578125 39.3125 \r\nQ 47.65625 37.796875 51.625 33 \r\nQ 55.609375 28.21875 55.609375 21.1875 \r\nQ 55.609375 10.40625 48.1875 4.484375 \r\nQ 40.765625 -1.421875 27.09375 -1.421875 \r\nQ 22.515625 -1.421875 17.65625 -0.515625 \r\nQ 12.796875 0.390625 7.625 2.203125 \r\nL 7.625 11.71875 \r\nQ 11.71875 9.328125 16.59375 8.109375 \r\nQ 21.484375 6.890625 26.8125 6.890625 \r\nQ 36.078125 6.890625 40.9375 10.546875 \r\nQ 45.796875 14.203125 45.796875 21.1875 \r\nQ 45.796875 27.640625 41.28125 31.265625 \r\nQ 36.765625 34.90625 28.71875 34.90625 \r\nL 20.21875 34.90625 \r\nL 20.21875 43.015625 \r\nL 29.109375 43.015625 \r\nQ 36.375 43.015625 40.234375 45.921875 \r\nQ 44.09375 48.828125 44.09375 54.296875 \r\nQ 44.09375 59.90625 40.109375 62.90625 \r\nQ 36.140625 65.921875 28.71875 65.921875 \r\nQ 24.65625 65.921875 20.015625 65.03125 \r\nQ 15.375 64.15625 9.8125 62.3125 \r\nL 9.8125 71.09375 \r\nQ 15.4375 72.65625 20.34375 73.4375 \r\nQ 25.25 74.21875 29.59375 74.21875 \r\nQ 40.828125 74.21875 47.359375 69.109375 \r\nQ 53.90625 64.015625 53.90625 55.328125 \r\nQ 53.90625 49.265625 50.4375 45.09375 \r\nQ 46.96875 40.921875 40.578125 39.3125 \r\nz\r\n\" id=\"DejaVuSans-51\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-51\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_5\">\r\n     <g id=\"line2d_11\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"20.5625\" xlink:href=\"#mc7764e66bb\" y=\"52.773379\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_11\">\r\n      <!-- 4 -->\r\n      <g transform=\"translate(7.2 56.572598)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-52\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_6\">\r\n     <g id=\"line2d_12\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"20.5625\" xlink:href=\"#mc7764e66bb\" y=\"10.999219\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_12\">\r\n      <!-- 5 -->\r\n      <g transform=\"translate(7.2 14.798437)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 10.796875 72.90625 \r\nL 49.515625 72.90625 \r\nL 49.515625 64.59375 \r\nL 19.828125 64.59375 \r\nL 19.828125 46.734375 \r\nQ 21.96875 47.46875 24.109375 47.828125 \r\nQ 26.265625 48.1875 28.421875 48.1875 \r\nQ 40.625 48.1875 47.75 41.5 \r\nQ 54.890625 34.8125 54.890625 23.390625 \r\nQ 54.890625 11.625 47.5625 5.09375 \r\nQ 40.234375 -1.421875 26.90625 -1.421875 \r\nQ 22.3125 -1.421875 17.546875 -0.640625 \r\nQ 12.796875 0.140625 7.71875 1.703125 \r\nL 7.71875 11.625 \r\nQ 12.109375 9.234375 16.796875 8.0625 \r\nQ 21.484375 6.890625 26.703125 6.890625 \r\nQ 35.15625 6.890625 40.078125 11.328125 \r\nQ 45.015625 15.765625 45.015625 23.390625 \r\nQ 45.015625 31 40.078125 35.4375 \r\nQ 35.15625 39.890625 26.703125 39.890625 \r\nQ 22.75 39.890625 18.8125 39.015625 \r\nQ 14.890625 38.140625 10.796875 36.28125 \r\nz\r\n\" id=\"DejaVuSans-53\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-53\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n   </g>\r\n   <g id=\"line2d_13\">\r\n    <path clip-path=\"url(#pd5305c160b)\" d=\"M 35.780682 19.743853 \r\nL 36.390018 62.809393 \r\nL 36.999355 85.354101 \r\nL 37.608692 94.292686 \r\nL 38.522697 102.27349 \r\nL 39.436701 108.292437 \r\nL 40.655375 114.793476 \r\nL 42.178716 121.468233 \r\nL 43.702058 127.122255 \r\nL 45.530068 133.011898 \r\nL 47.662746 139.028968 \r\nL 49.795424 144.388836 \r\nL 52.23277 149.918513 \r\nL 54.974785 155.554219 \r\nL 57.7168 160.694669 \r\nL 60.763483 165.925643 \r\nL 63.810166 170.724695 \r\nL 67.161517 175.57179 \r\nL 70.512869 180.023569 \r\nL 74.168888 184.486549 \r\nL 77.824908 188.591084 \r\nL 81.785596 192.686016 \r\nL 85.746284 196.46294 \r\nL 90.01164 200.221817 \r\nL 94.581665 203.941323 \r\nL 98.237684 206.716323 \r\nL 98.847021 207.157213 \r\nL 102.198372 209.534273 \r\nL 102.503041 209.621847 \r\nL 103.112377 210.112071 \r\nL 103.721714 210.49454 \r\nL 104.940387 211.334057 \r\nL 107.073065 212.409164 \r\nL 107.377733 212.847179 \r\nL 107.682402 212.732506 \r\nL 107.98707 213.125864 \r\nL 108.291738 213.002448 \r\nL 108.596407 213.294244 \r\nL 108.901075 213.397367 \r\nL 109.205743 213.728903 \r\nL 109.81508 214.025937 \r\nL 110.424416 213.931213 \r\nL 111.64309 214.852394 \r\nL 111.947758 214.756314 \r\nL 112.252426 214.958944 \r\nL 112.557095 214.56735 \r\nL 112.861763 214.676352 \r\nL 113.166431 215.046199 \r\nL 113.775768 215.237226 \r\nL 114.080436 214.950844 \r\nL 114.385104 215.556664 \r\nL 114.689773 215.104577 \r\nL 114.994441 215.786881 \r\nL 115.299109 215.840586 \r\nL 115.603778 215.424685 \r\nL 115.908446 215.887233 \r\nL 116.213114 215.158662 \r\nL 116.822451 215.690977 \r\nL 117.127119 215.737787 \r\nL 117.431787 216.195211 \r\nL 118.041124 216.051375 \r\nL 118.345792 216.082995 \r\nL 118.650461 216.234798 \r\nL 119.564466 216.03781 \r\nL 119.869134 216.133477 \r\nL 120.478471 216.01412 \r\nL 120.783139 216.535604 \r\nL 121.087807 216.297714 \r\nL 121.392475 216.858016 \r\nL 121.697144 216.483693 \r\nL 122.001812 216.299329 \r\nL 122.30648 216.469746 \r\nL 122.915817 216.499458 \r\nL 123.220485 216.777947 \r\nL 123.525154 215.992633 \r\nL 123.829822 216.836634 \r\nL 124.13449 216.313193 \r\nL 124.743827 216.664418 \r\nL 125.048495 216.307628 \r\nL 125.353163 216.164694 \r\nL 125.657832 216.152574 \r\nL 126.267168 216.83195 \r\nL 126.571837 216.605881 \r\nL 126.876505 216.922448 \r\nL 127.181173 216.558313 \r\nL 127.485842 216.932955 \r\nL 128.095178 216.275233 \r\nL 128.399846 216.968355 \r\nL 128.704515 216.762356 \r\nL 129.009183 216.720193 \r\nL 129.313851 216.857932 \r\nL 129.923188 216.48642 \r\nL 130.227856 216.526083 \r\nL 130.532525 216.044465 \r\nL 130.837193 216.643101 \r\nL 131.141861 216.294153 \r\nL 131.446529 216.504723 \r\nL 131.751198 216.875218 \r\nL 132.360534 216.479656 \r\nL 132.665203 216.910378 \r\nL 132.969871 216.519229 \r\nL 133.274539 216.501523 \r\nL 133.579208 216.180603 \r\nL 133.883876 216.711813 \r\nL 134.188544 216.720787 \r\nL 134.797881 216.361439 \r\nL 135.102549 216.755415 \r\nL 135.407217 216.869376 \r\nL 135.711886 216.253027 \r\nL 136.016554 216.572549 \r\nL 136.625891 216.65418 \r\nL 136.930559 216.025365 \r\nL 137.235227 216.741369 \r\nL 137.844564 216.422024 \r\nL 138.4539 217.014654 \r\nL 138.758569 216.550835 \r\nL 139.063237 216.575 \r\nL 139.367905 216.972083 \r\nL 139.672574 216.310951 \r\nL 139.977242 216.374391 \r\nL 140.28191 216.658824 \r\nL 140.586579 216.45915 \r\nL 140.891247 217.017793 \r\nL 142.10992 216.73972 \r\nL 142.414588 216.39149 \r\nL 142.719257 216.290932 \r\nL 143.023925 216.793424 \r\nL 143.328593 216.223736 \r\nL 143.633262 216.341347 \r\nL 143.93793 216.775571 \r\nL 144.242598 216.403844 \r\nL 144.547267 216.71717 \r\nL 144.851935 217.379849 \r\nL 145.156603 216.320073 \r\nL 145.461271 216.320897 \r\nL 145.76594 217.050537 \r\nL 146.070608 216.404352 \r\nL 146.375276 216.852495 \r\nL 146.679945 216.445728 \r\nL 146.984613 216.394377 \r\nL 147.289281 216.905347 \r\nL 147.59395 216.470377 \r\nL 147.898618 216.897892 \r\nL 148.203286 216.488688 \r\nL 148.507955 216.586156 \r\nL 148.812623 216.885459 \r\nL 149.117291 216.7984 \r\nL 149.421959 216.877096 \r\nL 150.031296 216.428536 \r\nL 150.335964 216.810546 \r\nL 150.640633 216.818355 \r\nL 150.945301 215.862304 \r\nL 151.249969 216.192438 \r\nL 151.859306 216.448926 \r\nL 152.163974 216.292186 \r\nL 152.773311 217.04113 \r\nL 153.077979 216.96601 \r\nL 153.382647 216.586173 \r\nL 153.687316 216.709555 \r\nL 153.991984 216.476675 \r\nL 154.296652 216.681942 \r\nL 155.210657 216.469231 \r\nL 155.515326 216.600169 \r\nL 155.819994 216.449361 \r\nL 156.124662 216.432535 \r\nL 156.42933 216.807608 \r\nL 156.733999 216.805251 \r\nL 157.648004 216.32137 \r\nL 157.952672 216.678418 \r\nL 158.25734 216.758783 \r\nL 158.562009 216.639694 \r\nL 158.866677 216.850427 \r\nL 159.171345 216.753514 \r\nL 159.476014 216.312317 \r\nL 159.780682 216.341873 \r\nL 160.08535 217.102808 \r\nL 160.390018 216.156373 \r\nL 160.694687 216.988942 \r\nL 160.999355 216.68225 \r\nL 161.304023 216.876321 \r\nL 161.91336 216.379475 \r\nL 162.218028 216.248683 \r\nL 162.827365 216.493778 \r\nL 163.132033 216.840366 \r\nL 163.436701 216.229974 \r\nL 164.046038 216.715259 \r\nL 164.350706 217.016871 \r\nL 164.655375 217.112681 \r\nL 164.960043 216.704143 \r\nL 165.264711 216.779999 \r\nL 165.56938 216.501147 \r\nL 165.874048 216.524766 \r\nL 166.178716 217.254552 \r\nL 166.788053 216.416652 \r\nL 167.092721 216.66251 \r\nL 167.397389 216.568012 \r\nL 167.702058 216.763219 \r\nL 168.006726 217.235403 \r\nL 168.311394 216.252172 \r\nL 168.616063 216.890608 \r\nL 169.225399 216.690806 \r\nL 169.530068 216.945445 \r\nL 169.834736 216.59256 \r\nL 170.139404 216.705753 \r\nL 170.444072 216.994326 \r\nL 170.748741 216.44135 \r\nL 171.053409 216.928919 \r\nL 171.358077 216.455522 \r\nL 171.967414 217.246278 \r\nL 172.272082 216.727006 \r\nL 172.576751 216.664534 \r\nL 173.186087 216.838525 \r\nL 173.490756 216.778643 \r\nL 173.795424 216.599106 \r\nL 174.40476 216.927057 \r\nL 174.709429 216.752627 \r\nL 175.014097 216.280656 \r\nL 175.318765 216.336399 \r\nL 175.623434 216.927404 \r\nL 175.928102 216.546613 \r\nL 176.23277 216.891565 \r\nL 176.537439 216.499047 \r\nL 176.842107 216.464459 \r\nL 177.146775 216.801551 \r\nL 177.451443 216.418158 \r\nL 177.756112 216.753163 \r\nL 178.06078 216.373097 \r\nL 178.365448 217.076138 \r\nL 178.670117 216.915016 \r\nL 178.974785 216.570212 \r\nL 179.279453 216.565639 \r\nL 179.584122 216.946574 \r\nL 179.88879 216.630281 \r\nL 180.193458 216.783015 \r\nL 181.412131 216.663842 \r\nL 181.7168 216.773283 \r\nL 182.021468 216.111156 \r\nL 182.326136 216.782406 \r\nL 182.630805 216.282188 \r\nL 182.935473 216.733301 \r\nL 183.240141 216.594315 \r\nL 183.54481 216.840704 \r\nL 183.849478 216.721775 \r\nL 184.458814 216.012271 \r\nL 184.763483 216.893273 \r\nL 185.068151 217.10472 \r\nL 185.677488 216.489202 \r\nL 186.286824 216.453509 \r\nL 186.591493 217.133232 \r\nL 186.896161 216.683232 \r\nL 187.200829 217.342345 \r\nL 187.505498 216.639336 \r\nL 187.810166 216.586116 \r\nL 188.419502 216.966561 \r\nL 189.028839 216.437991 \r\nL 189.333507 216.894117 \r\nL 189.638176 216.7468 \r\nL 189.942844 216.785065 \r\nL 190.247512 216.103306 \r\nL 190.552181 217.081049 \r\nL 191.161517 216.647101 \r\nL 191.466186 216.758887 \r\nL 191.770854 216.661004 \r\nL 192.075522 215.897203 \r\nL 192.38019 216.728054 \r\nL 192.684859 216.660888 \r\nL 192.989527 216.398989 \r\nL 193.294195 216.481417 \r\nL 193.598864 216.781412 \r\nL 193.903532 216.472943 \r\nL 194.2082 216.886605 \r\nL 194.512869 216.929049 \r\nL 194.817537 216.315379 \r\nL 195.122205 216.415247 \r\nL 195.426873 216.821055 \r\nL 195.731542 216.419247 \r\nL 196.03621 216.831214 \r\nL 196.340878 217.020666 \r\nL 196.645547 216.47881 \r\nL 197.559552 216.895587 \r\nL 197.86422 216.457594 \r\nL 198.168888 216.816515 \r\nL 198.473557 216.485708 \r\nL 198.778225 216.567582 \r\nL 199.082893 217.177723 \r\nL 199.387561 216.739714 \r\nL 199.996898 216.725104 \r\nL 200.301566 216.263405 \r\nL 200.606235 217.002598 \r\nL 201.215571 216.569291 \r\nL 201.52024 216.674608 \r\nL 201.824908 217.195298 \r\nL 202.434244 216.600062 \r\nL 202.738913 216.772195 \r\nL 203.043581 216.288676 \r\nL 203.348249 217.14735 \r\nL 203.652918 216.782199 \r\nL 203.957586 216.756869 \r\nL 204.262254 217.073504 \r\nL 204.566923 216.505841 \r\nL 204.871591 216.76906 \r\nL 205.480928 216.504768 \r\nL 205.785596 216.807268 \r\nL 206.090264 216.39275 \r\nL 206.394932 216.685806 \r\nL 206.699601 216.657838 \r\nL 207.004269 216.966387 \r\nL 207.918274 216.463764 \r\nL 208.222942 216.802571 \r\nL 208.527611 216.9177 \r\nL 208.832279 216.397697 \r\nL 209.441615 216.876546 \r\nL 209.746284 216.887631 \r\nL 210.050952 216.762934 \r\nL 210.35562 216.9176 \r\nL 210.660289 216.5798 \r\nL 210.964957 217.289479 \r\nL 211.269625 216.584575 \r\nL 211.574294 216.681244 \r\nL 211.878962 216.338404 \r\nL 212.18363 216.632381 \r\nL 212.488299 216.486419 \r\nL 213.097635 216.800299 \r\nL 213.402303 216.059435 \r\nL 213.706972 216.513834 \r\nL 214.316308 216.74657 \r\nL 214.925645 216.502424 \r\nL 215.230313 216.880094 \r\nL 215.534982 216.573517 \r\nL 215.83965 216.862328 \r\nL 216.144318 216.913656 \r\nL 216.448986 216.489093 \r\nL 216.753655 217.340826 \r\nL 217.058323 216.559124 \r\nL 217.362991 216.604481 \r\nL 217.66766 216.787317 \r\nL 217.972328 216.584382 \r\nL 218.581665 216.849218 \r\nL 218.886333 216.496705 \r\nL 219.191001 216.373858 \r\nL 219.49567 216.371825 \r\nL 219.800338 217.026602 \r\nL 220.714343 216.873136 \r\nL 221.019011 216.476811 \r\nL 221.323679 216.932502 \r\nL 221.628348 216.645563 \r\nL 222.237684 217.303322 \r\nL 222.542353 216.495559 \r\nL 222.847021 216.773627 \r\nL 223.151689 216.193547 \r\nL 223.456357 216.969485 \r\nL 223.761026 216.519823 \r\nL 224.065694 216.876978 \r\nL 224.370362 216.584352 \r\nL 224.675031 216.566473 \r\nL 224.979699 216.344003 \r\nL 225.284367 216.95686 \r\nL 225.589036 215.919205 \r\nL 225.893704 216.717991 \r\nL 226.198372 216.80856 \r\nL 226.503041 217.225309 \r\nL 226.807709 216.425381 \r\nL 227.112377 216.555583 \r\nL 227.417045 217.067794 \r\nL 227.721714 216.463646 \r\nL 228.026382 216.640947 \r\nL 228.33105 216.273854 \r\nL 228.635719 216.411866 \r\nL 230.463729 216.246219 \r\nL 230.768397 216.458655 \r\nL 231.073065 216.914321 \r\nL 231.682402 216.524435 \r\nL 231.98707 216.850728 \r\nL 232.291738 216.917003 \r\nL 232.596407 216.554824 \r\nL 232.901075 216.828436 \r\nL 233.205743 216.875723 \r\nL 233.510412 217.12028 \r\nL 233.81508 216.709737 \r\nL 234.119748 216.896818 \r\nL 235.033753 216.639307 \r\nL 235.338421 216.095275 \r\nL 235.64309 216.779675 \r\nL 235.947758 216.760324 \r\nL 236.252426 216.872842 \r\nL 236.861763 216.467653 \r\nL 237.166431 216.739347 \r\nL 237.775768 216.35359 \r\nL 238.385104 216.540114 \r\nL 238.689773 216.789915 \r\nL 238.994441 216.650686 \r\nL 239.299109 215.528085 \r\nL 239.603778 216.780205 \r\nL 239.908446 216.667782 \r\nL 240.213114 216.97838 \r\nL 240.517783 217.029825 \r\nL 240.822451 216.429467 \r\nL 241.127119 216.663753 \r\nL 241.431787 216.161542 \r\nL 241.736456 216.482857 \r\nL 242.041124 217.25333 \r\nL 242.650461 216.469185 \r\nL 243.259797 216.942846 \r\nL 243.564466 216.648343 \r\nL 243.869134 217.310756 \r\nL 244.478471 216.356028 \r\nL 244.783139 217.079557 \r\nL 245.087807 216.702018 \r\nL 245.392475 216.547584 \r\nL 245.697144 216.916458 \r\nL 246.001812 216.817621 \r\nL 246.30648 216.214592 \r\nL 246.915817 216.76476 \r\nL 247.220485 216.870603 \r\nL 247.525154 216.311789 \r\nL 247.829822 216.399215 \r\nL 248.13449 216.708673 \r\nL 248.439158 216.475994 \r\nL 248.743827 216.440627 \r\nL 249.048495 217.24024 \r\nL 249.353163 216.578923 \r\nL 249.657832 217.200933 \r\nL 249.9625 216.242449 \r\nL 250.267168 216.668532 \r\nL 250.876505 216.559323 \r\nL 251.181173 216.725978 \r\nL 251.485842 216.733671 \r\nL 251.79051 216.389098 \r\nL 252.095178 217.196626 \r\nL 252.399846 216.535853 \r\nL 252.704515 216.149346 \r\nL 253.313851 216.986004 \r\nL 253.61852 216.679934 \r\nL 253.923188 217.218116 \r\nL 254.532525 216.771836 \r\nL 254.837193 216.373669 \r\nL 255.141861 216.836219 \r\nL 255.446529 216.520539 \r\nL 255.751198 216.628069 \r\nL 256.055866 216.318714 \r\nL 256.360534 216.694684 \r\nL 256.665203 216.023784 \r\nL 256.969871 216.695775 \r\nL 257.579208 216.448109 \r\nL 257.883876 216.879558 \r\nL 258.493213 216.893824 \r\nL 258.797881 216.641752 \r\nL 259.407217 216.960196 \r\nL 259.711886 216.512447 \r\nL 260.016554 216.891484 \r\nL 260.321222 216.477275 \r\nL 260.625891 216.45715 \r\nL 260.930559 216.756819 \r\nL 261.539896 216.234318 \r\nL 261.844564 216.914226 \r\nL 262.149232 216.647009 \r\nL 262.4539 216.818533 \r\nL 262.758569 216.265285 \r\nL 263.063237 216.992464 \r\nL 263.367905 216.495459 \r\nL 263.672574 216.940638 \r\nL 263.977242 216.631735 \r\nL 264.28191 216.693622 \r\nL 264.586579 216.551206 \r\nL 264.891247 216.24579 \r\nL 265.195915 216.460526 \r\nL 265.500584 216.500412 \r\nL 265.805252 216.719242 \r\nL 266.10992 216.674931 \r\nL 266.414588 216.109518 \r\nL 266.719257 216.435568 \r\nL 267.023925 216.966435 \r\nL 267.328593 216.231142 \r\nL 267.633262 216.739627 \r\nL 267.93793 216.361907 \r\nL 268.547267 216.917718 \r\nL 268.851935 217.008925 \r\nL 269.156603 216.474742 \r\nL 269.461271 216.69944 \r\nL 269.76594 216.532267 \r\nL 270.070608 216.632153 \r\nL 270.375276 216.974277 \r\nL 270.679945 216.834351 \r\nL 270.984613 217.149289 \r\nL 271.289281 216.656598 \r\nL 271.59395 216.852283 \r\nL 271.898618 216.348056 \r\nL 272.203286 217.010555 \r\nL 272.507955 216.840476 \r\nL 272.812623 216.444939 \r\nL 273.117291 216.85371 \r\nL 273.421959 216.91084 \r\nL 273.726628 216.673857 \r\nL 274.031296 216.624013 \r\nL 274.335964 217.061767 \r\nL 274.640633 216.81609 \r\nL 275.249969 216.83304 \r\nL 275.554638 216.939238 \r\nL 275.859306 216.861039 \r\nL 276.468643 216.379763 \r\nL 276.773311 216.301305 \r\nL 277.077979 217.050335 \r\nL 277.382647 216.498589 \r\nL 277.991984 217.115704 \r\nL 278.601321 216.484029 \r\nL 278.905989 216.754658 \r\nL 279.210657 216.528546 \r\nL 279.515326 217.260318 \r\nL 279.819994 216.346876 \r\nL 280.124662 216.764542 \r\nL 280.42933 216.166861 \r\nL 280.733999 216.844087 \r\nL 281.038667 216.672526 \r\nL 281.952672 216.507405 \r\nL 282.25734 216.818619 \r\nL 282.562009 216.788817 \r\nL 282.866677 216.387286 \r\nL 283.171345 216.483556 \r\nL 283.476014 217.214297 \r\nL 283.780682 216.842135 \r\nL 284.08535 216.664066 \r\nL 284.390018 216.188234 \r\nL 284.999355 217.070176 \r\nL 285.608692 216.693296 \r\nL 285.91336 216.693216 \r\nL 286.218028 217.169706 \r\nL 286.522697 217.101795 \r\nL 287.132033 216.495719 \r\nL 287.436701 216.915134 \r\nL 287.74137 216.981329 \r\nL 288.046038 216.321202 \r\nL 288.350706 216.494302 \r\nL 288.655375 216.800035 \r\nL 288.960043 216.815993 \r\nL 289.264711 217.41658 \r\nL 289.56938 216.694388 \r\nL 289.874048 216.901895 \r\nL 290.178716 216.441134 \r\nL 290.788053 217.364706 \r\nL 291.092721 216.734575 \r\nL 291.397389 216.940015 \r\nL 292.006726 216.686197 \r\nL 292.616063 216.636602 \r\nL 292.920731 217.171219 \r\nL 293.225399 216.801194 \r\nL 293.530068 216.86489 \r\nL 294.139404 216.373209 \r\nL 294.444072 216.774302 \r\nL 294.748741 216.901894 \r\nL 295.053409 216.067471 \r\nL 295.358077 216.741307 \r\nL 295.662746 216.488801 \r\nL 296.576751 216.451937 \r\nL 296.881419 216.480183 \r\nL 297.186087 215.973349 \r\nL 297.490756 216.923336 \r\nL 297.795424 216.890574 \r\nL 298.100092 216.28494 \r\nL 298.40476 216.910302 \r\nL 299.014097 216.723591 \r\nL 299.318765 216.785176 \r\nL 299.623434 216.238476 \r\nL 299.928102 216.957712 \r\nL 300.23277 216.374621 \r\nL 301.146775 216.84785 \r\nL 301.451443 216.215422 \r\nL 301.756112 217.26422 \r\nL 302.06078 216.119489 \r\nL 302.365448 216.410794 \r\nL 302.670117 217.237078 \r\nL 303.279453 216.294307 \r\nL 303.584122 216.865997 \r\nL 303.88879 216.195893 \r\nL 304.193458 215.966976 \r\nL 304.802795 216.376004 \r\nL 305.107463 216.428883 \r\nL 305.7168 216.997335 \r\nL 306.326136 216.793426 \r\nL 306.630805 216.440112 \r\nL 306.935473 216.302393 \r\nL 307.240141 216.747189 \r\nL 307.54481 216.820857 \r\nL 307.849478 216.770288 \r\nL 308.458814 216.07667 \r\nL 308.763483 217.115456 \r\nL 309.677488 216.792652 \r\nL 309.982156 216.283483 \r\nL 310.286824 216.98609 \r\nL 310.591493 216.706009 \r\nL 311.200829 216.718368 \r\nL 311.505498 217.229353 \r\nL 311.810166 216.779254 \r\nL 312.114834 217.088181 \r\nL 312.419502 216.379678 \r\nL 313.638176 216.783472 \r\nL 313.942844 216.420696 \r\nL 314.247512 216.541752 \r\nL 314.552181 216.313744 \r\nL 315.161517 216.904302 \r\nL 315.466186 216.502496 \r\nL 315.770854 216.660144 \r\nL 316.075522 216.153827 \r\nL 316.38019 216.50779 \r\nL 316.684859 216.55724 \r\nL 316.989527 216.721128 \r\nL 317.294195 216.518486 \r\nL 317.598864 216.699624 \r\nL 318.2082 216.79979 \r\nL 318.512869 216.450134 \r\nL 318.817537 216.787795 \r\nL 319.122205 216.664009 \r\nL 319.426873 217.319445 \r\nL 319.731542 217.19363 \r\nL 320.03621 216.403145 \r\nL 320.340878 216.662508 \r\nL 320.645547 215.746578 \r\nL 320.950215 216.545459 \r\nL 321.254883 216.60003 \r\nL 321.559552 216.930261 \r\nL 322.168888 216.851103 \r\nL 322.473557 216.461784 \r\nL 322.778225 216.61975 \r\nL 323.082893 216.481774 \r\nL 323.387561 216.597644 \r\nL 323.69223 216.186983 \r\nL 323.996898 216.730836 \r\nL 324.301566 216.463198 \r\nL 324.606235 216.42487 \r\nL 324.910903 216.022757 \r\nL 325.215571 216.589007 \r\nL 325.52024 216.513487 \r\nL 325.824908 217.070532 \r\nL 326.434244 216.606946 \r\nL 326.738913 216.805022 \r\nL 327.043581 216.528086 \r\nL 327.348249 216.719213 \r\nL 327.652918 217.156791 \r\nL 327.957586 216.074542 \r\nL 328.262254 216.755113 \r\nL 328.566923 216.807821 \r\nL 328.871591 216.05678 \r\nL 329.176259 216.561633 \r\nL 329.480928 215.977972 \r\nL 329.785596 216.741695 \r\nL 330.090264 216.365551 \r\nL 330.394932 216.684575 \r\nL 330.699601 216.642133 \r\nL 331.004269 216.435536 \r\nL 331.308937 216.759575 \r\nL 331.918274 216.491435 \r\nL 332.222942 217.035153 \r\nL 332.527611 216.596327 \r\nL 332.832279 217.203269 \r\nL 333.136947 216.569316 \r\nL 333.441615 216.398981 \r\nL 334.050952 216.789821 \r\nL 334.35562 216.672061 \r\nL 334.660289 216.73662 \r\nL 334.964957 216.286704 \r\nL 335.269625 216.28065 \r\nL 336.18363 216.69471 \r\nL 336.792967 216.501644 \r\nL 337.097635 216.856759 \r\nL 337.402303 216.579209 \r\nL 337.706972 217.035638 \r\nL 338.01164 216.880556 \r\nL 338.316308 217.20902 \r\nL 338.620977 216.286623 \r\nL 339.534982 216.810735 \r\nL 339.83965 216.36015 \r\nL 340.144318 216.584857 \r\nL 340.144318 216.584857 \r\n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\r\n   </g>\r\n   <g id=\"line2d_14\">\r\n    <path clip-path=\"url(#pd5305c160b)\" d=\"M 35.780682 19.751217 \r\nL 36.390018 62.811887 \r\nL 36.999355 85.356237 \r\nL 37.608692 94.29274 \r\nL 38.522697 102.27504 \r\nL 39.436701 108.289137 \r\nL 40.655375 114.796271 \r\nL 42.178716 121.473387 \r\nL 43.702058 127.125986 \r\nL 45.530068 133.015018 \r\nL 47.662746 139.024459 \r\nL 49.795424 144.395182 \r\nL 52.537439 150.573535 \r\nL 54.670117 154.950485 \r\nL 57.412131 160.144612 \r\nL 60.458814 165.417431 \r\nL 62.896161 169.326502 \r\nL 66.247512 174.291501 \r\nL 70.2082 179.640601 \r\nL 74.473557 184.84663 \r\nL 78.129576 188.919034 \r\nL 83.004269 193.8777 \r\nL 86.35562 197.013926 \r\nL 89.706972 199.961235 \r\nL 93.66766 203.222637 \r\nL 98.237684 206.720163 \r\nL 98.847021 207.157948 \r\nL 102.198372 209.534113 \r\nL 102.503041 209.618312 \r\nL 103.112377 210.111713 \r\nL 103.721714 210.494548 \r\nL 104.940387 211.330247 \r\nL 107.073065 212.406614 \r\nL 107.377733 212.848858 \r\nL 107.682402 212.727638 \r\nL 107.98707 213.12066 \r\nL 108.291738 213.006956 \r\nL 108.596407 213.299029 \r\nL 108.901075 213.395142 \r\nL 109.205743 213.730405 \r\nL 109.81508 214.022168 \r\nL 110.424416 213.935153 \r\nL 111.64309 214.855383 \r\nL 111.947758 214.758264 \r\nL 112.252426 214.962419 \r\nL 112.557095 214.570584 \r\nL 112.861763 214.673391 \r\nL 113.166431 215.042394 \r\nL 113.775768 215.233303 \r\nL 114.080436 214.948991 \r\nL 114.385104 215.558911 \r\nL 114.689773 215.105663 \r\nL 114.994441 215.787833 \r\nL 115.299109 215.843381 \r\nL 115.603778 215.421516 \r\nL 115.908446 215.889245 \r\nL 116.213114 215.161787 \r\nL 116.822451 215.692941 \r\nL 117.127119 215.737244 \r\nL 117.431787 216.195559 \r\nL 118.041124 216.046296 \r\nL 118.345792 216.073867 \r\nL 118.650461 216.232799 \r\nL 119.564466 216.037305 \r\nL 119.869134 216.132815 \r\nL 120.478471 216.012697 \r\nL 120.783139 216.536108 \r\nL 121.087807 216.296641 \r\nL 121.392475 216.860293 \r\nL 121.697144 216.485387 \r\nL 122.001812 216.298283 \r\nL 122.30648 216.470946 \r\nL 122.915817 216.500063 \r\nL 123.220485 216.780332 \r\nL 123.525154 215.99153 \r\nL 123.829822 216.832915 \r\nL 124.13449 216.315187 \r\nL 124.743827 216.662255 \r\nL 125.048495 216.308923 \r\nL 125.353163 216.161846 \r\nL 125.657832 216.154359 \r\nL 126.267168 216.833714 \r\nL 126.571837 216.608206 \r\nL 126.876505 216.923234 \r\nL 127.181173 216.560086 \r\nL 127.485842 216.934644 \r\nL 128.095178 216.269923 \r\nL 128.399846 216.970571 \r\nL 128.704515 216.759875 \r\nL 129.009183 216.720974 \r\nL 129.313851 216.857634 \r\nL 129.923188 216.484844 \r\nL 130.227856 216.527466 \r\nL 130.532525 216.047385 \r\nL 130.837193 216.644505 \r\nL 131.141861 216.295811 \r\nL 131.446529 216.504922 \r\nL 131.751198 216.876964 \r\nL 132.360534 216.478935 \r\nL 132.665203 216.910603 \r\nL 132.969871 216.518274 \r\nL 133.274539 216.502566 \r\nL 133.579208 216.18273 \r\nL 133.883876 216.711094 \r\nL 134.188544 216.719932 \r\nL 134.797881 216.3602 \r\nL 135.102549 216.755919 \r\nL 135.407217 216.870562 \r\nL 135.711886 216.253875 \r\nL 136.016554 216.57023 \r\nL 136.625891 216.655256 \r\nL 136.930559 216.024153 \r\nL 137.235227 216.739034 \r\nL 137.844564 216.421584 \r\nL 138.4539 217.013315 \r\nL 138.758569 216.553061 \r\nL 139.063237 216.57642 \r\nL 139.367905 216.973266 \r\nL 139.672574 216.31333 \r\nL 139.977242 216.376183 \r\nL 140.28191 216.660289 \r\nL 140.586579 216.455539 \r\nL 140.891247 217.01922 \r\nL 142.10992 216.740783 \r\nL 142.414588 216.393073 \r\nL 142.719257 216.292331 \r\nL 143.023925 216.792803 \r\nL 143.328593 216.225522 \r\nL 143.633262 216.339383 \r\nL 143.93793 216.774917 \r\nL 144.242598 216.405134 \r\nL 144.547267 216.718939 \r\nL 144.851935 217.380854 \r\nL 145.156603 216.32064 \r\nL 145.461271 216.321785 \r\nL 145.76594 217.051387 \r\nL 146.070608 216.399145 \r\nL 146.375276 216.853565 \r\nL 146.679945 216.442691 \r\nL 146.984613 216.391322 \r\nL 147.289281 216.907652 \r\nL 147.59395 216.472086 \r\nL 147.898618 216.89957 \r\nL 148.203286 216.489789 \r\nL 148.507955 216.583169 \r\nL 148.812623 216.885091 \r\nL 149.117291 216.798255 \r\nL 149.421959 216.878916 \r\nL 150.031296 216.428787 \r\nL 150.335964 216.806221 \r\nL 150.640633 216.816471 \r\nL 150.945301 215.862251 \r\nL 151.249969 216.195241 \r\nL 151.859306 216.449557 \r\nL 152.163974 216.291795 \r\nL 152.773311 217.03889 \r\nL 153.077979 216.968258 \r\nL 153.382647 216.586056 \r\nL 153.687316 216.709356 \r\nL 153.991984 216.479312 \r\nL 154.296652 216.68183 \r\nL 155.210657 216.469557 \r\nL 155.515326 216.601379 \r\nL 155.819994 216.449953 \r\nL 156.124662 216.430102 \r\nL 156.42933 216.807864 \r\nL 156.733999 216.807458 \r\nL 157.648004 216.32016 \r\nL 157.952672 216.678682 \r\nL 158.25734 216.761231 \r\nL 158.562009 216.639618 \r\nL 158.866677 216.849732 \r\nL 159.171345 216.754052 \r\nL 159.476014 216.309904 \r\nL 159.780682 216.342412 \r\nL 160.08535 217.101329 \r\nL 160.390018 216.159158 \r\nL 160.694687 216.98997 \r\nL 160.999355 216.684355 \r\nL 161.304023 216.87576 \r\nL 161.91336 216.378982 \r\nL 162.218028 216.24794 \r\nL 162.827365 216.492863 \r\nL 163.132033 216.841102 \r\nL 163.436701 216.227192 \r\nL 163.74137 216.553435 \r\nL 164.046038 216.711852 \r\nL 164.350706 217.019065 \r\nL 164.655375 217.11435 \r\nL 164.960043 216.706088 \r\nL 165.264711 216.782295 \r\nL 165.56938 216.50275 \r\nL 165.874048 216.525567 \r\nL 166.178716 217.25361 \r\nL 166.788053 216.419232 \r\nL 167.092721 216.663457 \r\nL 167.397389 216.565064 \r\nL 167.702058 216.762712 \r\nL 168.006726 217.236628 \r\nL 168.311394 216.247614 \r\nL 168.616063 216.891013 \r\nL 169.225399 216.691874 \r\nL 169.530068 216.943676 \r\nL 169.834736 216.593808 \r\nL 170.139404 216.707939 \r\nL 170.444072 216.993747 \r\nL 170.748741 216.442119 \r\nL 171.053409 216.93039 \r\nL 171.358077 216.455949 \r\nL 171.967414 217.248227 \r\nL 172.272082 216.727653 \r\nL 172.576751 216.663707 \r\nL 173.186087 216.836024 \r\nL 173.490756 216.780975 \r\nL 173.795424 216.595338 \r\nL 174.40476 216.925735 \r\nL 174.709429 216.749911 \r\nL 175.014097 216.275463 \r\nL 175.318765 216.33579 \r\nL 175.623434 216.925572 \r\nL 175.928102 216.538329 \r\nL 176.23277 216.885206 \r\nL 176.537439 216.496384 \r\nL 176.842107 216.464594 \r\nL 177.146775 216.803668 \r\nL 177.451443 216.41885 \r\nL 177.756112 216.749598 \r\nL 178.06078 216.373937 \r\nL 178.365448 217.077441 \r\nL 178.670117 216.914761 \r\nL 178.974785 216.56865 \r\nL 179.279453 216.566786 \r\nL 179.584122 216.94666 \r\nL 179.88879 216.632031 \r\nL 180.193458 216.784113 \r\nL 181.412131 216.66612 \r\nL 181.7168 216.771737 \r\nL 182.021468 216.113404 \r\nL 182.326136 216.783832 \r\nL 182.630805 216.284811 \r\nL 182.935473 216.731632 \r\nL 183.240141 216.589896 \r\nL 183.54481 216.842798 \r\nL 183.849478 216.722249 \r\nL 184.458814 216.014354 \r\nL 184.763483 216.892329 \r\nL 185.068151 217.105588 \r\nL 185.677488 216.490784 \r\nL 186.286824 216.454981 \r\nL 186.591493 217.133682 \r\nL 186.896161 216.683244 \r\nL 187.200829 217.341952 \r\nL 187.505498 216.639655 \r\nL 187.810166 216.587864 \r\nL 188.419502 216.966343 \r\nL 189.028839 216.436796 \r\nL 189.333507 216.894023 \r\nL 189.638176 216.747805 \r\nL 189.942844 216.785583 \r\nL 190.247512 216.104676 \r\nL 190.552181 217.08167 \r\nL 191.161517 216.648875 \r\nL 191.466186 216.756733 \r\nL 191.770854 216.661026 \r\nL 192.075522 215.89542 \r\nL 192.38019 216.728055 \r\nL 192.684859 216.659163 \r\nL 192.989527 216.400749 \r\nL 193.294195 216.48143 \r\nL 193.598864 216.782426 \r\nL 193.903532 216.47075 \r\nL 194.2082 216.886697 \r\nL 194.512869 216.928853 \r\nL 194.817537 216.317499 \r\nL 195.122205 216.412578 \r\nL 195.426873 216.819197 \r\nL 195.731542 216.415736 \r\nL 196.03621 216.830191 \r\nL 196.340878 217.017805 \r\nL 196.645547 216.480315 \r\nL 197.559552 216.896586 \r\nL 197.86422 216.45932 \r\nL 198.168888 216.817771 \r\nL 198.473557 216.487765 \r\nL 198.778225 216.566089 \r\nL 199.082893 217.177954 \r\nL 199.387561 216.734815 \r\nL 199.996898 216.725571 \r\nL 200.301566 216.262468 \r\nL 200.606235 217.000448 \r\nL 201.215571 216.568451 \r\nL 201.52024 216.675842 \r\nL 201.824908 217.197364 \r\nL 202.434244 216.601138 \r\nL 202.738913 216.771108 \r\nL 203.043581 216.286659 \r\nL 203.348249 217.149473 \r\nL 203.652918 216.781393 \r\nL 203.957586 216.758407 \r\nL 204.262254 217.07568 \r\nL 204.566923 216.505522 \r\nL 204.871591 216.77053 \r\nL 205.480928 216.499256 \r\nL 205.785596 216.807949 \r\nL 206.090264 216.395329 \r\nL 206.394932 216.686136 \r\nL 206.699601 216.660156 \r\nL 207.004269 216.967853 \r\nL 207.918274 216.463208 \r\nL 208.222942 216.80377 \r\nL 208.527611 216.91936 \r\nL 208.832279 216.400321 \r\nL 209.441615 216.876866 \r\nL 209.746284 216.888361 \r\nL 210.050952 216.757097 \r\nL 210.35562 216.917122 \r\nL 210.660289 216.578277 \r\nL 210.964957 217.289748 \r\nL 211.269625 216.584041 \r\nL 211.574294 216.682369 \r\nL 211.878962 216.338156 \r\nL 212.18363 216.634618 \r\nL 212.488299 216.48271 \r\nL 213.097635 216.802427 \r\nL 213.402303 216.057957 \r\nL 213.706972 216.514575 \r\nL 214.316308 216.744728 \r\nL 214.925645 216.500236 \r\nL 215.230313 216.880177 \r\nL 215.534982 216.575239 \r\nL 215.83965 216.859277 \r\nL 216.144318 216.914084 \r\nL 216.448986 216.491139 \r\nL 216.753655 217.334633 \r\nL 217.058323 216.561197 \r\nL 217.362991 216.601968 \r\nL 217.66766 216.787099 \r\nL 217.972328 216.581067 \r\nL 218.581665 216.849236 \r\nL 218.886333 216.498244 \r\nL 219.49567 216.37088 \r\nL 219.800338 217.027929 \r\nL 220.714343 216.874985 \r\nL 221.019011 216.475323 \r\nL 221.323679 216.934734 \r\nL 221.628348 216.646231 \r\nL 222.237684 217.303029 \r\nL 222.542353 216.496904 \r\nL 222.847021 216.773604 \r\nL 223.151689 216.19295 \r\nL 223.456357 216.969655 \r\nL 223.761026 216.521144 \r\nL 224.065694 216.875385 \r\nL 224.370362 216.583724 \r\nL 224.675031 216.568788 \r\nL 224.979699 216.34482 \r\nL 225.284367 216.95691 \r\nL 225.589036 215.92203 \r\nL 225.893704 216.715397 \r\nL 226.198372 216.808115 \r\nL 226.503041 217.224959 \r\nL 226.807709 216.425911 \r\nL 227.112377 216.554339 \r\nL 227.417045 217.07 \r\nL 227.721714 216.465246 \r\nL 228.026382 216.639887 \r\nL 228.33105 216.276394 \r\nL 228.635719 216.410942 \r\nL 229.854392 216.341537 \r\nL 230.15906 216.441319 \r\nL 230.463729 216.245871 \r\nL 230.768397 216.460941 \r\nL 231.073065 216.91515 \r\nL 231.682402 216.523998 \r\nL 231.98707 216.853108 \r\nL 232.291738 216.916736 \r\nL 232.596407 216.556333 \r\nL 232.901075 216.829371 \r\nL 233.205743 216.87695 \r\nL 233.510412 217.121175 \r\nL 233.81508 216.710643 \r\nL 234.119748 216.897989 \r\nL 235.033753 216.63977 \r\nL 235.338421 216.09591 \r\nL 235.64309 216.781358 \r\nL 235.947758 216.760861 \r\nL 236.252426 216.874724 \r\nL 236.861763 216.467701 \r\nL 237.166431 216.740493 \r\nL 237.775768 216.356355 \r\nL 238.385104 216.542019 \r\nL 238.689773 216.788005 \r\nL 238.994441 216.652337 \r\nL 239.299109 215.525264 \r\nL 239.603778 216.778818 \r\nL 239.908446 216.669567 \r\nL 240.213114 216.980109 \r\nL 240.517783 217.030824 \r\nL 240.822451 216.432029 \r\nL 241.127119 216.664461 \r\nL 241.431787 216.162676 \r\nL 241.736456 216.482842 \r\nL 242.041124 217.255235 \r\nL 242.650461 216.467703 \r\nL 243.259797 216.943542 \r\nL 243.564466 216.644014 \r\nL 243.869134 217.312033 \r\nL 244.478471 216.355602 \r\nL 244.783139 217.080496 \r\nL 245.087807 216.696085 \r\nL 245.392475 216.548495 \r\nL 245.697144 216.916465 \r\nL 246.001812 216.819169 \r\nL 246.30648 216.213567 \r\nL 246.915817 216.763701 \r\nL 247.220485 216.871308 \r\nL 247.525154 216.310795 \r\nL 247.829822 216.395201 \r\nL 248.13449 216.70783 \r\nL 248.439158 216.476915 \r\nL 248.743827 216.441049 \r\nL 249.048495 217.24156 \r\nL 249.353163 216.577665 \r\nL 249.657832 217.202548 \r\nL 249.9625 216.244806 \r\nL 250.267168 216.667096 \r\nL 250.876505 216.560339 \r\nL 251.181173 216.726596 \r\nL 251.485842 216.7343 \r\nL 251.79051 216.384927 \r\nL 252.095178 217.198521 \r\nL 252.399846 216.538356 \r\nL 252.704515 216.147046 \r\nL 253.313851 216.984504 \r\nL 253.61852 216.681155 \r\nL 253.923188 217.217259 \r\nL 254.532525 216.769244 \r\nL 254.837193 216.371319 \r\nL 255.141861 216.83314 \r\nL 255.446529 216.520169 \r\nL 255.751198 216.627473 \r\nL 256.055866 216.319631 \r\nL 256.360534 216.694794 \r\nL 256.665203 216.018082 \r\nL 256.969871 216.696373 \r\nL 257.579208 216.449373 \r\nL 257.883876 216.879194 \r\nL 258.493213 216.893558 \r\nL 258.797881 216.644156 \r\nL 259.407217 216.961442 \r\nL 259.711886 216.513346 \r\nL 260.016554 216.892574 \r\nL 260.321222 216.475885 \r\nL 260.625891 216.459616 \r\nL 260.930559 216.75625 \r\nL 261.539896 216.231877 \r\nL 261.844564 216.916259 \r\nL 262.149232 216.648301 \r\nL 262.4539 216.820464 \r\nL 262.758569 216.261369 \r\nL 263.063237 216.990993 \r\nL 263.367905 216.494454 \r\nL 263.672574 216.941429 \r\nL 263.977242 216.633767 \r\nL 264.28191 216.695751 \r\nL 264.586579 216.551662 \r\nL 264.891247 216.248371 \r\nL 265.195915 216.462495 \r\nL 265.500584 216.499954 \r\nL 265.805252 216.719283 \r\nL 266.10992 216.676391 \r\nL 266.414588 216.111101 \r\nL 266.719257 216.435635 \r\nL 267.023925 216.965418 \r\nL 267.328593 216.230032 \r\nL 267.633262 216.741514 \r\nL 267.93793 216.361497 \r\nL 268.547267 216.918717 \r\nL 268.851935 217.009372 \r\nL 269.156603 216.477139 \r\nL 269.461271 216.70087 \r\nL 269.76594 216.528275 \r\nL 270.070608 216.630354 \r\nL 270.375276 216.974731 \r\nL 270.679945 216.836277 \r\nL 270.984613 217.15106 \r\nL 271.289281 216.658746 \r\nL 271.59395 216.851113 \r\nL 271.898618 216.348107 \r\nL 272.203286 217.00983 \r\nL 272.507955 216.839648 \r\nL 272.812623 216.445805 \r\nL 273.117291 216.854998 \r\nL 273.421959 216.910868 \r\nL 273.726628 216.67169 \r\nL 274.031296 216.623464 \r\nL 274.335964 217.061358 \r\nL 274.640633 216.812078 \r\nL 275.249969 216.831294 \r\nL 275.554638 216.941128 \r\nL 275.859306 216.862935 \r\nL 276.468643 216.380482 \r\nL 276.773311 216.299233 \r\nL 277.077979 217.05216 \r\nL 277.382647 216.497256 \r\nL 277.991984 217.117359 \r\nL 278.601321 216.483969 \r\nL 278.905989 216.756831 \r\nL 279.210657 216.527281 \r\nL 279.515326 217.262045 \r\nL 279.819994 216.347056 \r\nL 280.124662 216.760727 \r\nL 280.42933 216.166681 \r\nL 280.733999 216.845588 \r\nL 281.038667 216.671623 \r\nL 281.952672 216.504979 \r\nL 282.25734 216.820292 \r\nL 282.562009 216.787421 \r\nL 282.866677 216.388783 \r\nL 283.171345 216.484047 \r\nL 283.476014 217.215432 \r\nL 283.780682 216.839562 \r\nL 284.08535 216.662891 \r\nL 284.390018 216.191124 \r\nL 284.999355 217.070423 \r\nL 285.608692 216.695231 \r\nL 285.91336 216.691331 \r\nL 286.218028 217.16955 \r\nL 286.522697 217.103817 \r\nL 287.132033 216.495919 \r\nL 287.436701 216.917111 \r\nL 287.74137 216.979656 \r\nL 288.046038 216.320256 \r\nL 288.350706 216.49329 \r\nL 288.655375 216.800049 \r\nL 288.960043 216.815583 \r\nL 289.264711 217.416435 \r\nL 289.56938 216.695602 \r\nL 289.874048 216.903786 \r\nL 290.178716 216.443421 \r\nL 290.788053 217.363334 \r\nL 291.092721 216.73437 \r\nL 291.397389 216.938747 \r\nL 292.006726 216.687438 \r\nL 292.616063 216.63893 \r\nL 292.920731 217.172474 \r\nL 293.225399 216.802656 \r\nL 293.530068 216.863526 \r\nL 294.139404 216.375667 \r\nL 294.444072 216.774641 \r\nL 294.748741 216.903519 \r\nL 295.053409 216.067689 \r\nL 295.358077 216.741576 \r\nL 295.662746 216.490562 \r\nL 296.576751 216.450435 \r\nL 296.881419 216.481733 \r\nL 297.186087 215.974062 \r\nL 297.490756 216.924812 \r\nL 297.795424 216.891002 \r\nL 298.100092 216.286926 \r\nL 298.40476 216.91105 \r\nL 299.014097 216.71984 \r\nL 299.318765 216.785591 \r\nL 299.623434 216.239912 \r\nL 299.928102 216.958692 \r\nL 300.23277 216.376137 \r\nL 301.146775 216.847669 \r\nL 301.451443 216.216384 \r\nL 301.756112 217.262999 \r\nL 302.06078 216.1183 \r\nL 302.365448 216.411385 \r\nL 302.670117 217.237461 \r\nL 303.279453 216.294993 \r\nL 303.584122 216.864804 \r\nL 303.88879 216.19739 \r\nL 304.193458 215.966244 \r\nL 304.802795 216.374637 \r\nL 305.107463 216.426071 \r\nL 305.7168 216.999145 \r\nL 306.326136 216.793671 \r\nL 306.630805 216.440705 \r\nL 306.935473 216.303919 \r\nL 307.240141 216.748999 \r\nL 307.54481 216.820734 \r\nL 307.849478 216.769363 \r\nL 308.458814 216.07333 \r\nL 308.763483 217.116062 \r\nL 309.677488 216.792728 \r\nL 309.982156 216.284409 \r\nL 310.286824 216.982686 \r\nL 310.591493 216.706166 \r\nL 311.200829 216.71984 \r\nL 311.505498 217.230631 \r\nL 311.810166 216.778419 \r\nL 312.114834 217.087643 \r\nL 312.419502 216.381616 \r\nL 313.638176 216.784861 \r\nL 313.942844 216.422812 \r\nL 314.247512 216.536926 \r\nL 314.552181 216.315764 \r\nL 315.161517 216.905192 \r\nL 315.466186 216.501629 \r\nL 315.770854 216.66066 \r\nL 316.075522 216.147826 \r\nL 316.38019 216.509016 \r\nL 316.684859 216.555603 \r\nL 316.989527 216.721139 \r\nL 317.294195 216.518419 \r\nL 317.598864 216.694476 \r\nL 318.2082 216.800477 \r\nL 318.512869 216.449306 \r\nL 318.817537 216.788024 \r\nL 319.122205 216.662912 \r\nL 319.426873 217.319116 \r\nL 319.731542 217.195671 \r\nL 320.03621 216.399533 \r\nL 320.340878 216.663165 \r\nL 320.645547 215.749143 \r\nL 320.950215 216.544686 \r\nL 321.254883 216.600603 \r\nL 321.559552 216.930384 \r\nL 322.168888 216.852748 \r\nL 322.473557 216.463723 \r\nL 322.778225 216.622272 \r\nL 323.082893 216.483933 \r\nL 323.387561 216.596339 \r\nL 323.69223 216.188488 \r\nL 323.996898 216.728114 \r\nL 324.301566 216.460136 \r\nL 324.606235 216.421581 \r\nL 324.910903 216.019604 \r\nL 325.215571 216.587068 \r\nL 325.52024 216.510449 \r\nL 325.824908 217.071795 \r\nL 326.434244 216.604824 \r\nL 326.738913 216.806389 \r\nL 327.043581 216.529398 \r\nL 327.348249 216.720513 \r\nL 327.652918 217.156522 \r\nL 327.957586 216.077315 \r\nL 328.262254 216.751824 \r\nL 328.566923 216.808032 \r\nL 328.871591 216.058512 \r\nL 329.176259 216.56226 \r\nL 329.480928 215.98095 \r\nL 329.785596 216.740628 \r\nL 330.090264 216.366629 \r\nL 330.394932 216.686739 \r\nL 330.699601 216.643847 \r\nL 331.004269 216.435405 \r\nL 331.308937 216.759341 \r\nL 331.918274 216.492018 \r\nL 332.222942 217.036448 \r\nL 332.527611 216.596904 \r\nL 332.832279 217.204602 \r\nL 333.136947 216.571231 \r\nL 333.441615 216.39798 \r\nL 334.050952 216.792012 \r\nL 334.35562 216.671035 \r\nL 334.660289 216.739006 \r\nL 334.964957 216.288558 \r\nL 335.269625 216.281096 \r\nL 336.18363 216.69054 \r\nL 336.792967 216.501033 \r\nL 337.097635 216.855927 \r\nL 337.402303 216.57791 \r\nL 337.706972 217.033541 \r\nL 338.01164 216.881471 \r\nL 338.316308 217.207596 \r\nL 338.620977 216.28567 \r\nL 339.534982 216.811059 \r\nL 339.83965 216.359045 \r\nL 340.144318 216.582818 \r\nL 340.144318 216.582818 \r\n\" style=\"fill:none;stroke:#ff7f0e;stroke-linecap:square;stroke-width:1.5;\"/>\r\n   </g>\r\n   <g id=\"patch_3\">\r\n    <path d=\"M 20.5625 227.300216 \r\nL 20.5625 9.860216 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_4\">\r\n    <path d=\"M 355.3625 227.300216 \r\nL 355.3625 9.860216 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_5\">\r\n    <path d=\"M 20.5625 227.300216 \r\nL 355.3625 227.300216 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_6\">\r\n    <path d=\"M 20.5625 9.860216 \r\nL 355.3625 9.860216 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"legend_1\">\r\n    <g id=\"patch_7\">\r\n     <path d=\"M 264.229688 47.494591 \r\nL 348.3625 47.494591 \r\nQ 350.3625 47.494591 350.3625 45.494591 \r\nL 350.3625 16.860216 \r\nQ 350.3625 14.860216 348.3625 14.860216 \r\nL 264.229688 14.860216 \r\nQ 262.229688 14.860216 262.229688 16.860216 \r\nL 262.229688 45.494591 \r\nQ 262.229688 47.494591 264.229688 47.494591 \r\nz\r\n\" style=\"fill:#ffffff;opacity:0.8;stroke:#cccccc;stroke-linejoin:miter;\"/>\r\n    </g>\r\n    <g id=\"line2d_15\">\r\n     <path d=\"M 266.229688 22.958654 \r\nL 286.229688 22.958654 \r\n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\r\n    </g>\r\n    <g id=\"line2d_16\"/>\r\n    <g id=\"text_13\">\r\n     <!-- loss -->\r\n     <g transform=\"translate(294.229688 26.458654)scale(0.1 -0.1)\">\r\n      <defs>\r\n       <path d=\"M 9.421875 75.984375 \r\nL 18.40625 75.984375 \r\nL 18.40625 0 \r\nL 9.421875 0 \r\nz\r\n\" id=\"DejaVuSans-108\"/>\r\n       <path d=\"M 30.609375 48.390625 \r\nQ 23.390625 48.390625 19.1875 42.75 \r\nQ 14.984375 37.109375 14.984375 27.296875 \r\nQ 14.984375 17.484375 19.15625 11.84375 \r\nQ 23.34375 6.203125 30.609375 6.203125 \r\nQ 37.796875 6.203125 41.984375 11.859375 \r\nQ 46.1875 17.53125 46.1875 27.296875 \r\nQ 46.1875 37.015625 41.984375 42.703125 \r\nQ 37.796875 48.390625 30.609375 48.390625 \r\nz\r\nM 30.609375 56 \r\nQ 42.328125 56 49.015625 48.375 \r\nQ 55.71875 40.765625 55.71875 27.296875 \r\nQ 55.71875 13.875 49.015625 6.21875 \r\nQ 42.328125 -1.421875 30.609375 -1.421875 \r\nQ 18.84375 -1.421875 12.171875 6.21875 \r\nQ 5.515625 13.875 5.515625 27.296875 \r\nQ 5.515625 40.765625 12.171875 48.375 \r\nQ 18.84375 56 30.609375 56 \r\nz\r\n\" id=\"DejaVuSans-111\"/>\r\n       <path d=\"M 44.28125 53.078125 \r\nL 44.28125 44.578125 \r\nQ 40.484375 46.53125 36.375 47.5 \r\nQ 32.28125 48.484375 27.875 48.484375 \r\nQ 21.1875 48.484375 17.84375 46.4375 \r\nQ 14.5 44.390625 14.5 40.28125 \r\nQ 14.5 37.15625 16.890625 35.375 \r\nQ 19.28125 33.59375 26.515625 31.984375 \r\nL 29.59375 31.296875 \r\nQ 39.15625 29.25 43.1875 25.515625 \r\nQ 47.21875 21.78125 47.21875 15.09375 \r\nQ 47.21875 7.46875 41.1875 3.015625 \r\nQ 35.15625 -1.421875 24.609375 -1.421875 \r\nQ 20.21875 -1.421875 15.453125 -0.5625 \r\nQ 10.6875 0.296875 5.421875 2 \r\nL 5.421875 11.28125 \r\nQ 10.40625 8.6875 15.234375 7.390625 \r\nQ 20.0625 6.109375 24.8125 6.109375 \r\nQ 31.15625 6.109375 34.5625 8.28125 \r\nQ 37.984375 10.453125 37.984375 14.40625 \r\nQ 37.984375 18.0625 35.515625 20.015625 \r\nQ 33.0625 21.96875 24.703125 23.78125 \r\nL 21.578125 24.515625 \r\nQ 13.234375 26.265625 9.515625 29.90625 \r\nQ 5.8125 33.546875 5.8125 39.890625 \r\nQ 5.8125 47.609375 11.28125 51.796875 \r\nQ 16.75 56 26.8125 56 \r\nQ 31.78125 56 36.171875 55.265625 \r\nQ 40.578125 54.546875 44.28125 53.078125 \r\nz\r\n\" id=\"DejaVuSans-115\"/>\r\n      </defs>\r\n      <use xlink:href=\"#DejaVuSans-108\"/>\r\n      <use x=\"27.783203\" xlink:href=\"#DejaVuSans-111\"/>\r\n      <use x=\"88.964844\" xlink:href=\"#DejaVuSans-115\"/>\r\n      <use x=\"141.064453\" xlink:href=\"#DejaVuSans-115\"/>\r\n     </g>\r\n    </g>\r\n    <g id=\"line2d_17\">\r\n     <path d=\"M 266.229688 37.636779 \r\nL 286.229688 37.636779 \r\n\" style=\"fill:none;stroke:#ff7f0e;stroke-linecap:square;stroke-width:1.5;\"/>\r\n    </g>\r\n    <g id=\"line2d_18\"/>\r\n    <g id=\"text_14\">\r\n     <!-- loss_rmsle -->\r\n     <g transform=\"translate(294.229688 41.136779)scale(0.1 -0.1)\">\r\n      <defs>\r\n       <path d=\"M 50.984375 -16.609375 \r\nL 50.984375 -23.578125 \r\nL -0.984375 -23.578125 \r\nL -0.984375 -16.609375 \r\nz\r\n\" id=\"DejaVuSans-95\"/>\r\n       <path d=\"M 41.109375 46.296875 \r\nQ 39.59375 47.171875 37.8125 47.578125 \r\nQ 36.03125 48 33.890625 48 \r\nQ 26.265625 48 22.1875 43.046875 \r\nQ 18.109375 38.09375 18.109375 28.8125 \r\nL 18.109375 0 \r\nL 9.078125 0 \r\nL 9.078125 54.6875 \r\nL 18.109375 54.6875 \r\nL 18.109375 46.1875 \r\nQ 20.953125 51.171875 25.484375 53.578125 \r\nQ 30.03125 56 36.53125 56 \r\nQ 37.453125 56 38.578125 55.875 \r\nQ 39.703125 55.765625 41.0625 55.515625 \r\nz\r\n\" id=\"DejaVuSans-114\"/>\r\n       <path d=\"M 52 44.1875 \r\nQ 55.375 50.25 60.0625 53.125 \r\nQ 64.75 56 71.09375 56 \r\nQ 79.640625 56 84.28125 50.015625 \r\nQ 88.921875 44.046875 88.921875 33.015625 \r\nL 88.921875 0 \r\nL 79.890625 0 \r\nL 79.890625 32.71875 \r\nQ 79.890625 40.578125 77.09375 44.375 \r\nQ 74.3125 48.1875 68.609375 48.1875 \r\nQ 61.625 48.1875 57.5625 43.546875 \r\nQ 53.515625 38.921875 53.515625 30.90625 \r\nL 53.515625 0 \r\nL 44.484375 0 \r\nL 44.484375 32.71875 \r\nQ 44.484375 40.625 41.703125 44.40625 \r\nQ 38.921875 48.1875 33.109375 48.1875 \r\nQ 26.21875 48.1875 22.15625 43.53125 \r\nQ 18.109375 38.875 18.109375 30.90625 \r\nL 18.109375 0 \r\nL 9.078125 0 \r\nL 9.078125 54.6875 \r\nL 18.109375 54.6875 \r\nL 18.109375 46.1875 \r\nQ 21.1875 51.21875 25.484375 53.609375 \r\nQ 29.78125 56 35.6875 56 \r\nQ 41.65625 56 45.828125 52.96875 \r\nQ 50 49.953125 52 44.1875 \r\nz\r\n\" id=\"DejaVuSans-109\"/>\r\n       <path d=\"M 56.203125 29.59375 \r\nL 56.203125 25.203125 \r\nL 14.890625 25.203125 \r\nQ 15.484375 15.921875 20.484375 11.0625 \r\nQ 25.484375 6.203125 34.421875 6.203125 \r\nQ 39.59375 6.203125 44.453125 7.46875 \r\nQ 49.3125 8.734375 54.109375 11.28125 \r\nL 54.109375 2.78125 \r\nQ 49.265625 0.734375 44.1875 -0.34375 \r\nQ 39.109375 -1.421875 33.890625 -1.421875 \r\nQ 20.796875 -1.421875 13.15625 6.1875 \r\nQ 5.515625 13.8125 5.515625 26.8125 \r\nQ 5.515625 40.234375 12.765625 48.109375 \r\nQ 20.015625 56 32.328125 56 \r\nQ 43.359375 56 49.78125 48.890625 \r\nQ 56.203125 41.796875 56.203125 29.59375 \r\nz\r\nM 47.21875 32.234375 \r\nQ 47.125 39.59375 43.09375 43.984375 \r\nQ 39.0625 48.390625 32.421875 48.390625 \r\nQ 24.90625 48.390625 20.390625 44.140625 \r\nQ 15.875 39.890625 15.1875 32.171875 \r\nz\r\n\" id=\"DejaVuSans-101\"/>\r\n      </defs>\r\n      <use xlink:href=\"#DejaVuSans-108\"/>\r\n      <use x=\"27.783203\" xlink:href=\"#DejaVuSans-111\"/>\r\n      <use x=\"88.964844\" xlink:href=\"#DejaVuSans-115\"/>\r\n      <use x=\"141.064453\" xlink:href=\"#DejaVuSans-115\"/>\r\n      <use x=\"193.164062\" xlink:href=\"#DejaVuSans-95\"/>\r\n      <use x=\"243.164062\" xlink:href=\"#DejaVuSans-114\"/>\r\n      <use x=\"282.527344\" xlink:href=\"#DejaVuSans-109\"/>\r\n      <use x=\"379.939453\" xlink:href=\"#DejaVuSans-115\"/>\r\n      <use x=\"432.039062\" xlink:href=\"#DejaVuSans-108\"/>\r\n      <use x=\"459.822266\" xlink:href=\"#DejaVuSans-101\"/>\r\n     </g>\r\n    </g>\r\n   </g>\r\n  </g>\r\n </g>\r\n <defs>\r\n  <clipPath id=\"pd5305c160b\">\r\n   <rect height=\"217.44\" width=\"334.8\" x=\"20.5625\" y=\"9.860216\"/>\r\n  </clipPath>\r\n </defs>\r\n</svg>\r\n",
      "image/png": "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"
     },
     "metadata": {
      "needs_background": "light"
     }
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "model0.save('keras_model0_0821_2.h5')\r\n",
    "model1.save('keras_model1_0821_2.h5')\r\n",
    "model2.save('keras_model2_0821_2.h5')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df_test_tail = origin_train.iloc[per_step*-1:,:-3].copy()\r\n",
    "df_test = origin_test.copy()\r\n",
    "df_test = pd.concat([df_test_tail,df_test],axis = 0)\r\n",
    "df_test,datatime_test = feature_engine(df_test)\r\n",
    "df_test.head()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "   relative_humidity  absolute_humidity  sensor_1  sensor_2  sensor_3  \\\n",
       "0               47.2             0.5632     855.0     612.1     981.4   \n",
       "1               46.0             0.5546     807.0     633.9    1088.3   \n",
       "2               44.7             0.5489     890.4     619.8    1035.9   \n",
       "3               44.0             0.5355     781.8     584.0    1209.5   \n",
       "4               46.6             0.5153     843.8     539.6    1282.6   \n",
       "\n",
       "   sensor_4  sensor_5      hour     month  mean_deg_c  \n",
       "0     876.1     594.7  0.916667  0.847222        10.1  \n",
       "1     893.8     520.9  1.000000  0.833333         9.8  \n",
       "2     910.4     579.7  0.916667  0.847222         9.2  \n",
       "3     895.6     465.5  0.833333  0.861111         8.9  \n",
       "4     793.3     427.5  0.750000  0.875000         9.4  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>relative_humidity</th>\n",
       "      <th>absolute_humidity</th>\n",
       "      <th>sensor_1</th>\n",
       "      <th>sensor_2</th>\n",
       "      <th>sensor_3</th>\n",
       "      <th>sensor_4</th>\n",
       "      <th>sensor_5</th>\n",
       "      <th>hour</th>\n",
       "      <th>month</th>\n",
       "      <th>mean_deg_c</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>47.2</td>\n",
       "      <td>0.5632</td>\n",
       "      <td>855.0</td>\n",
       "      <td>612.1</td>\n",
       "      <td>981.4</td>\n",
       "      <td>876.1</td>\n",
       "      <td>594.7</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>0.847222</td>\n",
       "      <td>10.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>46.0</td>\n",
       "      <td>0.5546</td>\n",
       "      <td>807.0</td>\n",
       "      <td>633.9</td>\n",
       "      <td>1088.3</td>\n",
       "      <td>893.8</td>\n",
       "      <td>520.9</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>9.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>44.7</td>\n",
       "      <td>0.5489</td>\n",
       "      <td>890.4</td>\n",
       "      <td>619.8</td>\n",
       "      <td>1035.9</td>\n",
       "      <td>910.4</td>\n",
       "      <td>579.7</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>0.847222</td>\n",
       "      <td>9.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>44.0</td>\n",
       "      <td>0.5355</td>\n",
       "      <td>781.8</td>\n",
       "      <td>584.0</td>\n",
       "      <td>1209.5</td>\n",
       "      <td>895.6</td>\n",
       "      <td>465.5</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.861111</td>\n",
       "      <td>8.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>46.6</td>\n",
       "      <td>0.5153</td>\n",
       "      <td>843.8</td>\n",
       "      <td>539.6</td>\n",
       "      <td>1282.6</td>\n",
       "      <td>793.3</td>\n",
       "      <td>427.5</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>9.4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "execution_count": 73
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#df_test = pd.DataFrame(min_max_scaler.fit_transform(df_test))\r\n",
    "df_test = df_to_series(df_test)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "pred0 = model0.predict(df_test)\r\n",
    "pred1 = model1.predict(df_test)\r\n",
    "pred2 = model2.predict(df_test)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "result = pd.DataFrame(datatime_test[per_step:].reset_index(drop=True))\r\n",
    "result = pd.concat([result,pd.DataFrame(np.concatenate([pred0,pred1,pred2],axis=1))],axis = 1)\r\n",
    "summit_columns = pd.read_csv('E:\\jupyter\\mygithub\\Tabular-Playground-Series\\sample_submission.csv').columns\r\n",
    "result.columns=summit_columns\r\n",
    "result.to_csv('keras_sub_0822_01.csv',index = False)"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.7.0",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.0 64-bit ('tensorflow_gpu_3': conda)"
  },
  "interpreter": {
   "hash": "f898655ca784692559396b1af9626b34a703e94ac9a4d0c848cba42136b70251"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}